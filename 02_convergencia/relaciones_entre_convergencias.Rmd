---
title: "Relaciones entre conceptos de convergencia"
author: "Daniel Czarnievicz"
date: "2019"
output: pdf_document
header-includes:
   - \everymath{\displaystyle}
   - \usepackage[spanish]{babel}
   - \usepackage{xcolor}
   - \usepackage[makeroom]{cancel}
   - \DeclareMathOperator*{\plim}{plim}
   - \usepackage{mathrsfs}
   - \usepackage{amsmath}
   - \usepackage{mathtools}
   - \newcommand{\equalexpl}[1]{\underset{\substack{\uparrow\\\\\mathrlap{\text{\hspace{-2em}#1}}}}{\approx}}
   - \DeclareMathOperator{\E}{\mathbf{E}}
   - \DeclareMathOperator{\V}{\mathbf{V}}
geometry: margin=1in
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Visión global

```{r global_vision, echo=FALSE, out.width="60%", fig.align='center'}
knitr::include_graphics(path = "diff_types_of_convgnc_color.png")
```


# Convergencia casi segura implica convergencia en probabilidad

## Demostración

Para demostrar la implicancia utilizaremos las siguientes definiciones de convergencia casi segura y de convergencia en probabilidad respectivamente:

- $X_n \overset{cs}{\rightarrow} X \Leftrightarrow \forall \varepsilon > 0$ se cumple que $\Pr \left( \bigcup\limits_{n = k}^{\infty} \Big\{ \omega \in \Omega \, : \, \big| X_n(\omega) - X(\omega) \big| > \varepsilon \Big\} \right) \overset{k}{\rightarrow} 0$

- $X_n \overset{p}{\rightarrow} X \Leftrightarrow \forall \varepsilon > 0$ se cumple que $\Pr \left( \Big\{ \omega \in \Omega \, : \, \big| X_k(\omega) - X(\omega) \big| > \varepsilon \Big\} \right) \overset{k}{\rightarrow} 0$

Sean: 

i. $A_n = \Big\{ \omega \in \Omega \, : \, \big| X_n(\omega) - X(\omega) \big| > \varepsilon \Big\}$

ii. $A_k = \Big\{ \omega \in \Omega \, : \, \big| X_k(\omega) - X(\omega) \big| > \varepsilon \Big\}$

De esta forma, podemos rescribir las definiciones a utilizar de la siguiente forma:

- $X_n \overset{cs}{\rightarrow} X \Leftrightarrow \forall \varepsilon > 0$ se cumple que $\Pr \left( \bigcup\limits_{n = k}^{\infty} A_n \right) \overset{k}{\rightarrow} 0$

- $X_n \overset{p}{\rightarrow} X \Leftrightarrow \forall \varepsilon > 0$ se cumple que $\Pr \left( A_k \right) \overset{k}{\rightarrow} 0$

Notemos ahora que $\bigcup\limits_{n = k}^{\infty} A_n = A_k \cup A_{k+1} \cup A_{k+2} \cup \ldots$. Esto implica que $A_k \subset \bigcup\limits_{n = k}^{\infty}A_n$. Por lo tanto, $\Pr (A_k) \leq \Pr \left( \bigcup\limits_{n = k}^{\infty} A_n \right)$. Pero $\Pr \left( \bigcup\limits_{n = k}^{\infty} A_n \right) \overset{k}{\rightarrow} 0$ dado que $X_n \overset{cs}{\rightarrow} X$ por hipótesis. Esto implica que $\Pr (A_k)$ está acotado inferirormente por $0$ (por ser una probabilidad) y superiormente por una probabilidad que converge a cero. Entonces $\Pr (A_k) \overset{k}{\rightarrow} 0$, y por lo tanto, $X_n \overset{p}{\rightarrow} X$.

## Contraejemplo del recíproco

Consideremos los siguientes conjuntos: $I_n^i = \left[\frac{i-1}{n}, \, \frac{i}{n} \right]$, y la sucesión de variables aleatorias $X_n^i = \mathbb{I}_{ \{ \omega \in I_n^i \}}$ con $i = 1, 2, \ldots, n$. Es decir que nuestras variables son las indicatrices de pertenecer a los siguientes conjuntos:
$$\begin{array}{l l l}
I_1^1 = \left[0, 1\right] & & \\ \\
I_2^1 = \left[0, \, ^1\!/_2\right] & I_2^2 = \left[^1\!/_2, 1\right] & \\ \\
I_3^1 = \left[0, \, ^1\!/_3\right] & I_3^2 = \left[^1\!/_3, \, ^2\!/_3\right] & I_3^3 = \left[^2\!/_3, 1\right] \\
\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \vdots & & 
\end{array}$$

Luego entonces, la distribución de probabilidades de $X_n$ viene dada por:

- $\Pr (X_n^i = 1) = \Pr (x \in \mathbb{I}_{X_n^i}) = \frac{i}{n} - \frac{i-1}{n} = \frac{1}{n}$  

- $\Pr (X_n^i = 0) = 1 - \frac{1}{n}$

\vspace{0.3cm}

**Convergencia en probabilidad**
$$\forall \varepsilon > 0, \,\,\ \Pr \left( \Big\{ \omega \in \Omega : \big| X_n^i(\omega) - 0 \big| > \varepsilon \Big\} \right) = \Pr \left( \Big\{ \omega \in \Omega : X_n^i(\omega) = 1 \Big\} \right) = \frac{1}{n} \overset{n}{\rightarrow} 0 \Rightarrow X_n \overset{p}{\rightarrow} 0$$

\vspace{0.3cm}

**Convergencia casi segura**
$$\forall \varepsilon > 0, \,\,\ \Pr \left( \Big\{ \omega \in \Omega : \bigcup\limits_{n = k}^{\infty} \big| X_n^i(\omega) - 0 \big| > \varepsilon \Big\} \right) = \Pr \left( \Big\{ \omega \in \Omega : \bigcup\limits_{n = k}^{\infty} X_n^i(\omega) = 1 \Big\} \right) = $$
$$= \Pr \big( x \in [0,1] \big) = 1 \Rightarrow X_n \not\overset{cs}{\rightarrow} 0$$

\newpage

# Convergencia en probabilidad implica convergencia en distribución

## Demostración

Supongamos $x \in \mathscr{C}(F)$ y definamos las sucesiones reales $a_k = x - \,^1\!/_k$ y $b_k = x + \,^1\!/_k$. Dado que $a < x < b$, se cumple que $a_k < x < b_k$. Por otra parte, dado que $X_n \overset{p}{\rightarrow} X$:
$$F_X(a_k) \leq \liminf\limits_n F_{X_n}(x) \leq \limsup\limits_n F_{X_n}(x) \leq F_X(b_k)$$

Tomando límites cuando $k \rightarrow +\infty$ y teniendo en cuenta que $x \in \mathscr{C}(F)$ obtenemos que:
$$F_X(x) = \lim\limits_k F_X(a_k) \leq \lim\limits_k \liminf\limits_n F_{X_n}(x) \leq \lim\limits_k \limsup\limits_n F_{X_n}(x) \leq \lim\limits_k F_X(b_k) = F_X(x)$$
$$F_X(x) = \lim\limits_k F_X(a_k) \leq \liminf\limits_n F_{X_n}(x) \leq \limsup\limits_n F_{X_n}(x) \leq \lim\limits_k F_X(b_k) = F_X(x)$$

De donde podemos concluir que, dado que tanto el $\liminf$ como el $\limsup$ están acotados tanto superior como inferiormente por $F_X(x)$,
$$F_X(x) = \liminf\limits_n F_{X_n}(x) = \limsup\limits_n F_{X_n}(x)$$

## Contraejemplo del recíproco

Sean la variable aleatorias $X \sim \text{N}(0,1)$ y la sucesión de variables aleatorias $X_n = (-1)^n X$. Entonces, dado que la distribución normal es simétrica, se cumple que $X_n \sim \text{N}(0,1)$. Por lo tanto, 
$$F_{X_n}(x) = \Phi_n(x) \overset{n}{\rightarrow} \Phi(x) = F_X(x) \Rightarrow X_n \overset{d}{\rightarrow} X$$

Por otro lado tenemos que, si $n$ es par, entonces se cumple que $|X_n - X| = 0$ por lo tanto $P(|X_n - X| > \varepsilon) = 0$. Mientras que si $n$ es impar, tenemos que $|X_n - X| = 2|X|$, por lo tanto:
$$\Pr (|X_n - X| > \varepsilon) = \Pr ( 2 \, |X| > \varepsilon ) = \Pr ( |X| > \varepsilon / 2 ) = 2 \, \Phi (\varepsilon / 2) > 0$$

Por lo tanto, $X_n \not\overset{p}{\rightarrow} X$.

\newpage

# Convergencia en $L^p$ implica convergencia en probabilidad

## Demostración

$$\forall \varepsilon > 0, \,\,\,\,\,\, \Pr \big( |X_n - X| > \varepsilon \big) = \Pr \big( |X_n - X|^p > \varepsilon^p) \leq \frac{ \E \big( |X_n - X|^p \big) }{ \varepsilon^p } \overset{n}{\rightarrow} 0$$
donde la convergencia está dada porque $X_n \overset{L^p}{\rightarrow} X$. Dado que converge a cero, queda demostrado que $X_n \overset{p}{\rightarrow} X$.

## Contraejemplo del recíproco

Sea $\Omega = [0, \, 1]$ y la siguiente sucesión de variables aleatorias:
$$X_n(\omega) = \left\{ \begin{array}{c c c}
e^n & \text{si} & \omega \in \left[ 0, \, ^1\!/_n \right] \\ \\
0   & \text{si} & \omega \in \left( ^1\!/_n, \, 1 \right]
\end{array} \right.$$

Luego entonces:
$$\Pr \big( |X_n - 0| > \varepsilon \big) = \Pr \big( X_n > \varepsilon \big) = \Pr \big( X_n = e^n \big) = \frac{1}{n} \overset{n}{\rightarrow} 0 \Rightarrow X_n \overset{p}{\rightarrow} 0$$

Mientras que:
$$\E \big( |X_n - 0|^p \big) = \E \big( |X_n|^p \big) = \E \big( X^p \big) = (e^n)^p \, \frac{1}{n} + 0^p \left(1 - \frac{1}{n} \right) = e^{np} \overset{n}{\rightarrow} + \infty \Rightarrow X_n \not\overset{L^p}{\rightarrow} 0$$

