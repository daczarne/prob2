---
title: "Capítulo 2 - Convergencia de variables aleatorias"
author: "Daniel Czarnievicz"
date: "2019"
output: pdf_document
header-includes:
   - \everymath{\displaystyle}
   - \usepackage[spanish]{babel}
   - \usepackage{xcolor}
   - \usepackage[makeroom]{cancel}
   - \DeclareMathOperator*{\plim}{plim}
   - \usepackage{mathrsfs}
   - \usepackage{amsmath}
   - \usepackage{mathtools}
   - \newcommand{\equalexpl}[1]{\underset{\substack{\uparrow\\\\\mathrlap{\text{\hspace{-2em}#1}}}}{\approx}}
   - \DeclareMathOperator{\E}{\mathbf{E}}
   - \DeclareMathOperator{\Var}{\mathbf{Var}}
   - \DeclareMathOperator{\Cov}{\mathbf{Cov}}
geometry: margin=1in
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Convergencia casi segura, sucesiones de Cauchy, convergencia de series

#### Definición 2.1: Convergencia casi segura

Sean $X$ y $\{X_n\}_{n \in \mathbb{N}}$ variable aleatorias definidas en el mismo espacio de probabilidad $(\Omega, \, \mathcal{A}, \, \Pr)$ entonces, decimos que:

$$X_n \overset{cs}{\rightarrow} X \Leftrightarrow \Pr \big( \{ \omega \in \Omega : X_n(\omega) \overset{n}{\rightarrow} X(\omega) \} \big) = 1$$

#### Observación 2.2

- Si $X_n \overset{cs}{\rightarrow} X$, entonces $X$ es una variable aleatoria.  

- Si $g : \mathbb{R}^{d} \rightarrow \mathbb{R}$ es continua y $X_n^i \overset{cs}{\rightarrow} X^i$ para todo $1 \leq i \leq d$, entonces:
$$g \big(X_n^1, \, \ldots, \, X_n^d \big) \overset{cs}{\rightarrow} g \big(X^1, \, \ldots, \, X^d \big)$$

- Si $X_n \overset{cs}{\rightarrow} 0$ y $\exists k > 0$ tal que $\Pr \big( |Y_n| > k \big) = 0$ para todo $n$, entonces $X_n \, Y_n \overset{cs}{\rightarrow} 0$.

#### Teorema 2.3

$$X_n \overset{cs}{\rightarrow} X \Leftrightarrow \forall \varepsilon > 0, \,\,\, \lim\limits_{k \rightarrow + \infty} \Pr \left( \bigcap\limits_{n = k}^{+\infty} \big\{ |X_n - X| < \varepsilon \big\} \right) = 1 $$

*Dem*: observemos que:
$$\Pr \left( \lim\limits_{n \rightarrow +\infty} X_n = X \right) = 1 \Leftrightarrow \Pr \left( \bigcap\limits_{\varepsilon \in \mathbb{Q}^+} \bigcup\limits_{k = 1}^{+\infty} \bigcap\limits_{n = k}^{+\infty} \big\{ |X_n - X| < \varepsilon \big\} \right) = 1 \Leftrightarrow$$
$$\Leftrightarrow \Pr \left( \bigcup\limits_{k = 1}^{+\infty} \bigcap\limits_{n = k}^{+\infty} \big\{ |X_n - X| < \varepsilon \big\} \right) = 1 \,\,\, \forall \varepsilon \in \mathbb{Q}^+$$

Si llamemos $B_k = \bigcap\limits_{n = k}^{+\infty} \big\{ |X_n - X| < \varepsilon \big\}$, $B_k$ es entonces una sucesión creciente, por lo que:
$$\Pr \left( \bigcup\limits_{k = 1}^{+\infty} B_k \right) = \lim\limits_{k \rightarrow + \infty} \Pr \big( B_k \big) = \Pr \left( \bigcap\limits_{n = k}^{+\infty} \big\{ |X_n - X| < \varepsilon \big\} \right)$$

#### Corolario 2.4

Sea $\{X_n\}_{n \in \mathbb{N}}$ una sucesión de variables aleatorias tal que $\forall \varepsilon > 0$, $\sum\limits_{n = 1}^{+\infty} \Pr \big( |X_n - X| > \varepsilon \big) < +\infty$ entonces $X_n \overset{cs}{\rightarrow} X$.

#### Teorema 2.5

$$X_n \overset{cs}{\rightarrow} X \Leftrightarrow \forall \varepsilon > 0, \,\,\, \Pr \left( \sup\limits_{n \geq k} |X_n - X| > \varepsilon \right) = 0$$

#### Sucesión de Cauchy

Una sucesión $\{x_n\}_{n \in \mathbb{N}}$ es de Cauchy $\Leftrightarrow |x_n - x_m| \rightarrow 0$ cuando $n \rightarrow + \infty$ y $m \rightarrow + \infty$.

#### Definición 2.6: Sucesión de Cauchy (versión para variables aleatorias)

Sean $X$ y $\{X_n\}_{n \in \mathbb{N}}$ variables aleatorias definidas en el mismo espacio de probabilidad $(\Omega, \, \mathcal{A}, \, \Pr)$ a valores de $\mathbb{R}^d$. Entonces, decimos que:
$$X_n \text{ es de Cauchy con proabilidad 1 } \Leftrightarrow \Pr \big( \{ \omega \in \Omega : X_n(\omega) \text{ es de Cauchy} \} \big) = 1$$

#### Teorema 2.7: definiciones equivalentes de sucesión de Cauchy con prob. 1

La sucesión $\{X_n\}_{n \in \mathbb{N}}$ es de Cauchy con probabilidad 1 sí, y solo sí:

1. para todo $\varepsilon > 0$ se cumple que:
$$\Pr \left( \sup\limits_{ \substack{ k \geq n \\ l \geq n } } \big\{ | X_k - X_l | > \varepsilon \big\} \right) \overset{n}{\rightarrow} +\infty$$

2. para todo $\varepsilon > 0$ se cumple que:
$$\Pr \left( \sup\limits_{ k \geq 0 } \big\{ | X_{n + k} - X_n | > \varepsilon \big\} \right) \overset{n}{\rightarrow} +\infty$$

#### Teorema 2.8

Una sucesión $\{X_n\}_{n \in \mathbb{N}}$ es de Cauchy con probabilidad 1 $\Leftrightarrow \exists X$ tal que $X_n \overset{cs}{\rightarrow} X$.

## Teoremas de convergencia monótona y dominada

#### Teorema 2.9: Convergencia monótona

Sean $X$ y $\{X_n\}_{n \in \mathbb{N}}$ variables aleatorias definidas en el mismo espacio de probabilidad $(\Omega, \, \mathcal{A}, \, \Pr)$ tal que existe $\E(X)$, $X_n(\omega) \geq 0$ y $X_n(\omega) \uparrow X(\omega)$ para casi todo $\omega \in \Omega$ (es decir, $\exists D \subset \Omega$ con $\Pr(D) = 1$ tal que $X_n(\omega) \uparrow X(\omega)$ para todo $\omega \in D$). Entonces:

1. existe $\E(X_n)$ para todo $n$

2. $\lim\limits_{n \rightarrow +\infty} \E(X_n) = \E(X)$

#### Observación 2.10

El teorema 2.9 sigue siendo válido si $\E(X) = \infty$, pero lo único que se puede concluir es que $\E(X_n) \overset{n}{\rightarrow} \infty$.

#### Teorema 2.11: Convergencia dominada

Sean $X$, $Y$, y $\{X_n\}_{n \in \mathbb{N}}$ variables aleatorias definidas sobre el mismo espacio de probabilidad $(\Omega, \, \mathcal{A}, \, \Pr)$ tales que $X_n \overset{cs}{\rightarrow} X$, $|X_n(\omega)| \leq Y$ para casi todo $\omega \in \Omega$, y existe $\E(Y)$, entonces:

1. existe $\E(X_n)$ para todo $n$

2. existe $\E(X)$

3. $\lim\limits_{n \rightarrow +\infty} \E(X_n) = \E(X)$

#### Corolario 2.12

Si $f_n : [a,b] \rightarrow \mathbb{R}$ y $g : [a,b] \rightarrow \mathbb{R}$ son Riemann integrables tales que $\lim\limits_n f_n(x) = f(x)$ y $|f_n(x)| \leq g(x)$ para todo $x \in [a,b]$, entonces:
$$\lim\limits_n \int\limits_a^b f_n(x) dx = \int\limits_a^b f(x) dx$$

#### Teorema 2.13

Sean:

i. dos reales $a$ y $b$ tales que $-\infty < a < b < + \infty$

ii. $f : \mathbb{R} \times [a,b] \rightarrow \mathbb{R}$

iii. $F : \mathbb{R} \rightarrow \mathbb{R}$ una función monótona no decreciente tal que para todo $t \in [a,b]$ existe y es finita $\int\limits_{\mathbb{R}} f(t,x) dF(x)$

iv. $G(t) = \int\limits_{\mathbb{R}} f(x,t) d F(x)$

Supongamos que:

i. $\frac{\partial f(x,t)}{\partial t}$ existe para todo $x$

ii. existe $g$ tal que $\int\limits_{\mathbb{R}} g(x) dF(x) < \infty$ y $\left| \frac{\partial f(x,t)}{\partial t} \right| < g(x)$ para todo $t$, $x$

Entonces:

1. $G$ es derivable

2. $G'(x) = \int\limits_{\mathbb{R}} \frac{\partial f(x,t)}{\partial t} dF(x)$

<!--
Sea $f : \mathbb{R} \times [a,b] \rightarrow \mathbb{R}$ y $F : \mathbb{R} \rightarrow \mathbb{R}$ una función monótona no decrecientes con $-\infty < a < b < + \infty$, tal que para todo $t \in [a,b]$ la función $f(.,t) : \mathbb{R} \rightarrow \mathbb{R}$ es integrable respecto de $dF(x)$ (es decir, existe y es finita $\int\limits_{\mathbb{R}} f(t,x) dF(x)$), denotemos:
$$G(t) = \int\limits_{\mathbb{R}} f(x,t) d F(x)$$
supongamos que $\frac{\partial f(x,t)}{\partial t}$ existe para todo $x$ y además existe $g$ tal que $\int\limits_{\mathbb{R}} g(x) dF(x) < \infty$ y $\left| \frac{\partial f(x,t)}{\partial t} \right| < g(x)$ para todo $t$, $x$, entonces $G$ es derivable y vale:
$$G'(x) = \int\limits_{\mathbb{R}} \frac{\partial f(x,t)}{\partial t} dF(x)$$
-->

## Convergencia casi segura de series

#### Teorema 2.14: Kolmogorov-Khinchin

Sea $\{X_n\}_{n \in \mathbb{N}}$ una sucesión de variables aleatorias independientes tales que $\E(X_n) = 0$ para $n \geq 1$. 
$$\text{Si } \sum\limits_{n = 1}^{+\infty} \E(X_n^2) < +\infty \Rightarrow \sum\limits_{n = 1}^{+\infty} X_n < +\infty \text{ con probabilidad 1}$$

***

Si las variables $X_k$ son uniformemente acotadas (es decir, existe un real $c < +\infty$ tal que $\Pr(|X_k| < c) = 1$ para todo $k$), el recíproco es cierto. Por lo tanto, se cumple que:
$$\text{Si } \sum\limits_{n = 1}^{+\infty} X_n < +\infty \Rightarrow \sum\limits_{n = 1}^{+\infty} \E(X_n^2) < +\infty$$

# Convergencia en Probabilidad

#### Definición 2.15: Convergencia en probabilidad

Sean $X$ y $\{X_n\}_{n \in \mathbb{N}}$ variable aleatorias definidas sobre el mismo espacio de probabilidad $(\Omega, \, \mathcal{A}, \, \Pr)$.
$$X_n \overset{p}{\rightarrow} X \Leftrightarrow \forall \varepsilon > 0, \,\, \Pr \left( \big\{ \omega \in \Omega : |X_n(\omega) - X(\omega)| > \varepsilon \big\} \right) \overset{n}{\rightarrow} 0$$

#### Proposición 2.16

Si $X_n \overset{p}{\rightarrow} X$ y $X_n \overset{p}{\rightarrow} Y$, entonces $\Pr(X = Y) = 1$.

#### Teorema 2.17

Sean $X$ y $\{X_n\}_{n \in \mathbb{N}}$ variables aleatorias definidas sobre el mismo espacio de probabilidad $(\Omega, \, \mathcal{A}, \, \Pr)$. Si $X_n \overset{cs}{\rightarrow} X$, entonces $X_n \overset{p}{\rightarrow} X$.

#### Proposición 2.19

Si $g : \mathbb{R}^d \rightarrow \mathbb{R}$ es una función continua, y $X_n^i \overset{p}{\rightarrow} X^i$ para todo $1 \leq i \leq d$ (es decir, cada uno de sus componentes converge en probabilidad), entonces $g \big( X_n^1, \, \ldots, \, X_n^d \big) \overset{p}{\rightarrow} g \big( X^1, \, \ldots, \, X^d \big)$.

***

En particular, tomando $d = 2$, obtenemos que si $X_n \overset{p}{\rightarrow} X$ y $Y_n \overset{p}{\rightarrow} Y$ entonce se cumple que para todo $a, b \in \mathbb{R}$:

1. $a \, X_n + b \, Y_n \overset{p}{\rightarrow} a \, X + b \, Y$

2. $(a \, X_n) (b \, Y_n) \overset{p}{\rightarrow} (a \, X) (b \, Y)$

#### Proposición 2.20

Si $X_n \overset{p}{\rightarrow} 0$ y $\exists k > 0$ tal que $\Pr ( | Y_n | > k ) = 0$ para todo $n$, entonces $X_n \, Y_n \overset{p}{\rightarrow} 0$.

#### Teorema 2.21

Sea $\{X_n\}_{n \in \mathbb{N}}$ una sucesión de variables aleatorias. Enotnces $X_n \overset{p}{\rightarrow} X \Leftrightarrow$ para toda subsucesión $\{X_{n_k}\}_{k \in \mathbb{N}}$ existe una sub-subsucesión $\{X_{n_{k_j}}\}_{j \in \mathbb{N}}$ (es decir, una subsucesión de $\{X_{n_k}\}_{k \in \mathbb{N}}$) tal que $\{X_{n_{k_j}}\}_{j \in \mathbb{N}} \overset{cs}{\rightarrow} X$ cuando $j \rightarrow +\infty$.

#### Teorema 2.22

$$X_n \overset{p}{\rightarrow} X \Leftrightarrow \E \left( \frac{ |X_n - X| }{ 1 + |X_n - X| } \right) \overset{n}{\rightarrow} 0$$

## Sobre la diferencia entre la convergencia casi segura y la convergencia en probabilidad

> Supongamos que el comité de una empresa está compuesto por 100 miembros, los cuales se deben reunir todos los días durante un año, y queremos estudiar la asistencia a las reuniones por parte de los miembros del comité. La *convergencia casi segura* es como preguntarse si casi todos los miembros del comité tuvieron asistencia perfecta. La *convergencia en probabilidad* es como preguntarse si todas las reuniones estuvieron casi completas.

> Si casi todos los miembros tuvieron asistencia perfecta (convergencia casi segura), entonces cada reunión estuvo casi completa (convergencia en probabilidad). Por lo tanto, la convergencia casi segura, implica la convergencia en probabilidad.

> Pero que todas las reuniones hayan estado casi completas (convergencia en probabilidad), no implica que ninguno de los miembros haya tenido asistencia perfecta (convergencia casi segura). Por ejemplo, cada miembro podría haber faltado unas pocas veces. Por lo tanto, la convergencia en probabilidad no implíca la convergencia casi segura.

Vemos entonces que la convergencia casi segura implica que $X_n(\omega)$ ``se acerca'' a $X(\omega)$ a medida que $n$ aumenta para casi todos los elementos ($\omega$) del espacio muestral ($\Omega$). Es entonces una afirmación acerca de la convergencia de $X_n(\omega)$ para casi todos los $\omega$'s, de forma individual.

Por su parte, la convergencia en probabilidad implica que el sub-conjunto de elementos $\omega$ para los cuales $X_n(\omega)$ ``se acerca'' a $X(\omega)$, tiene una probabilidad que "se acerca" a 1, a medida que $n$ aumenta. Es entonces una afirmación acerca del tamaño del sub-conjunto de $\omega$'s que satisfacen la "propiedad de acercamiento", pero no acerca de los $\omega$'s de forma individual.

# Ley Débil de los Grandes Números

#### Teorema 2.23

Sea $\{X_n\}_{n \in \mathbb{N}}$ una sucesión de variables aleatorias con esperanza finita.
$$\text{Si } \frac{1}{n^2} \Var \left( \sum\limits_{i = 1}^{n} X_i \right) \overset{n}{\rightarrow} 0 \Rightarrow \frac{1}{n} \left[ \sum\limits_{i = 1}^n \big( X_i - \E(X_i) \big) \right] \overset{p}{\rightarrow} 0$$

En particular, si las $X_i$ son independientes dos a dos, y existe $C$ tal que $\Var(X_i) < C$ para todo $i$, entonces se cumple el Teorema 2.8

#### Definición 2.24: Coeficiente de correlación

Sean $X$ e $Y$ variables aleatorias definidas en un mismo espacio de probabilidad con varianza finita, se define el coeficiente de correlación entre ellas como:
$$\rho(X,Y) = \frac{ \Cov(X,Y) }{ \sqrt{ \Var(X) } \sqrt{ \Var(Y) } } $$

#### Observación 2.25

- Si $X$ e $Y$ son independientes, $\rho(X,Y) = 0$. El recíproco no es cierto en general

- $-1 \leq \rho(X,Y) \leq 1$

- $|\rho(X,Y)| = 1 \Leftrightarrow X = a Y + b$ con $a,b \in \mathbb{R}$

#### Definición 2.26: Sucesión débilmente estacionaria

Una sucesión $\{X_n\}_{n \in \mathbb{N}}$ se dice débilmente estacionaria si:

1. $\E \big( X_n \big) = c < +\infty$ para todo $n$ (la esperanza de cada variable es constante).

2. $\E \big( X_n^2 \big) < +\infty$ (segundo momento finito).

3. $\E \big( X_n, X_m \big) = f(n - m)$ (la covarianza depende únicamente de la diferencia $n - m$). 

#### Teorema 2.28

Sea $\{X_n\}_{n \in \mathbb{N}}$ una sucesión de variables aleatorias tales que $\E(X_i) = a$. Si $\rho(X_n, \, X_m) \rightarrow 0$ cuando $|n - m| \rightarrow +\infty$ entonces:
$$\frac{1}{n} \sum\limits_{i = 1}^n X_i \overset{p}{\rightarrow} a$$

# Ley Fuerte de los Grandes Números

#### Lema 2.29: Lema de Kronecker

Sea $\{x_n\}_{n \in \mathbb{N}}$ una sucesión real tal que $\sum\limits_{i = 1}^{+\infty} x_n = s$. Sea $b_n \uparrow + \infty$ una sucesión real no decreciente de números positivos. Entonces:
$$\lim\limits_{n \rightarrow + \infty} \frac{1}{b_n} \sum\limits_{k = 1}^{n} b_k \, x_k = 0$$

En particular, si $b_n = n$ y $x_n = y_n / n$ tal que $\sum\limits_{n = 1}^{\infty} y_n / n$ converge, entonces:
$$\frac{y_1 + \ldots + y_n}{n} \overset{n}{\rightarrow} 0$$

#### Teorema 2.30: Ley Fuerte de Kolmogorov

Sea $\{X_n\}_{n \in \mathbb{N}}$ una sucesión de variables aleatorias independientes con segundo momento finito. Supongamos que existen números positivos $b_n$ tales que:
$$\left.
\begin{array}{cl}
1. & b_n \uparrow +\infty \\ \\
2. & \sum\limits_{n = 1}^{\infty} \frac{\Var(X_n)}{b_n^2} < \infty
\end{array} \right\}
\Rightarrow
\frac{ S_n - \E(S_n) }{ b_n } \overset{cs}{\rightarrow} 0$$

En particular,
$$\text{Si } \sum\limits_{n = 1}^{\infty} \frac{\Var(X_n)}{n^2} < \infty \Rightarrow \frac{ S_n - \E(S_n) }{ n } \overset{cs}{\rightarrow} 0$$

#### Lema 2.31

Sea $X$ una variable aleatoria no negativa, entonces:
$$\sum\limits_{n = 1}^{\infty} \Pr \big( X \geq n \big) \leq \E(X) \leq 1 + \sum\limits_{n = 1}^{\infty} \Pr \big( X \geq n \big)$$

#### Lema 2.32

Sea $X$ una variable aleatoria no negativa, entonces:
$$\E(X) = \int\limits_0^{+\infty} \Pr \big(X > x\big) dx$$

Esto es, el área entre la recta $y = 1$ y la gráfica de la función de distribución (acumulada) de $X$, es igual a la esperanza de la variable $X$. Podemos entonces rescribir la fórumla anterior como:
$$\begin{array}{rcl}
\E(X) & = & \int\limits_0^{+\infty} \Pr \big(X > x\big) dx \\ \\
   & = & \int\limits_0^{+\infty} \left[ 1 - \Pr \big(X \leq x\big) \right] dx \\ \\
   & = & \int\limits_0^{+\infty} \big[ 1 - F_X(x) \big] dx
\end{array}$$

#### Lema 2.33: Lema de Toeplitz

Sea $\{a_n\}_{n \in \mathbb{N}}$ una sucesión de números no negativos tales que $a_1 > 0$. Sea $b_n = \sum\limits_{i = 1}^{n} a_i$. Supongamos que $b_n \uparrow +\infty$. Sea $\{x_n\}_{n \in \mathbb{N}}$ tal que $x_n \overset{n}{\rightarrow} x$, entonces:
$$\frac{1}{b_n} \sum\limits_{j = 1}^{n} a_j \, x_j \overset{n}{\rightarrow} x$$

En particular, si $a_n = 1$ para todo $n$, entonces:
$$\frac{x_1 + \ldots + x_n}{n} \overset{n}{\rightarrow} x$$

#### Teorema 2.34

Sea $\{X_n\}_{n \in \mathbb{N}}$ una sucesión de variables aleatorias independientes e idénticamente distribuidas tales que $\E|X_i| < \infty$, entonces:
$$\frac{S_n}{n} \overset{cs}{\rightarrow} \E (X_1)$$

#### Teorema 2.35: recíproco del teorema 2.34

Sea $\{X_n\}_{n \in \mathbb{N}}$ una sucesión de variables aleatorias independientes e idénticamente distribuidas tales que $\frac{X_1 + \ldots + X_n}{n} \overset{cs}{\rightarrow} C$, entonces:

1. $\E|X_i| < \infty$

2. $\E(X_1) = C$

\newpage

# Convergencia en Distribución

#### Definición 2.36

Sean $\{X_n\}_{n \in \mathbb{N}}$ una sucesión de variables aleatorias definidas en $(\Omega_n, \, \mathcal{A}_n, \, {\Pr}_n)$, y $X$ una variable aleatoria definida en $(\Omega, \, \mathcal{A}, \, \Pr)$.
$$X_n \overset{d}{\rightarrow} X \Leftrightarrow \lim\limits_{n \rightarrow + \infty} F_{X_n}(x) = F_X(x) \,\,\,\, \forall x \in \mathcal{C}(F_X)$$

#### Observación 2.37

Las descontinuidades de una función de distribución son de salto (el límite por izquiera y el límite por derecha no coinciden), con lo cual, dado que la suma de dichos saltos tiene que estar incluido en $[0,1]$, solo pueden ser una cantidad numerable de saltos.

#### Teorema 2.38

Si $X_n \overset{d}{\rightarrow} X$ y $F_X(x)$ es continua para todo $x$, entonces $F_{X_n} \overset{n}{\rightarrow} F_X$ uniformemente.

#### Teorema 2.39

Sean $\{X_n\}_{n \in \mathbb{N}}$ una sucesión de variables aleatorias definidas en $(\Omega_n, \, \mathcal{A}_n, \, {\Pr}_n)$, y $X$ una variable aleatoria definida en $(\Omega, \, \mathcal{A}, \, \Pr)$. Sea $f$ una función de codominio real, continua y acotada, entonces, se verifica que:
$$X_n \overset{d}{\rightarrow} X \Leftrightarrow \lim\limits_{n \rightarrow +\infty} \E \big( f(X_n) \big) = \E \big( f(X) \big)$$

#### Proposición 2.40

Sean $\{X_n\}_{n \in \mathbb{N}}$ y $X$ variables aleatorias definidas en $(\Omega, \, \mathcal{A}, \, \Pr)$. Sean $a$ y $b$ dos reales tales que $a < x < b$. Si $X_n \overset{p}{\rightarrow} X$, entonces:
$$F_X(a) \leq \liminf\limits_n F_n(x) \leq \limsup\limits_n F_n(x) \leq F_X (b)$$

*Demostración*: 

Parte 1: $F_X(a) \leq \liminf\limits_n F_n(x)$

$$\begin{array}{rcl}
F_X(a) & = & \Pr (X \leq a) \\ \\
   & = & \Pr \left( \big\{ X \leq a \big\} \cap \Omega \right) \\ \\
   & = & \Pr \left( \big\{ X \leq a \big\} \cap \left( \big\{ X_n - X < x - a \big\} \cup \big\{ X_n - X \geq x - a \big\} \right) \right) \\ \\
   & = & \Pr \left[ \left( \big\{ X \leq a \big\} \cap \big\{ X_n - X < x - a \big\} \right) \cup \left( \big\{ X \leq a \big\} \cap \big\{ X_n - X \geq x - a \big\} \right) \right] \\ \\
\end{array}$$

Notemos que la unión es de conjuntos disjuntos por construncción, dado que $\big\{ X_n - X < x - a \big\}$ y $\big\{ X_n - X \geq x - a \big\}$ son complementarios (su unión da como resultado $\Omega$). Por lo tanto:
$$\begin{array}{rcl}
F_X(a) & = & \Pr \left[ \left( \big\{ X \leq a \big\} \cap \big\{ X_n - X < x - a \big\} \right) \cup \left( \big\{ X \leq a \big\} \cap \big\{ X_n - X \geq x - a \big\} \right) \right] \\ \\
   & = & \Pr \left( \big\{ X \leq a \big\} \cap \big\{ X_n - X < x - a \big\} \right) + \color{orange}{\Pr \left( \big\{ X \leq a \big\} \cap \big\{ X_n - X \geq x - a \big\} \right)} \\ \\
   & \color{orange}{\leq} & \color{blue}{ \Pr \left( \big\{ X \leq a \big\} \cap \big\{ X_n - X < x - a \big\} \right)} \color{black}{+} \color{orange}{ \Pr \big( X_n - X \geq x - a \big)} \\ \\
   & \color{blue}{\leq} & \color{blue}{\Pr \left( X_n \leq x \right)} \color{black}{ + } \color{magenta}{\Pr \big( X_n - X \geq x - a \big)} \\ \\
   & \color{magenta} \leq & \color{black} \Pr \left( X_n \leq x \right) + \color{magenta} \Pr \big( |X_n - X| \geq x - a \big) \\ \\
\end{array}$$
donde:

- en la primer desigualdad utilizamos que:
$$\underbrace{ \color{orange}{ \big\{ X \leq a \big\} } }_{A} \color{orange}{ \cap } \color{black}{ \underbrace{ \color{orange}{ \big\{ X_n - X \geq x - a \big\} } }_{B} } \color{black}{ \subset } \underbrace{ \color{orange}{ \big\{ X_n - X \geq x - a \big\} } }_{B}$$
lo cual se cumple por construcción, $(A \cap B) \subset B$.

- en la segunda desigualdad utilizamos que:
$$\color{blue} \big\{ X \leq a \big\} \cap \big\{ X_n - X < x - a \big\} \color{black} \subset  \color{blue} \big\{ X_n \leq x \big\}$$

- en la tercer desigualdad utilizamos que:
$$\color{magenta} \big\{ X_n - X \geq x - a \big\} \color{black} \subset \color{magenta} \big\{ |X_n - X| \geq x - a \big\}$$
dado que el término de la derecha podemos escribirlo como la unión de ambas colas.

Por lo tanto, obtenemos que:
$$F_X(a) \leq \underbrace{ Pr \left( X_n \leq x \right) }_{F_{X_n}(x)} + \underbrace{ \Pr \big( |X_n - X| \geq x - a \big) }_{ \substack{ \overset{n}{\rightarrow} 0 \text{, dado que } X_n \overset{p}{\rightarrow} X \\ \text{por hipótesis}}}$$
Si ahora tomamos $\liminf\limits_n$ a ambos lados obtenemos que:
$$\liminf\limits_n F_X(a) \leq \liminf\limits_n F_{X_n}(x)$$
$$F_X(a) \leq \liminf\limits_n F_{X_n}(x)$$

\newpage

Parte 2: $\liminf\limits_n F_n(x) \leq \limsup\limits_n F_n(x)$

Esta desigualdad se desprende del análisis real dado que $F_{X_n}(x)$ es una sucesión de números reales positivos (entre 0 y 1).

Parte 3: $\limsup\limits_n F_n(x) \leq F_X(b)$

Procediendo de forma análoga a la parte 1 obtenemos que:
$$\begin{array}{rcl}
F_{X_n}(x) & = & \Pr(X_n \leq x) \\ \\
   & = & \Pr \left[ \big\{ X_n \leq x \big\} \cap \Omega \right] \\ \\
   & = & \Pr \left[ \big\{ X_n \leq x \big\} \cap \big( \big\{ X - X_n \leq b - x \big\} \cup \big\{ X - X_n > b - x \big\} \big) \right] \\ \\
   & = & \Pr \left[ \left( \big\{ X_n \leq x \big\} \cap \big\{ X - X_n \leq b - x \big\} \right) \cup \left( \big\{ X_n \leq x \big\} \cap \big\{ X - X_n > b - x \big\} \right) \right] \\ \\
   & = & \Pr \left( \big\{ X_n \leq x \big\} \cap \big\{ X - X_n \leq b - x \big\} \right) + \Pr \left( \big\{ X_n \leq x \big\} \cap \big\{ X - X_n > b - x \big\} \right) \\ \\
   & \leq & \Pr \left( \big\{ X_n \leq x \big\} \cap \big\{ X - X_n \leq b - x \big\} \right) + \Pr \big( X - X_n > b - x \big) \\ \\
   & \leq & \Pr \big( X \leq b \big) + \Pr \big( X - X_n > b - x \big) \\ \\
   & \leq & \Pr \big( X \leq b \big) + \Pr \big( |X - X_n| > b - x \big) \\ \\
\end{array}$$

donde:

- en la primer desigualdad utilizamos que:
$$\big\{ X_n \leq x \big\} \cap \big\{ X - X_n > b - x \big\} \subset \big\{ X - X_n > b - x \big\}$$

- en la segunda desigualdad utilizamos que:
$$\big\{ X_n \leq x \big\} \cap \big\{ X - X_n \leq b - x \big\} \subset \big\{ X \leq b \big\}$$

- en la tercer desigualdad utilizamos que:
$$\big\{ X - X_n > b - x \big\} \subset \big\{ |X - X_n| > b - x \big\}$$

Por lo tanto, obtuvimos que:
$$F_{X_n}(x) \leq \underbrace{ \Pr \big( X \leq b \big) }_{F_X(b)} + \underbrace{ \Pr \big( |X - X_n| > b - x \big) }_{ \substack{ \overset{n}{\rightarrow} 0 \text{, dado que } X_n \overset{p}{\rightarrow} X \\ \text{por hipótesis} }} \Rightarrow F_{X_n}(x) \leq F_X(b)$$

Tomando $\limsup\limits_n$ concluimos que:
$$\limsup\limits_n F_{X_n}(x) \leq \limsup\limits_n F_X(b) \Rightarrow \limsup\limits_n F_{X_n}(x) \leq F_X(b)$$

#### Corolario 2.41

Sean $\{X_n\}_{n \in \mathbb{N}}$ y $X$ variables aleatorias definidas en $(\Omega, \, \mathcal{A}, \, \Pr)$. Sea $x$ un punto de continuidad de $F_X$. Si $X_n \overset{p}{\rightarrow} X$, entocnes $F_{X_n}(x) \overset{n}{\rightarrow} F_X(x)$.

El recíproco del corolario 2.41 no es cierto.

Si $X_n \overset{d}{\rightarrow} X$ y $Y_n \overset{d}{\rightarrow} Y$, no es cierto en general que $X_n + Y_n \overset{d}{\rightarrow} X + Y$.

#### Teorema 2.42: Teorema de representación de Skorokhod

Sean $\{X_n\}_{n \in \mathbb{N}}$ una sucesión de variables aleatorias definidas en $(\Omega_n, \, \mathcal{A}_n, \, {\Pr}_n)$ y $X$ una variable aleatoria definida en $(\Omega_0, \, \mathcal{A}_0, \, {\Pr}_{0})$ tal que $X_n \overset{d}{\rightarrow} X$. Entonces:

1. existen $X^*_n$ y $X^*$ definidas en $(\Omega, \, \mathcal{A}, \, \Pr)$

2. $X_n^* \overset{d}{=} X_n$ para todo $n$ y $X^* \overset{d}{=} X$

3. $X_n^* \overset{cs}{\rightarrow} X^*$

#### Lema 2.43: Lema dee Slutsky

Si $X_n \overset{d}{\rightarrow} X$ y $Y_n \overset{p}{\rightarrow} c \in \mathbb{R}$, entonces:

1. $X_n + Y_n \overset{d}{\rightarrow} X + c$

2. $X_n \, Y_n \overset{d}{\rightarrow} c \,X$

3. $X_n / Y_n \overset{d}{\rightarrow} X / c$ siempre que $c \neq 0$

## Métricas entre distribuciones

Definiremos distancias entre funciones de distribución $F$ y $G$ de dos variables aleatorias $X$ e $Y$ a valores reales de modo tal que la convergencia en distribución de una sucesión de variables aleatorias sea equivalente a la convergencia en dicha distancia.

### Distancia de Levy

#### Definición 2.44

Sean $F$ y $G$ dos distribuciones de probabilidad en $\mathbb{R}$. Se define la **distancia de Levy** entre $F$ y $G$ como:
$$d_L(F, G) = \inf \big\{ \varepsilon > 0 : F (x - \varepsilon) - \varepsilon \leq G(x) \leq F (x + \varepsilon) + \varepsilon \,\,\, \forall x \in \mathbb{R} \big\}$$

#### Teorema 2.45

Sean $\{F_n\}_{n \in \mathbb{N}}$ funciones de distribución de variables aleatorias a valores reales, y $F$ otra función dee distribución. Entonces, son equivalentes:

1. $F_n(x) \overset{n}{\rightarrow} F(x)$ para todo $x$ punto de continuidad de $F$

2. $d_L(F_n, \, F) \overset{n}{\rightarrow} 0$

### Distancia de Wasserstein

#### Definición 2.46

Se define la **distancia de Wasserstein** entre la función de distribución $F$ y la función de distribución $G$ como:
$$\mathcal{W}_2^2(F, \, G) = \inf \big\{ \E |X - Y|^2 : X \sim F \,\,\, \text{ y } \,\,\, Y \sim G \big\} $$

***

Se puede probar que $\mathcal{W}_2^2(F_n, \, F) \overset{n}{\rightarrow} 0 \Leftrightarrow F_n \overset{n}{\rightarrow} F$ y el segundo momento de variables con distribución $F_n$ converge al segundo momento de una variable aleatoria con distribución $F$. Es decir, $\int x^2 dF_n(x) \overset{n}{\rightarrow} \int x^2 dF(x)$.

# Convergencia en $L^p$

#### Definición 2.47

Sean $X$ y $\{X_n\}_{n \in \mathbb{N}}$ variables aleatorias definidas en el mismo espacio de probabilidad $(\Omega, \, \mathcal{A}, \, \Pr)$. Para $p > 0$, decimos que:
$$X_n \overset{L^p}{\rightarrow} X \Leftrightarrow \E |X_n - X|^p \overset{n}{\rightarrow} 0$$

- Si $X_n \overset{L^p}{\rightarrow} X$ entonces $X_n \overset{p}{\rightarrow} X$.

- El recíproco no es cierto.

#### Proposición 2.49

Si $\{X_n\}_{n \in \mathbb{N}}$ es una sucesión de variables aleatorias no negativas tales que $X_n \overset{cs}{\rightarrow} X$ y $\E(X_n) \overset{n}{\rightarrow} \E(X) < \infty$, entonces $\E |X_n - X| \overset{n}{\rightarrow} 0$.

## Desigualdades en espacios $L^p$

#### Teorema 2.50: desigualdades de variables aleatorias

- **Desigualdad de Jensen**

Si $g$ es convexa y $\E|X| < \infty$, entonces $g \big( \E(X) \big) \leq \E\big( g(X) \big)$.

- **Desigualdad de Lyapunov**

Si $0 < s < t$, entonces $\big( \E|X|^s \big)^{1/s} \leq \big( \E|X|^t \big)^{1/t}$.

- **Desigualdad de Hölder**

Sean $p$ y $q$ tales que $1 < p < \infty$ y $(1/p) + (1/q) = 1$. Si $\E|X|^p < \infty$ y $\E|Y|^q < \infty$, entonces:

   i. $\E |XY| < \infty$
   
   ii. $\E |XY| \leq \left( \E|X|^p \right)^{1/p} \left( \E|Y|^q \right)^{1/q}$

- **Desigualdad de Minkowski**

Si $\E|X|^p < \infty$ y $\E|Y|^p < \infty$, con $1 < p < \infty$, entonces:

   i. $\E |X + Y |^p < \infty$
 
   ii. $\big( \E |X + Y |^p \big)^{1/p} \leq \big( \E|X|^p \big)^{1/p} + \big( \E|Y|^p \big)^{1/p}$
