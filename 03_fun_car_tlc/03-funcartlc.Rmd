---
title: "Capítulo 3 - Funciones características y TLC"
author: "Daniel Czarnievicz"
date: "2019"
output: pdf_document
header-includes:
   - \everymath{\displaystyle}
   - \usepackage[spanish]{babel}
   - \usepackage{xcolor}
   - \usepackage[makeroom]{cancel}
   - \DeclareMathOperator*{\plim}{plim}
   - \usepackage{mathrsfs}
   - \usepackage{amsmath}
   - \usepackage{mathtools}
   - \newcommand{\equalexpl}[1]{\underset{\substack{\uparrow\\\\\mathrlap{\text{\hspace{-2em}#1}}}}{\approx}}
   - \DeclareMathOperator{\E}{\mathbf{E}}
   - \DeclareMathOperator{\V}{\mathbf{V}}
geometry: margin=1in
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

- Denotaremos como $\mathbb{C}$ al conjunto de los números complejos.  
- **Módulo**: si $z = a + ib \in \mathbb{C} \Rightarrow |z| = \sqrt{a^2 + b^2}$.  
- **Conjugado**: si $z = a + ib \in \mathbb{C} \Rightarrow \bar{z} = a - ib$.  
- **Exponencial compleja**: $e^z = e^{a + ib} = e^a \, e^{ib} = e^a \big[ \cos(b) + i \sin(b) \big]$.

# Función Característica

#### Definición 3.1: Función característica

Dado $(\Omega, \, \mathcal{A}, \, P)$ un espacio de probabilidad y $X : \Omega \rightarrow \mathbb{R}$ una variable aleatoria, definimos la función característica de $X$ como:
$$\varphi_X : \mathbb{R} \rightarrow \mathbb{C} \, / \, \varphi_X(t) = \E \big( e^{itX} \big)$$

#### Observación 3.2

Dado que $e^{itX} = \cos(tX) + i \sin(tX)$, se tiene que:
$$\varphi_X(t) = \int\limits_{-\infty}^{+\infty} e^{itx} dF_X(x) = \E \big[ \cos(tX) \big] + i \, \E \big[ \sin(tX) \big]$$
como $|e^{itX}| = 1$ para todo $t$, $\exists \varphi(t) < \infty$ para todo $t$.

#### Proposición 3.3: propiedades de la función característica

1. $\big| \varphi_X(t) \big| \leq 1$ para todo $t \in \mathbb{R}$

*Dem*: $|\varphi_X(t)| = \Big| \E \big(e^{itX} \big) \Big| \leq \E \big|e^{itX} \big| = \E (1) = 1$, dado que $g(z) = |z|$ es una función convexa.

***

2. $\varphi_X(0) = 1$

*Dem*: $\varphi_X(0) = \E \big(e^{i(0)X} \big) = \E \big( e^{0} \big) = \E (1) = 1$.

\newpage

3. $\varphi_{aX+b}(t) = e^{itb} \, \varphi_X(at)$

*Dem*: 
$$\varphi_{aX+b}(t) = \E \left( \exp\big\{ it(aX + b) \big\} \right) = \E \left( \exp\big\{ itaX + itb \big\} \right) = \E \left( \exp\big\{ itaX \big\} \exp\big\{ itb \big\} \right) =$$
$$= e^{itb} \E \Big( e^{itaX} \Big) = e^{itb} \varphi_X(ta)$$

***

4. Si $X$ e $Y$ son independientes, entonces $\varphi_{X + Y}(t) = \varphi_X (t) \, \varphi_Y (t)$ para todo $t \in \mathbb{R}$

*Dem*: 
$$\varphi_{X + Y}(t) = \E \Big[ e^{it(X+Y)} \Big] = \E \Big[ e^{itX + itY} \Big] = \E \Big[ e^{itX} e^{itY} \Big] = \E \big[ e^{itX} \big] \E \big[ e^{itY} \big] = \varphi_X(t) \, \varphi_Y(t)$$

***

5. $\varphi_X(t) = \overline{ \varphi_X (-t) }$

*Dem*: recordemos que:

- $\sin(x)$ es impar, por lo tanto, $\sin(x) = - \sin(-x)$, y $\int\limits_{\mathbb{R}} \sin(x) dx = 0$. 
- $\cos(x)$ es par, por lo tanto, $\cos(x) = \cos(-x)$, y $\int\limits_{\mathbb{R}} \cos(x) dx = 2 \int\limits_0^{+\infty} \cos(x) dx$. 

$$\overline{ \varphi_X (-t) } = \overline{ \E \big[ \cos(-tX) + i \sin(-tX) \big] } = \overline{ \E \big[ \cos(tX) - i \sin(tX) \big] } = \E \big[ \cos(tX) + i \sin(tX) \big] = \varphi_x(t)$$

***

#### Proposición 3.4: función característica de una variable aleatoria con distribución Normal

Si $X \sim \text{N}(\mu, \, \sigma^2)$, entonces $\varphi_X(t) = \exp\left\{ i t \mu - \frac{1}{2} (\sigma t)^2 \right\}$

*Dem*: primero hallamos la función característica de una distribución $\text{Normal}(0, 1)$, y luego utilizamos la propiedad 3 para hallar la de una $\text{Normal}(\mu, \sigma^2)$.

$$\varphi_X(t) = \E \big[ e^{itX} \big] = \int\limits_{-\infty}^{+\infty} e^{itx} \, \phi_X(x) dx = \int\limits_{-\infty}^{+\infty} e^{itx} \, \frac{1}{ \sqrt{2 \pi} } e^{-x^2 / 2} dx$$

$$\frac{ \partial }{ \partial t } \varphi_X (t) = \frac{ \partial }{\partial t} \int\limits_{-\infty}^{+\infty} e^{itx} \frac{1}{ \sqrt{2 \pi} } \, e^{- x^2 / 2} dx = \int\limits_{-\infty}^{+\infty} \left( \frac{ \partial }{\partial t} \, e^{itx} \right) \frac{1}{ \sqrt{2 \pi} } \, e^{- x^2 / 2} dx =$$
$$= \int\limits_{-\infty}^{+\infty} i x \, e^{itx} \frac{1}{ \sqrt{2 \pi} } \, e^{- x^2 / 2} dx = -\frac{ i }{ \sqrt{2 \pi} } \int\limits_{-\infty}^{+\infty} e^{itx} \, (-x) \, e^{- x^2 / 2} dx$$

Podemos luego entonces aplicar integración por partes donde $g(x) = e^{itx} \Rightarrow g'(x) = it \, e^{itx}$, y $f'(x) = (-x) \, e^{- x^2 / 2} \Rightarrow f(x) = e^{-x^2 / 2}$ y obtenemos que:

$$\frac{ \partial }{ \partial t } \varphi_X (t) = i \left[ \frac{ \big( -e^{-x^2 / 2} \big) \big( e^{itx} \big) }{ \sqrt{2 \pi} } \, \Bigg|_{-\infty}^{+\infty} - \int\limits_{-\infty}^{+\infty} \left( \frac{ -e^{-x^2 / 2} }{ \sqrt{2 \pi} } \right) e^{itx} (it) \, dx \right]$$
Donde el primer sumando es cero dado que $-e^{-x^2 / 2}$ tiende a cero en los límites propuestos, mientras que $e^{itx}$ está acotada, dado que podemos escribirla como una función de senos y cosenos, los cuales están acotados. Por lo tanto, obtenemos que:

$$\frac{ \partial }{ \partial t } \varphi_X (t) = - i \int\limits_{-\infty}^{+\infty} \left( \frac{ -e^{-x^2 / 2} }{ \sqrt{2 \pi} } \right) e^{itx} (it) \, dx = - t \int\limits_{-\infty}^{+\infty} \left( \frac{ -e^{-x^2 / 2} }{ \sqrt{2 \pi} } \right) e^{itx} dx = - t \, \varphi_X(t)$$

Por lo tanto, hallamos que $\varphi'_X(t) = - t \varphi_X(t)$. Esto equivale a resolver la ecuación diferencial lineal de primer orden $\varphi'_X(t) + t \varphi_X(t) = 0$. Por lo tanto, tenemos que:

$$\left. \begin{array}{l}
\varphi_X(t) = c \, e^{-t^2 / 2} \\
\varphi_X(0) = 1 \Rightarrow c = 1
\end{array} \right\} \Rightarrow \varphi_X(t) = e^{-t^2 / 2}$$

Aplicando ahora la propiedad 3, obtenemos que:
$$\varphi_{\sigma X + \mu} (t) = e^{it \mu} e^{-(\sigma t)^2 / 2} = \exp\left\{ it \mu - \frac{ (\sigma t)^2 }{2} \right\}$$

#### Proposición 3.5

$\varphi_X(t)$ es uniformemente continua.

<!--
*Dem*: observemos que:

$$\varphi_X(t) - \varphi_X(s) = \E \big( e^{itX} \big) - \E \big( e^{isX} \big) $$
-->

#### Proposición 3.6

Si $\E |X|^n < \infty$ para algún $n \geq 1$ entonces:

- Existen las derivadas de $\varphi_X$ de orden $1, \ldots, n$

- $\varphi_X^{(r)}(t) = \int\limits_{\mathbb{R}} (ix)^r e^{itx} dF(x) = i^r \, \E(X^r \, e^{itX} )$

- $i^r \, \E(X^r) = \left. \frac{ \partial^r }{ \partial^r t } \, \varphi_X(t) \right|_{t = 0} = \varphi_X^{(r)}(0)$ 

- $\varphi_X(t) = \sum\limits_{r = 0}^{n} \frac{(it)^r}{r!} \E (X^r) + \frac{(it)^n}{n!} \varepsilon_n(t)$ donde $|\varepsilon_n(t)| \leq 3 \E(|X|^n)$ y $\varepsilon_n(t) \overset{t}{\rightarrow} 0$

#### Teorema 3.7: Fórmula de inversión

sea $F = F(x)$ una función de distribución y $\varphi(t) = \int\limits_{\mathbb{R}} e^{itx} dF(x)$ su función característica, entonces:

1. Para todo par $a < b$ de puntos de continuidad de $F$ se cumple que:
$$F(b) - F(a) = \lim\limits_{c \rightarrow +\infty} \frac{1}{2 \pi} \int\limits_{-c}^{c} \frac{ e^{-ita} - e^{-itb} }{it} \varphi(t) dt$$

2. Si además, $\int\limits_{\mathbb{R}} |\varphi (t)| dt < \infty$ entonces, $F$ tiene densidad $f$ dada por:
$$f(x) = \frac{1}{2 \pi} \int\limits_{\mathbb{R}} e^{-tx} \varphi(t) dt$$

*Dem 1*: comencemos estableciendo la hoja de ruta de la demostración.

i. Del capítulo 1 (prop 1.44) sabemos que $\int\limits_a^b dF_X(x) = \Pr(a < X \leq b) = F_X(b) - F_X(a)$. Por lo tanto, la demostración de la parte 1 se reduce a demostrar que:
$$\lim\limits_{c \rightarrow +\infty} \frac{1}{2 \pi} \int\limits_{-c}^{c} \frac{ e^{-ita} - e^{-itb} }{it} \varphi(t) dt = \int\limits_a^b dF_X(x)$$

ii. Por la definición de función característica sabemos que:
$$\varphi_X(t) = \E \big( e^{itX} \big) = \int\limits_{-\infty}^{+\infty} e^{itx} dF_X(x)$$

Por lo tanto, nuestra hoja de ruta tiene los siguientes pasos a cumplir:

a. Usar la definición de función característica en 1 para obtener nuestro $dF_X(x)$ 

b. Intercambiar las integrales (requiere demostrar que estamos en las hipótesis del teorema de Fubini).

c. Resolver la integral interior (que estará en términos de $dt$ luego del intercambio del punto anterior).

d. Intercambiar el límite con la integral (es decir, poner el límite dentro de la integral).

e. Resolver el límite.

f. Resolver la segunda integral.

\newpage

Comenzamos con el paso a. Para ello simplemente remplazamos $\varphi_X(t)$ por su definición, y obtenemos:
$$\lim\limits_{c \rightarrow +\infty} \frac{1}{2 \pi} \int\limits_{-c}^{c} \frac{ e^{-ita} - e^{-itb} }{it} \varphi(t) dt = \lim\limits_{c \rightarrow +\infty} \frac{1}{2 \pi} \int\limits_{-c}^{c} \frac{ e^{-ita} - e^{-itb} }{it} \left[ \int\limits_{\mathbb{R}} e^{itx} dF(x) \right] dt = $$
$$= \lim\limits_{c \rightarrow +\infty} \frac{1}{2 \pi} \int\limits_{-c}^{c} \int\limits_{\mathbb{R}} \frac{ e^{-ita} - e^{-itb} }{it} e^{itx} dF(x) dt$$

Para el paso b debemos probar que estamos en las hipótesis del teorema de Fubini. El mismo dice que, para resolver un integral doble, podemos intercambiar las integrales, sí y solo si, la integral del valor absoluto del integrando es finita. Es decir, si el integrando está en $L^1$. Para ello procedemos de la siguiente forma:
$$\left| \frac{ e^{-ita} - e^{-itb} }{it} e^{itx} \right| = \left| \frac{ e^{-ita} - e^{-itb} }{it} \right| \underbrace{\left| e^{itx} \right|}_{ = 1} = \left| \frac{ e^{-ita} - e^{-itb} }{it} \right| = \left| \int\limits_a^b e^{itu} du \right| \leq \int\limits_a^b \underbrace{ \left| e^{itu} \right| }_{ = 1} du =$$
$$= \int\limits_a^b du = \left. u \right|_{a}^{b} = b - a < +\infty$$

Queda demostrado entonces que el valor absoluto del integrando está acotado, por lo tanto, podemos intercambiar las integrales, obteniendo:
$$\lim\limits_{c \rightarrow +\infty} \frac{1}{2 \pi} \int\limits_{-c}^{c} \int\limits_{\mathbb{R}} \frac{ e^{-ita} - e^{-itb} }{it} e^{itx} dF(x) dt = \lim\limits_{c \rightarrow +\infty} \frac{1}{2 \pi} \int\limits_{\mathbb{R}} \int\limits_{-c}^{c} \frac{ e^{-ita} - e^{-itb} }{it} e^{itx} dt \, dF(x)$$

Para el punto c de nuestra demostración debemos ahora resolver la integral "interior" (la que está en términos de $dt$). Para ello comencemos notando que podemos separar el problema en dos partes.
$$\lim\limits_{c \rightarrow +\infty} \frac{1}{2 \pi} \int\limits_{\mathbb{R}} \int\limits_{-c}^{c} \frac{ e^{-ita} }{it} \, e^{itx} dt \, dF(x) - \lim\limits_{c \rightarrow +\infty} \frac{1}{2 \pi} \int\limits_{\mathbb{R}} \int\limits_{-c}^{c} \frac{ e^{-itb} }{it} \, e^{itx} dt \, dF(x)$$

Nótese que dichas partes son análogas, por lo que solo debemos demostrar una de ellas (por ejemplo la primera). Comencemos agrupando términos dentro de la integral, y utilizando la definición de la exponencial compleja:
$$\lim\limits_{c \rightarrow +\infty} \frac{1}{2 \pi} \int\limits_{\mathbb{R}} \int\limits_{-c}^{c} \frac{ e^{-ita} }{it} \, e^{itx} dt \, dF(x) = \lim\limits_{c \rightarrow +\infty} \frac{1}{2 \pi} \int\limits_{\mathbb{R}} \int\limits_{-c}^{c} \frac{ e^{it(x-a)} }{it} dt \, dF(x) =$$
$$= \lim\limits_{c \rightarrow +\infty} \frac{1}{2 \pi} \int\limits_{\mathbb{R}} \int\limits_{-c}^{c} \frac{ \cos(t(x - a)) + i \, \sin(t(x - a)) }{it} dt \, dF(x) =$$
$$= \lim\limits_{c \rightarrow +\infty} \frac{1}{2 \pi} \int\limits_{\mathbb{R}} \int\limits_{-c}^{c} \frac{ \cos(t(x - a)) }{it} dt \, dF(x) + \lim\limits_{c \rightarrow +\infty} \frac{1}{2 \pi} \int\limits_{\mathbb{R}} \int\limits_{-c}^{c} \frac{ \sin(t(x - a)) }{t} dt \, dF(x)$$

Comenzamos con la integral del coseno. Para resolverla utilizamos el siguiente cambio de variable:

$$u = t(x - a) \Rightarrow t = \frac{u}{x - a} \Rightarrow \frac{dt}{du} = \frac{1}{ x - a } \Rightarrow dt = (x - a) du$$

$$\begin{array}{rclcrcl}
\text{Si } t = c & \Rightarrow & u = c(x - a) & & \text{Si } t = -c & \Rightarrow & u = -c(x - a)
\end{array}$$

Por lo tanto,
$$\lim\limits_{c \rightarrow +\infty} \frac{1}{2 \pi} \int\limits_{\mathbb{R}} \int\limits_{-c}^{c} \frac{ \cos(t(x - a)) }{it} dt \, dF(x) = \lim\limits_{c \rightarrow +\infty} \frac{1}{2 \pi} \int\limits_{\mathbb{R}} \int\limits_{-c(x - a)}^{c (x - a)} \frac{ \cos(u) }{iu} du \, dF(x) $$

Observemos que $\cos(u)$ es una función par, mientras que $u$ es una función impar, por lo que el cociente es impar, de donde concluimos que la integral vale 0.

Resolvemos ahora la integral del seno. Comenzamos aplicando el mismo cambio de variable que en la parte anterior para obtener:
$$\lim\limits_{c \rightarrow +\infty} \frac{1}{2 \pi} \int\limits_{\mathbb{R}} \int\limits_{-c}^{c} \frac{ \sin(t(x - a)) }{t} dt \, dF(x) = \lim\limits_{c \rightarrow +\infty} \frac{1}{2 \pi} \int\limits_{\mathbb{R}} \int\limits_{-c(x - a)}^{c (x - a)} \frac{ \sin(u) }{u} du \, dF(x)$$

Para la parte d debemos intercambiar el límite con la primer integral. Ahora, notemos que el límite que estamos tomando es cuando $c \rightarrow + \infty$, mientras que la primer integral no depende de $c$. Por lo tanto, intercambiarlos no requiere probar que estemos en las hipótesis del teorema de convergencia dominada.
$$\lim\limits_{c \rightarrow +\infty} \frac{1}{2 \pi} \int\limits_{\mathbb{R}} \int\limits_{-c(x - a)}^{c (x - a)} \frac{ \sin(u) }{u} du \, dF(x) = \frac{1}{2 \pi} \int\limits_{\mathbb{R}} \lim\limits_{c \rightarrow +\infty} \left[ \int\limits_{-c(x - a)}^{c (x - a)} \frac{ \sin(u) }{u} du \right] dF(x)$$

<!--
Los pasos e y f nos conviene resolverlos conjuntamente.Primero un comentario respecto del paso f. Supongamos que límite en cuestión tiene por resultado una constante $C$ (constante como función de $x$). En ese caso ya sabemos que, por el teorema 1.35 (ver capítulo 1), el resultado será:
$$\frac{1}{2 \pi} \int\limits_{\mathbb{R}} C dF_X(x) = \frac{C}{2 \pi} \int\limits_{\mathbb{R}} dF_X(x) = \frac{C}{2 \pi} \int\limits_{\mathbb{R}} f_X(x) dx = \frac{C}{2 \pi}$$
-->

Para resolver el paso e debemos discutir respecto de la relación entre $a$, $x$, y $b$. Para ello es conveniente recordar que hasta ahora solo estábamos trabajando con la parte del problema que dependía de $a$ y tener presentes ambas partes. Entonces, el problema completo a resolver es el siguiente:
$$\frac{1}{2 \pi} \int\limits_{\mathbb{R}} \lim\limits_{c \rightarrow +\infty} \left[ \int\limits_{-c(x - a)}^{c (x - a)} \frac{ \sin(u) }{u} du \right] dF(x) - \frac{1}{2 \pi} \int\limits_{\mathbb{R}} \lim\limits_{c \rightarrow +\infty} \left[ \int\limits_{-c(x - b)}^{c (x - b)} \frac{ \sin(u) }{u} du \right] dF(x) =$$
$$= \int\limits_{\mathbb{R}} \left( \frac{1}{2 \pi} \lim\limits_{c \rightarrow +\infty} \left[ \int\limits_{-c(x - a)}^{c (x - a)} \frac{ \sin(u) }{u} du \right] - \lim\limits_{c \rightarrow +\infty} \left[ \int\limits_{-c(x - b)}^{c (x - b)} \frac{ \sin(u) }{u} du \right] \right) dF_X(x) =$$
$$= \int\limits_{\mathbb{R}} \left( \frac{1}{2 \pi} \lim\limits_{c \rightarrow +\infty} \left[ \int\limits_{-c(x - a)}^{c (x - a)} \frac{ \sin(u) }{u} du - \int\limits_{-c(x - b)}^{c (x - b)} \frac{ \sin(u) }{u} du \right] \right) dF_X(x)$$

Supongamos, si pérdida de generalidad, que $a < b$, y discutamos en función de $x$. 

$\bullet$ Si $x < a$ entonces $x < b$, por lo que tenemos que:
$$\lim\limits_{c \rightarrow +\infty} c(x - a) = - \infty \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \lim\limits_{c \rightarrow +\infty} -c(x - a) = + \infty $$
$$\lim\limits_{c \rightarrow +\infty} c(x - b) = - \infty \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \lim\limits_{c \rightarrow +\infty} -c(x - b) = + \infty$$

Por lo que nuestro problema será entonces:
$$\frac{1}{2 \pi} \lim\limits_{c \rightarrow +\infty} \left[ \int\limits_{-c(x - a)}^{c (x - a)} \frac{ \sin(u) }{u} du - \int\limits_{-c(x - b)}^{c (x - b)} \frac{ \sin(u) }{u} du \right] = \frac{1}{2 \pi} \left[ \int\limits_{+ \infty}^{- \infty} \frac{ \sin(u) }{u} du - \int\limits_{+ \infty}^{- \infty} \frac{ \sin(u) }{u} du \right] = 0$$

$\bullet$ Si $x > b$ entonces $x > a$, por lo que tenemos que:
$$\lim\limits_{c \rightarrow +\infty} c(x - a) = + \infty \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \lim\limits_{c \rightarrow +\infty} -c(x - a) = - \infty $$
$$\lim\limits_{c \rightarrow +\infty} c(x - b) = + \infty \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \lim\limits_{c \rightarrow +\infty} -c(x - b) = - \infty$$

Por lo que nuestro problema será entonces:
$$\frac{1}{2 \pi} \lim\limits_{c \rightarrow +\infty} \left[ \int\limits_{-c(x - a)}^{c (x - a)} \frac{ \sin(u) }{u} du - \int\limits_{-c(x - b)}^{c (x - b)} \frac{ \sin(u) }{u} du \right] = \frac{1}{2 \pi} \left[ \int\limits_{- \infty}^{+ \infty} \frac{ \sin(u) }{u} du - \int\limits_{- \infty}^{+ \infty} \frac{ \sin(u) }{u} du \right] = 0$$

$\bullet$ Si $x = a$ entonces $x < b$, por lo que tenemos que:
$$\lim\limits_{c \rightarrow +\infty} c(x - a) = 0 \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \lim\limits_{c \rightarrow +\infty} -c(x - a) = 0$$
$$\lim\limits_{c \rightarrow +\infty} c(x - b) = - \infty \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \lim\limits_{c \rightarrow +\infty} -c(x - b) = + \infty$$

Por lo que nuestro problema será entonces:
$$\frac{1}{2 \pi} \lim\limits_{c \rightarrow +\infty} \left[ \int\limits_{-c(x - a)}^{c (x - a)} \frac{ \sin(u) }{u} du - \int\limits_{-c(x - b)}^{c (x - b)} \frac{ \sin(u) }{u} du \right] = \frac{1}{2 \pi} \left[ \int\limits_{0}^{0} \frac{ \sin(u) }{u} du - \int\limits_{+ \infty}^{- \infty} \frac{ \sin(u) }{u} du \right] = \frac{1}{2}$$

$\bullet$ Si $x = b$ entonces $x > a$, por lo que tenemos que:
$$\lim\limits_{c \rightarrow +\infty} c(x - a) = + \infty \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \lim\limits_{c \rightarrow +\infty} -c(x - a) = - \infty$$
$$\lim\limits_{c \rightarrow +\infty} c(x - b) = 0 \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \lim\limits_{c \rightarrow +\infty} -c(x - b) = 0$$

Por lo que nuestro problema será entonces:
$$\frac{1}{2 \pi} \lim\limits_{c \rightarrow +\infty} \left[ \int\limits_{-c(x - a)}^{c (x - a)} \frac{ \sin(u) }{u} du - \int\limits_{-c(x - b)}^{c (x - b)} \frac{ \sin(u) }{u} du \right] = \frac{1}{2 \pi} \left[ \int\limits_{- \infty}^{+ \infty} \frac{ \sin(u) }{u} du - \int\limits_{0}^{0} \frac{ \sin(u) }{u} du \right] = \frac{1}{2}$$

$\bullet$ Si $a < x < b$, tenemos que:
$$\lim\limits_{c \rightarrow +\infty} c(x - a) = + \infty \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \lim\limits_{c \rightarrow +\infty} -c(x - a) = - \infty$$
$$\lim\limits_{c \rightarrow +\infty} c(x - b) = - \infty \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \lim\limits_{c \rightarrow +\infty} -c(x - b) = + \infty$$

Por lo que nuestro problema será entonces:
$$\frac{1}{2 \pi} \lim\limits_{c \rightarrow +\infty} \left[ \int\limits_{-c(x - a)}^{c (x - a)} \frac{ \sin(u) }{u} du - \int\limits_{-c(x - b)}^{c (x - b)} \frac{ \sin(u) }{u} du \right] = \frac{1}{2 \pi} \left[ \int\limits_{- \infty}^{+ \infty} \frac{ \sin(u) }{u} du - \int\limits_{+ \infty}^{- \infty} \frac{ \sin(u) }{u} du \right] = 1$$

Por lo tanto, obtuvimos que:
$$\Psi(x) = \frac{1}{2 \pi} \lim\limits_{c \rightarrow +\infty} \left[ \int\limits_{-c(x - a)}^{c (x - a)} \frac{ \sin(u) }{u} du - \int\limits_{-c(x - b)}^{c (x - b)} \frac{ \sin(u) }{u} du \right] = \left\{ 
\begin{array}{ccc}
0 & \text{si} & x < a \text{ ó } x > b \\
1/2 & \text{si} & x = a \text{ ó } x = b \\
1 & \text{si} & a < x < b
\end{array}
\right.$$

Por lo que $\Psi(x)$ no es otra más que la función del ejercicio 5 del práctico 1. Utilizando el resultado de dicho ejercicio, sabemos que la parte f de nuestra demostración no es más que:
$$\int\limits_{\mathbb{R}} \Psi(x) d F_X(x) = F_X(b) - F_X(a)$$

*Dem 2*: para probar la parte 2 del teorema debemos primero tener claro lo siguiente: el teorema nos da la "solución", lo que nosotros debemos demostrar es que dicha solución se corresponde con la función de densidad de la variable aleatoria cuyas funciones de distribución y característica están dadas en las hipótesis del teorema (es decir, $F$ y $\varphi$). 

Para probar que $f(x) = \frac{1}{2\pi} \int\limits_{\mathbb{R}} e^{-itx} \varphi_X(t) dt$ es la función de densidad de una variable aleatoria cuya función de distribución es $F$, un posible camino (el que utilizaremos), será integrar $f$ en $[a, b]$ y probar que el resultado de dicha integral es $F(b) - F(a)$ (tal como debe suceder si $f$ es la función de densidad de una variable aleatoria con función de distribución $F$).

Para empezar aplicaremos el Teorema Fundamental del Cálculo. El mismo establece que: si $f : [a, b] \rightarrow \mathbb{R}$ es una función continua en $[a, b]$ y $F$ una función definida para todo $x \in [a, b]$ como $F(x) = \int\limits_a^x f(u) du$, entonces $F$ es uniformemente continua en $[a,b]$, y diferenciable en $(a,b)$ siendo su derivada $F'(x) = f(x)$. Del mismo se desprende que $\int\limits_a^b f(u)du = F(b) - F(a)$ si $f$ es continua en todo el intervalo $[a, b]$.

De esta forma, la primer parte de nuestra prueba es demostrar que el integrando de $f$ es continuo. Para ello debemos hacer uso de la hipótesis adicional de la segunda parte del teorma: $\int\limits_{\mathbb{R}} |\varphi(t)| dt < \infty$. Ahora, consideremos una sucesión $x_n \overset{n}{\rightarrow} x$, entonces:
$$e^{-itx_n} \varphi(t) \overset{n}{\rightarrow} e^{-itx} \varphi(t)$$
dado que $e^{-itx_n}$ está acotada (dado que puede expresarse como la suma de $\cos$ y $\sin$), y $\varphi(t)$ es uniformemente continua. Para establecer la convergencia utilizamos tambien que $|e^{-itx_n}| = 1$. Dado que el integrando es continuo podemos utilizar el Teorema Fundamental del Cálculo.
$$\begin{array}{rclcl}
\int\limits_a^{b} f(x) dx & = & \int\limits_{a}^{b} \frac{1}{2\pi} \left( \int\limits_{t \in \mathbb{R}} e^{-itx} \varphi(t) dt \right) dx & & \text{por el teorema fundamental del cálculo} \\
   & = & \int\limits_{t \in \mathbb{R}} \frac{1}{2\pi} \varphi(t) \left( \int\limits_{a}^{b} e^{-itx} dx \right) dt & & \text{por el teorema de Fubini} \\
   & = & \int\limits_{t \in \mathbb{R}} \frac{1}{2\pi} \varphi(t) \left( \left. \frac{e^{-itx}}{-it} \right|_a^b \right) dt \\
   & = & \int\limits_{t \in \mathbb{R}} \frac{1}{2\pi} \varphi(t) \left( \frac{e^{-ita} - e^{-itb} }{it} \right) dt \\
   & = & \lim\limits_{c \rightarrow + \infty} \frac{1}{2\pi} \int\limits_{-c}^{c} \frac{e^{-ita} - e^{-itb} }{it} \varphi(t) dt \\
   & = & F(b) - F(a)
\end{array}$$

donde la última desigualdad se desprende de utilizar la parte 1 del teorema, la cual nos asegura que la misma se cumple para todo $a$ y $b$ de continuidad de $F$. Adicionalmente, dado que $F$ es una función de densidad, $f \geq 0$. Por lo tanto, hemos probado que $f$ es una función tal que:

1. $f$ es continua
2. $F'(x) = f(x)$
3. $f \geq 0$

Por lo tanto, $f$ es la función de densidad de la variable aleatoria cuuyas funciones de distribución y características vienen dadas por $F$ y $\varphi$.

#### Corolario 3.8

Si $F$ y $G$ son distribuciones tales que $\int\limits_{\mathbb{R}} e^{itx} dF(x) = \int\limits_{\mathbb{R}} e^{itx} dG(x)$ para todo $t \in \mathbb{R} \Rightarrow F = G$. 

#### Definición 3.9: Función característica en general

Dado $(\Omega, \, \mathcal{A}, \, P)$ un espacio de probabilidad y $X : \Omega \rightarrow H$ siendo $H$ un espacio vectorial con un producto interno $\langle \cdot \, , \,  \cdot \rangle$ definimos la función característica como:
$$\varphi_X : H \rightarrow \mathbb{C} / \varphi_X (t) = \E \big( e^{i \langle t, X \rangle } \big)$$

#### Teorema 3.10

Una condición necesaria y suficiente para que las componentes del vector $\xi = (\xi_1, \ldots, \xi_d)'$ sean independientes es que la función característica de $\xi$ sea el producto de las funciones características de las componentes $\xi_i$.
$$\E \left[ \exp\big\{ i ( t_1 \xi_1 + \ldots + t_d \xi_d ) \big\} \right] = \prod\limits_{k = 1}^{d} \E \left[ \exp\big\{ i t_k \xi_k \big\} \right] $$

*Dem*:

$(\Rightarrow)$
$$\E \left[ e^{i(t_1X_1 + \ldots + t_d X_d)} \right] = \E \left[ e^{it_1X_1} \ldots e^{i t_d X_d} \right] = \E \left[ e^{it_1X_1} \right] \ldots \E \left[ e^{i t_d X_d} \right] =$$
$$= \varphi_{X_1}(t_1) \ldots \varphi{X_d}(t_d) = \prod\limits_{j = 1}^{d} \varphi_{X_j}(t_j)$$

$(\Leftarrow)$ Por un lado sabemos, por la hipótesis del teorema, que se cumple lo siguiente:
$$\varphi_X(t) = \prod\limits_{j = 1}^{d} \varphi_{X_j}(t_j) = \int\limits_{\mathbb{R}^d} e^{i(t_1 X_1 + \ldots + t_d X_d)} dF(\mathbf{x}) $$

Por otro lado, aplicando el teorema de Fubini, tenemos que:
$$\int\limits_{\mathbb{R}^d} e^{i(t_1 X_1 + \ldots + t_d X_d)} dG = \int\limits_{\mathbb{R}} e^{it_1 X_1} dF_1\int\limits_{\mathbb{R}}  \ldots \int\limits_{\mathbb{R}} e^{ i t_d X_d } dF_d = \prod\limits_{j = 1}^{d} \varphi_{x_j}(t_j)$$
donde $G = \prod\limits_{j = 1}^{d} F_j$ siendo $F_j$ la $j$-ésima componente de $F : \mathbb{R}^d \rightarrow [0, 1]$. Pero si $G$ puede expresarse como la productoria de sus marginales, entonces las componentes de $G$ son independientes entre sí.

#### Teorema 3.11

Dada $\{X_n\}_{n \in \mathbb{N}}$ una sucesión de variables aleatorias definidas en $(\Omega_n, \, \mathcal{A}_n, \, P_n)$ y una variable aleatoria $X$ definida en $(\Omega, \, \mathcal{A}, \, P)$, son equivalentes:

1. $X_n \overset{d}{\rightarrow} X$

2. $\varphi_{X_n} (t) \overset{n}{\rightarrow} \varphi_X(t)$ para todo $t \in \mathbb{R}$

\newpage

# Teorema del Límite Central

## Variables iid

#### Teorema 3.12: Teorema de Levy

Sean $X_1; \, \ldots; \, X_n$ iid con función de distribución $F$ tal que $\E(X_1) = \mu$ y $\V(X_1) = \sigma^2$. Si denotamos a $S_n = \sum\limits_{i = 1}^{n} X_i$, entonces:
$$\frac{ S_n - n \, \mu }{ \sigma \sqrt{n} } \overset{d}{\rightarrow} Z \sim \text{N}(0, 1)$$

*Dem*: para facilitar notación, llamemos $Z_n = \frac{ S_n - n \, \mu }{ \sigma \sqrt{n} }$. Sean las variables $Y_i$ tales que:

- $Y_i = \frac{ X_i - \mu }{ \sigma }$.

- $\E(Y_i) = \E \left[ \frac{ X_i - \mu }{ \sigma } \right] = \frac{1}{\sigma} \left[ \E(X_i) - \mu \right] = 0$

- $\V(Y_i) = \V \left[ \frac{ X_i - \mu }{ \sigma } \right] = \frac{1}{\sigma^2} \V(X_i) = 1$

- $Z_n = \frac{ S_n - n \, \mu }{ \sigma \sqrt{n} } = \frac{ \sum\limits_{i = 1}^{n} X_i - n \, \mu }{ \sigma \sqrt{n} } = \frac{ \sum\limits_{i = 1}^{n} ( X_i - \mu ) }{ \sigma \sqrt{n} } = \frac{1}{\sqrt{n}} \sum\limits_{i = 1}^{n} \left( \frac{ X_i - \mu }{ \sigma } \right) = \frac{1}{\sqrt{n}} \sum\limits_{i = 1}^{n} Y_i$

Luego entonces,

$$\begin{array}{rclcl}
\varphi_{Z_n}(t) & = & \E \big( e^{itZ_n} \big) & & \\ \\
   & = & \E \left( \exp\left\{ it \frac{1}{\sqrt{n}} \sum\limits_{i = 1}^{n} Y_i \right\} \right) & & \\ \\
   & = & \E \left( \exp\left\{ it \frac{1}{\sqrt{n}} Y_1 \right\} \right) \ldots \E \left( \exp\left\{ it \frac{1}{\sqrt{n}} Y_n \right\} \right) & & \text{por independencia} \\ \\
   & = & \varphi_{Y_1} \left( ^t/\!_{\sqrt{n}} \right) \ldots \varphi_{Y_n} \left( ^t/\!_{\sqrt{n}} \right) & & \text{por definición de función característica} \\ \\
   & = & \left[ \varphi_{Y_1} \left( ^t/\!_{\sqrt{n}} \right) \right]^n & & \text{por igual distribución}
\end{array}$$

Procedemos desarrollando el polinomio de Taylor de segundo orden (dado que sabemos que tiene derivada segunda por tener segundo momento centrado finito) en torno a $t = 0$, por lo que obtenemos que:
$$\left[ \varphi_{Y_1} \left( ^t/\!_{\sqrt{n}} \right) \right]^n = \left[ \varphi_{Y_1} (0) + \varphi'_{Y_1} (0) \frac{^t/\!_{\sqrt{n}} }{1!} + \varphi''_{Y_1} (0) \frac{ (^t/\!_{\sqrt{n}} )^2 }{ 2! } + \varepsilon_2 \left( \frac{t_1}{ \sqrt{n} } \right) \frac{ (^t/\!_{\sqrt{n}})^2 }{ 2! } \right]^n \,\,\,\,\,\,\,\, \text{con } 0 \leq t_1 \leq t$$

Luego $\varphi_{Y_1} (0) = 1$ por prop 3.3/2. Por otro lado, siguiendo la prop 3.6, obtenemos que $\varphi_{Y_1}'(0) = 0$ dado que $Y_1$ es una variable aleatoria centrada en 0, mientras que $\varphi''_{Y_1} (0) = -1$ dado que $\V(Y_1) = 1$. Por lo tanto,
$$\left[ \varphi_{Y_1} \left( ^t/\!_{\sqrt{n}} \right) \right]^n = \left[ 1 - \frac{ t^2 }{ 2 n } + \frac{ t^2 }{ 2n } \, \varepsilon_2 \left( \frac{t_1}{ \sqrt{n} } \right) \right]^n$$

Si tomamos el límite cuando $n \rightarrow +\infty$ hallamos que:
$$\lim\limits_{n \rightarrow +\infty} \varphi_{Z_n}(t) = \lim\limits_{n \rightarrow +\infty} \left[ \varphi_{Y_1} \left( ^t/\!_{\sqrt{n}} \right) \right]^n = \lim\limits_{n \rightarrow +\infty} \left[ 1 - \frac{ t^2 }{ 2 n } + \frac{ t^2 }{ 2n } \, \varepsilon_2 \left( \frac{t_1}{ \sqrt{n} } \right) \right]^n = e^{ -t^2 /2 } = \varphi_Z(t)$$

#### Teorema 3.13: Desigualdad de Berry y Esseen

Si $X_1, \, \ldots, \, X_n$ son variables aleatorias iid con $\E(X_1) = 0$, $\V(X_1) = \sigma^2 < \infty$, y $\E(|X_1|^3) < \infty$, entonces:
$$\sup\limits_{x \in \mathbb{R}} \big| F_n(x) - \Phi(x) \big| \leq C \, \frac{ \E( |X_1|^3) }{ \sigma^3 \sqrt{n} } $$

donde:

- $F_n = \Pr (S_n \leq x)$

- $S_n = \frac{X_1 + \ldots + X_n}{ \sigma \sqrt{n} }$

- $\frac{1}{\sqrt{2 \pi}} \leq C < 0.8$

## Arreglos triangulares

Un arreglo triangular es un conjunto de variables aleatorias tales que en cada fila las variables son independientes (no necesariamente igualmente distribuidas). No tiene porqué existir independencia entre las filas, solo dentro de ellas.

$$\begin{array}{ccccc}
X_{11} \\
X_{21} & X_{22} \\
X_{31} & X_{32} & X_{33} \\
\vdots & \vdots & \vdots & \ddots \\
X_{n1} & X_{n2} & X_{n3} & \cdots & X_{nn}
\end{array}$$

Para lo que sigue consideraremos siempre un arreglo triangular tal que:

1. $\E(X_{nk}) = 0$

2. $\E(X_{nk}^2) = \sigma_{nk}^2 > 0$

3. $\sum\limits_{k = 1}^{n} \sigma_{nk}^2 = 1$

Y denotaremos:

1. $S_n = X_{n1} + \ldots + X_{nn}$ (es decir, $S_n$ es la suma de la $n$-ésima fila).

2. $F_{nk}(x) = \Pr ( X_{nk} \leq x )$ (es decir, $F_{nk}(x)$ es la función de distribución de probabilidad de la $k$-ésima variable aleatoria de la $n$-ésima fila del arreglo).

3. $\Phi(x) = \Pr( \text{N}(0,1) \leq x)$

4. $\Phi_{nk}(x) = \Pr( \text{N}(0, \sigma_{nk}^2) \leq x) = \Phi(x/\sigma_{nk})$

#### Teorema 3.14: Condición necesaria y suficiente de Lindeberg $(\Lambda)$

$$(\Lambda) \,\,\,\,\,\,\,\, S_n \overset{d}{\rightarrow} Z \sim \text{N}(0,1) \Leftrightarrow \sum\limits_{k = 1}^{n} \int\limits_{|x| > \varepsilon} |x| \, \big| F_{nk}(x) - \Phi_{nk}(x) \big| dx \overset{n}{\rightarrow} 0$$

Para demostrar la el teorema de Lindeberg se hará uso de los siguientes dos lemas.

#### Lema 3.15

$$\left| e^{ix} - \sum\limits_{k = 0}^{n - 1} \frac{ (ix)^k }{ k! } \right| \leq \frac{ |x|^n }{ n! }$$

*Dem*: para demostrar la desigualdad utilizaremos el siguiente desarrollo de Taylor respecto de $x = 0$ para funciones infinitamente derivables:

$$f(x) = \sum\limits_{k = 0}^{+\infty} f^{(k)}(0) \frac{x^k}{k!} = \sum\limits_{k = 0}^{n - 1} f^{(k)}(0) \frac{x^k}{k!} + f^{(n)}(y) \frac{x^n}{n!}$$
donde $0 \leq y \leq x$ y $f^{(k)}$ es la $k$-ésima derivada de $f$ respecto de $x$. Luego entonces:
$$\left| e^{ix} - \sum\limits_{k = 0}^{n - 1} \frac{ (ix)^k }{ k! } \right| = \Bigg| \sum\limits_{k = 0}^{n - 1} \left. \left( \frac{ \partial^{k} }{ \partial^k x } e^{ix} \right) \right|_{x = 0} \frac{x^k}{k!} + \left. \left( \frac{\partial^{n}}{\partial^n x} e^{ix} \right) \right|_{x = y} \, \frac{x^n}{n!} - \sum\limits_{k = 0}^{n - 1} \frac{ (ix)^k }{ k! } \Bigg| =$$
$$= \Bigg| \sum\limits_{k = 0}^{n - 1} i^k \left. e^{ix} \right|_{x = 0} \frac{x^k}{k!} + i^n \left. e^{ix} \right|_{x = y} \, \frac{x^n}{n!} - \sum\limits_{k = 0}^{n - 1} \frac{ (ix)^k }{ k! } \Bigg| = \Bigg| \sum\limits_{k = 0}^{n - 1} \frac{(ix)^k}{k!} + e^{iy} \frac{ (ix)^n }{ n! } - \sum\limits_{k = 0}^{n - 1} \frac{ (ix)^k }{ k! } \Bigg| =$$
$$ = \left| e^{iy} \frac{ (ix)^n }{ n! } \right| = \left| e^{iy} \right| \, \left| \frac{ (ix)^n }{ n! } \right| = \left| \frac{ x^n }{ n! } \right| = \frac{|x^n|}{n!}$$

\newpage

#### Lema 3.16

Si $E(|X|^n) < \infty$, entonces $x^n \big( F(x) - F(-x) -1 \big) \overset{x}{\rightarrow} 0$ siendo $F$ la función de distribución de $X$.

*Dem*: el lema implíca resolver un límite indeterminado de la forma $+\infty$ por $0$:

$$\lim\limits_{x \rightarrow + \infty} \underbrace{ x^n }_{ \rightarrow + \infty } \underbrace{ \big( F(x) - F(-x) -1 \big) }_{ \rightarrow 0 }$$

Para levantar la indeterminación comenzamos operando con el segundo término:
$$\begin{array}{rcl}
F(x) - F(-x) - 1 & = & \Pr(X \leq x) - \Pr(X \leq -x) - 1 \\
   & = & \Pr(X \leq x) - \Pr(-X \geq x) - 1 \\
   & = & -\Pr(-X \geq x) - \big[ 1 - \Pr(X \leq x) \big] \\
   & = & - \Pr(-X \geq x) - \Pr(X > x) \\
   & = & - \big[ \Pr(-X \geq x) + \Pr(X > x) \big]
\end{array}$$

Multiplicando por $-1$ de ambos lados de la igualdad obtenemos que:
$$\begin{array}{rcl}
-F(x) + F(-x) + 1 & = & \Pr(-X \geq x) + \Pr(X > x) \\
1 -F(x) + F(-x) & = & \Pr(-X \geq x) + \Pr(X > x) \\
   & \leq & \Pr(-X \geq x) + \Pr(x \geq x) \\
   & = & \Pr(|X| \geq x)
\end{array}$$

Por lo tanto, sabemos que $x^n \big( 1 - F(x) + F(-x) \big) \leq x^n \Pr(|X| \geq x)$ si $x > 0$. Esta restricción no son afecta dado que tomaremos el límite cuando $x \rightarrow +\infty$. Trabajando ahora con $\E|X|^n$ obtenemos que:
$$\begin{array}{rcl}
\E(|X|^n) & = & \E \big( |X|^n \mathbb{I}_{ \{ |X| \geq x \} } + |X|^n \mathbb{I}_{ \{ |X| < x \} } \big) \\
   & = & \E \big( |X|^n \mathbb{I}_{ \{ |X| \geq x \} } \big) + \E \big( |X|^n \mathbb{I}_{ \{ |X| < x \} } \big)  \\
   & < & +\infty
\end{array}$$
donde la cota superiror del último paso está proporcionada por las hipótesis del lema. Luego si $E|X|^n < \infty$, entonces ambos sumandos deben estar acotados superiormente, y podemos trabajar con cualquiera de ellos. Así, obtenemos que:
$$\begin{array}{rcl}
|x|^n \Pr(|X| \geq x) & = & |x|^n \E \left( \mathbb{I}_{ \{ |X| \geq x\} } \right) \\
   & = & \E \left( |x|^n \, \mathbb{I}_{ \{ |X| \geq x\} } \right) \\
   & \leq & \E \left( |X|^n \, \mathbb{I}_{ \{ |X| \geq x\} } \right) \\
   & \leq & +\infty
\end{array}$$

Hemos demostrado entonces que $|x|^n \Pr(|X| \geq x)$ está acotado superiormente. Ahora tenemos todos los insumos para resolver el límite:
$$\begin{array}{rcl}
\lim\limits_{x \rightarrow + \infty} x^n \big( F(x) - F(-x) -1 \big)  & \leq & \lim\limits_{x \rightarrow + \infty} x^n \Pr(|X| \geq x) \\
   & \leq & \lim\limits_{x \rightarrow + \infty} \E \big( |X|^n \, \mathbb{I}_{ \{ |X| \geq x \} } \big) \\
   & = & \E \left( \lim\limits_{x \rightarrow + \infty} |X|^n \, \mathbb{I}_{ \{ |X| \geq x \} } \right) \\
   & = & 0
\end{array}$$
donde en el tercer paso utilizamos convergencia dominada. Dicha expresión tiende a cero porque la indicatriz tiende a cero cuando $x \rightarrow + \infty$.

#### Teorema 3.18: Condición **(L)** de Lindeberg

Sean $X_{1n}, \, \ldots, \, X_{nn}$ v.a.s independientes tales que $\E(X_{nk}) = m_{nk}$, y $\V(X_{nk}) = \sigma_{nk}^2 < \infty$. Denotemos $S_n = \sum\limits_{k = 1}^{n} X_{nk}$ y $V^2_n = \sum\limits_{k = 1}^{n} \sigma_{nk}^2$. Si se cumple la siguiente condición de Lindeberg (condición **L**):
$$\mathbf{(L)} \,\,\,\,\,\,\, \varepsilon > 0, \, \lim\limits_{n \rightarrow + \infty} \frac{1}{V_n^2} \sum\limits_{k = 1}^{n} \E \big[ ( X_{nk} - m_{nk} )^2 \, \mathbb{I}_{ \{ |X_{nk} - m_{nk} | > \varepsilon V_n  \} } \big] = 0$$
entonces:
$$\frac{1}{V_n} \sum\limits_{k = 1}^{n} (X_{nk} - m_{nk}) \overset{d}{\rightarrow} Z \sim \text{N}(0, 1)$$

#### Definición 3.19: Condición de Lyapunov

Sean $X_1, \, \ldots, \, X_n$ variables aleatorias independientes con segundo momento finito. Sean $m_{k} = \E(X_k)$, $\sigma_k^2 = \V(X_k)$, $S_n = X_1 + \ldots + X_n$, y $V_n^2 = \sum\limits_{k = 1}^{n} \sigma_k^2$. Decimos que las variables $X_i$ verifican la condición **(LY)** de Lyapunov si $\exists \delta > 0$ tal que:
$$\lim\limits_{n \rightarrow + \infty} \frac{1}{V_n^{2 + \delta}} \sum\limits_{k = 1}^{n} \E \big| X_k - m_k \big|^{2 + \delta} = 0$$

Luego si la sucesión de variables aleatorias cumple la condición **LY**, entonces $\frac{1}{V_n^2} \sum\limits_{k = 1}^{n} |X_{k} - m_{k}| \overset{d}{\rightarrow} Z \sim \text{N}(0, \, 1)$.
