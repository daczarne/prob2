---
title: "Práctico 3"
author: "Daniel Czarnievicz"
date: "2019"
output: pdf_document
header-includes:
   - \everymath{\displaystyle}
   - \usepackage{xcolor}
   - \usepackage[makeroom]{cancel}
   - \DeclareMathOperator*{\plim}{plim}
   - \usepackage{mathrsfs}
   - \usepackage{amsmath}
   - \usepackage{mathtools}
   - \newcommand{\equalexpl}[1]{\underset{\substack{\uparrow\\\\\mathrlap{\text{\hspace{-2em}#1}}}}{\approx}}
geometry: margin=1in
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Ejercicio 1

No se cumple. Para demostrarlo, tomemos el siguiente contraejemplo.

***

Sea $\{X_n\}_{n \in \mathbb{N}}$ una sucesión de variables aleatorias tales que:
$$X_n = \left\{ \begin{array}{c c r}
n^2 & \text{ con probabilidad } & 1/n^2 \\
0 & \text{ con probabilidad } & 1 - 2/n^2 \\
-n^2 & \text{ con probabilidad } & 1/n^2
\end{array} \right.$$

Cakulemos entonces su esperanza, y la esperanza de su valor absoluto.

$$E(X_n) = \sum\limits_{ x \in Rec(X_n) } x \, P(X_n = x) = n^2 \left( \frac{1}{n^2} \right) + 0 \left( 1 - \frac{2}{n^2} \right) + (-n^2) \left( \frac{1}{n^2} \right) = 1 - 1 = 0$$
$$E(|X_n|) = \sum\limits_{ x \in Rec(X_n) } |x| \, P(X_n = x) = |n^2| \left( \frac{1}{n^2} \right) + |0| \left( 1 - \frac{2}{n^2} \right) + |-n^2| \left( \frac{1}{n^2} \right) = 1 + 1 = 2$$

Veamos que la sucesión converge casi seguramente a cero.
$$\sum\limits_{n = 1}^{\infty} P(|X_n - 0| > \varepsilon) = \sum\limits_{n = 1}^{\infty} P(|X_n| > \varepsilon) = \sum\limits_{n = 1}^{\infty} P(|X_n| > 0) =$$
$$P(X_n = n^2) + P(X_n = -n^2) = \frac{2}{n^2} \overset{n}{\rightarrow} 0 \Rightarrow X_n \overset{cs}{\rightarrow} 0$$

Pero $E|X_n - 0| = E|X_n| = 2 \neq 0 = E(X)$, por lo que no se cumple la tésis del teorema si la sucesión es de variables no negativas.

\newpage

# Ejercicio 2

## Parte a: $X_n \overset{d}{\rightarrow} c \in \mathbb{R} \Leftrightarrow X_n \overset{p}{\rightarrow} c$

$(\Rightarrow)$ Si $X_n \overset{d}{\rightarrow} c$ entonces sabemos que $F_{X_n}(x) = F_c(x)$ para todo $x \in \mathscr{C}(F_c)$. Luego:

$$P(|X_n - c| > \varepsilon) = 1 - P(|X_n - c| \leq \varepsilon) = $$
$$1 - P( -\varepsilon \leq X_n - c \leq \varepsilon) = 1 - P( c - \varepsilon \leq X_n \leq c + \varepsilon) =$$
$$= 1 - \big[ \underbrace{ F_{X_n}(\underbrace{ c + \varepsilon }_{> c}) }_{= 1} - \underbrace{ F_{X_n}(\underbrace{ c - \varepsilon }_{< c}) }_{ = 0} \big] = 1 - 1 = 0 \Rightarrow X_n \overset{p}{\rightarrow} c$$

$(\Leftarrow)$ Si $X_n \overset{p}{\rightarrow} c$ entonces sabemos que $X_n \overset{d}{\rightarrow} c$ siempre.

## Parte b: si $X_n \overset{d}{\rightarrow} X$ y $Y_n \overset{d}{\rightarrow} c \in \mathbb{R}$ entonces $X_n + Y_n \overset{d}{\rightarrow} X + c$

Utilizaremos los siguientes cuatro postulados:

i. Si $Y_n \overset{d}{\rightarrow} c \Rightarrow Y_n \overset{p}{\rightarrow} c$ (esto no es cierto en general, pero sí cuando la convergencia es a una constante).

ii. Si $|Y_n - X_n| \overset{p}{\rightarrow} 0$ y $X_n \overset{d}{\rightarrow} X \Rightarrow Y_n \overset{d}{\rightarrow} X$.

iii. Si $X_n \overset{d}{\rightarrow} X$ y $Y_n \overset{p}{\rightarrow} c \Rightarrow (X_n, \, Y_n) \overset{d}{\rightarrow} (X, \, c)$.

iv. Si $X_n \overset{d}{\rightarrow} X$ y $g$ es una función continua, entonces $g(X_n) \overset{d}{\rightarrow} g(X)$.

***

*Dem i*: demostrado en la parte a del ejercicio.

***

*Dem ii*: consideremos una función $f$ acotada y Lipschitz continua\footnote{Dados dos espacios métricos $(X, d_X)$ e $(Y, d_Y)$ decimos que una función $f : X \rightarrow Y$ es Lipschitz continua si $\exists K \geq 0$ tal que, para todo $x_1, x_2 \in X$, se cumpla que $d_Y(f(x_1), f(x_2)) \leq K d_X(x_1, x_2)$. Esto implíca que para todo par de puntos sobre la gráfica de $f$, el valor absoluto de la pendiente del segmento de recta que conecta dichos puntos es mejor que $K$. En este sentido, la continuidad de Lipschitz es una cota a cuán rápido permitimos que la función cambie (crezca o decrezca). }. Entonces, $\exists K > 0$, tal que $\forall x,y$ se cumple que $|f(x) - f(y)| \leq K |x - y|$. Luego, tomemos un $\varepsilon > 0$ y acotemos la siguiente expresión:

$$\Big| E \big[ f(Y_n) \big] - E \big[ f(X_n) \big] \Big| = \Big| E \big[ f(Y_n) - f(X_n) \big] \Big| \leq E \Big[ \big| f(Y_n) - f(X_n) \big| \Big] = $$
$$= E \Big[ \big| f(Y_n) - f(X_n) \big| \mathbb{I}_{ \{ |Y_n - X_n| < \varepsilon \} } \Big] + E \Big[ \big| f(Y_n) - f(X_n) \big| \mathbb{I}_{ \{ |Y_n - X_n| \geq \varepsilon \} } \Big] \leq$$
$$ \leq E \Big[ K |Y_n - X_n| \mathbb{I}_{ \{ |Y_n - X_n| < \varepsilon \} } \Big] + E \Big[ 2 \, M \mathbb{I}_{ \{ |Y_n - X_n| \geq \varepsilon \} } \Big] \leq$$
$$\leq E \Big[ K \, \varepsilon \, \mathbb{I}_{ \{ |Y_n - X_n| < \varepsilon \} } \Big] + E \Big[ 2 \, M \mathbb{I}_{ \{ |Y_n - X_n| \geq \varepsilon \} } \Big] = $$
$$= K \, \varepsilon \, E \Big[ \mathbb{I}_{ \{ |Y_n - X_n| < \varepsilon \} } \Big] + 2 \, M \, E \Big[ \mathbb{I}_{ \{ |Y_n - X_n| \geq \varepsilon \} } \Big] = $$
$$= K \, \varepsilon \, P \big( |Y_n - X_n| < \varepsilon \big) + 2 \, M \, P \big( |Y_n - X_n| \geq \varepsilon \big) \leq $$
$$ \leq K \, \varepsilon + 2 \, M \, P \big( |Y_n - X_n| \geq \varepsilon \big)$$

Donde la primer desigualdad se cumple por la desigualdad de Jensen. La segunda desigualdad se cumple porque para el primer sumando utilizamos la continuidad de Lipschitz, y para el segundo el hecho de que $f$ es una función acotada por $M$. En el primer término de la tercer desigualdad acotamos a $|Y_n - X_n|$ por $\varepsilon$ dado que ese el conjunto que determina la indicatriz. Por último, en la cuarta desigualdad simplemente acotamos la probabilidad del suceso $\{ |Y_n - X_n| < \varepsilon \}$ por 1 (lo cual es válido dado que toda probabilidad está acotada superiormente por 1).

El objetivo ahora es utilizar las definiciones alternativas de la convergencia en distribución que nos brinda el lema de Portmanteau para probar la tésis. El mismo establece que $Y_n$ convergerá en distribución a $X$, sí y solo sí $E \big[ f(Y_n) \big] \overset{n}{\rightarrow} E \big[ f(X) \big]$ para toda $f$ continua y acotada. Procedemos de la siguiente manera:

$$\Big| E \big[ f(Y_n) \big] - E \big[ f(X) \big] \Big| = \Big| E \big[ f(Y_n) \big] - E \big[ f(X_n) \big] + E \big[ f(X_n) \big] - E \big[ f(X) \big] \Big| \leq$$
$$\leq \Big| E \big[ f(Y_n) \big] - E \big[ f(X_n) \big] \Big| + \Big| E \big[ f(X_n) \big] - E \big[ f(X) \big] \Big| \leq$$
$$\leq K \, \varepsilon + 2 \, M \, \underbrace{ P \big( |Y_n - X_n| \geq \varepsilon \big) }_{ \overset{n}{\rightarrow} 0 \text{ dado que } |Y_n - X_n| \overset{p}{\rightarrow} 0 } + \Big| \underbrace{ E \big[ f(X_n) \big] - E \big[ f(X) \big] }_{ \overset{n}{\rightarrow} 0 \text{ dado que } X_n \overset{d}{\rightarrow} X } \Big| = K \, \varepsilon \overset{n}{\rightarrow} 0$$

Donde la primer desigualdad se cumple por desigualdad triangular y la segunda desigualdad se cumple utilizando la cota hallada en el paso anterior. 

Por lo tanto, queda probado que $E \big[ f(Y_n) \big] \overset{n}{\rightarrow} E \big[ f(X) \big]$, de donde se desprende, por el lema de Portmanteau que $Y_n \overset{n}{\rightarrow} X$.

***

*Dem iii*: primero queremos demostrar que $(X_n, c) \overset{d}{\rightarrow} (X, c)$. Por el teorema 2.39 (lema de Portmanteau) sabemos que $(X_n, c) \overset{d}{\rightarrow} (X, c) \Leftrightarrow E \big[f(X_n, \, c)\big] \overset{n}{\rightarrow} E \big[f(X, \, c)\big]$ para toda $f(x,y)$ continua y acotada.

\vspace{.3cm}

Tomemos una función $f$ que cumpla con ser continua y acotada, y consideremos a la función $g(x) \coloneqq f(x, c)$ (es decir, la función definida manteniendo la variable $y$ constante en $c$). Debido a como la definimos, $g$ también será continua y acotada. Por lo tanto, $E \big[ g(X_n) \big] \overset{n}{\rightarrow} E \big[ g(X) \big]$. Pero dicha expresión es equivalente a decir que, $E \big[ f(X_n, \, c) \big] \overset{n}{\rightarrow} E \big[ f(X, \, c) \big]$, y por lo tanto, dado que $X_n \overset{d}{\rightarrow} X$, sabemos que $(X_n, \, c) \overset{d}{\rightarrow} (X, \, c)$.

En segundo lugar consideremos la expresión $|(X_n, \, Y_n) - (X_n, \, c)| = |X_n - X_n, \, Y_n - c| = |(0, \, Y_n - c)|$. Dicha expresión converge en probabilidad a cero dado que $Y_n \overset{p}{\rightarrow} c$.

Por lo tanto, hemos demostrado que:

a. $|(X_n, \, Y_n) - (X_n, \, c)| \overset{p}{\rightarrow} 0$  
b. $(X_n, \, c) \overset{d}{\rightarrow} (X, \, c)$

Por lo tanto, estamos en las hipótesis del postulado ii. Entonces, podemos deducir que $(X_n, \, Y_n) \overset{d}{\rightarrow} (X, \, Y)$.

***

*Dem iv*: demostrado en el ejercicio 4.

***

Recapitulando:

1. Por el postulado i sabemos que si $Y_n \overset{d}{\rightarrow} c$, entonces $Y_n \overset{p}{\rightarrow} c$.

2. Por los postulados ii y iii sabemos que si $X_n \overset{d}{\rightarrow} X$ y $Y_n \overset{d}{\rightarrow} c \Rightarrow (X_n, \, Y_n) \overset{d}{\rightarrow} (X, \, c)$.

3. Y por el postulado iv sabemos que si $X_n \overset{d}{\rightarrow} X$ y $g$ es continua, entonces $g(X_n) \overset{d}{\rightarrow} g(X)$.

Consideremos entonces la función $g(x, y) = x + y$, entonces $X_n + Y_n \overset{d}{\rightarrow} X + c$.

## Parte c: si $X_n \overset{d}{\rightarrow} X$ y $Y_n \overset{d}{\rightarrow} c \in \mathbb{R}$ entonces $X_n \, Y_n \overset{d}{\rightarrow} X \, c$

Ídem parte b, solo que en este caso debemos tomar la función $g(x,y) = xy$.

## Parte d: si $X_n \overset{d}{\rightarrow} X$ y $Y_n \overset{d}{\rightarrow} 1 \in \mathbb{R}$ entonces $X_n^{Y_n} \overset{d}{\rightarrow} X$ 

Por la parte a sabemos que si $Y_n \overset{d}{\rightarrow} c \Rightarrow Y_n \overset{p}{\rightarrow} c$ (esto no es cierto en general, pero sí cuando la convergencia es a una constante).

$$X_n^{Y_n} = e^{Y_n \log(X_n)} \overset{d}{\rightarrow} e^{1 \log X} = e^{\log X} = X$$

## Parte e: si $X_n \overset{d}{\rightarrow} X$ y $Y_n \overset{d}{\rightarrow} 1 \in \mathbb{R}$ entonces $Y_n^{X_n} \overset{d}{\rightarrow} 1$ 

Por la parte a sabemos que si $Y_n \overset{d}{\rightarrow} c \Rightarrow Y_n \overset{p}{\rightarrow} c$ (esto no es cierto en general, pero sí cuando la convergencia es a una constante).

$$Y_n^{X_n} = e^{X_n \log(Y_n)} \overset{d}{\rightarrow} e^{X \log 1} = e^{0} = 1$$

# Ejercicio 3

**Convergencia uniforme de funciones**:

Sean $f_n : E \rightarrow \mathbb{R}$ funciones a valores reales. Decimos que la sucesión de funciones $\{f_n\}_{n \in \mathbb{N}}$ converge uniformemente a $f$ sí, y solo sí, $\forall \varepsilon > 0$, $\exists N$ tal que para todo $n \geq N$ se cumple que $|f_n(x) - f(x)| < \varepsilon$ para todo $x \in E$.

Una forma alternativa de caracterizar este concepto (la que usaremos en este ejercicio) es la siguiente. Sea $d_n(f_n, f) = \sup\limits_{x \in E} |f_n(x) - f(x)|$. Entonces, $\{f_n\}_{n \in \mathbb{N}} \rightrightarrows f \Leftrightarrow d_n(f_n, f) \overset{n}{\rightarrow} 0$.

Por lo tanto, probar una convergencia uniforme implíca probar que la sucesión de funciones se encuentra siempre (léase, a partir de un $n$) cerca de la función límite $f$. Es decir, $f(x) - \varepsilon \leq f_n(x) \leq f(x) + \varepsilon$ para todos los valores del $x$ en el conjunto $E$.

***

Dado que lo que querémos probar es que $\sup\limits_{x \in E}|f_n(x) - f(x)| \overset{n}{\rightarrow} 0$, definamos el conjunto $E = [-R, \, R]$ y estudiemos los dos supremos por separado: i) cuando $x \in E$, y ii) cuando $x \in E^c$.

Sea $\varepsilon > 0$, entonces existe $R = R(\varepsilon) > 0$ tal que $F(R) - F(-R) > 1 - \varepsilon$. Por lo tanto, $F(-R) < \varepsilon$, mientras que $F(R) > 1 - \varepsilon$.

```{r fig1, out.width="60%", fig.align='center', echo=FALSE}
x <- seq(-3, 3, by = 0.01)
cdf <- pnorm(x)
epsilon <- .1
par(las = 1)
plot(x, cdf, type = "l", col = "red", axes = FALSE, ylab = expression(F[n]), xlab = expression(x))
axis(side = 1, at = c(-3, -2, 2, 3), labels = c("", "-R", "R", ""))
axis(side = 2, at = c(0, epsilon, 1 - epsilon, 1), 
     labels = c("0", expression(epsilon), expression(1 - epsilon), "1"))
abline(h = 1, lty = "dashed", col = "black")
abline(h = epsilon, lty = "dashed", col = "darkgreen")
abline(h = 1 - epsilon, lty = "dashed", col = "darkgreen")
```

Dado que $F$ es continua en $R$ y en $-R$ (por hipótesis) y $F_n$ converge puntualmente a $F$ (dado que, por hipótesis, converge en distribución), sabemos que $\exists N_0$ tal que $\forall n \geq N_0$ se cumple que $|F_n(R) - F(R)| < \varepsilon$ y $|F_n(-R) - F(-R)| < \varepsilon$. Es decir, $d_n(F_n, F) \overset{n}{\rightarrow} 0$ en $x = R$ y en $x = -R$.

\vspace{0.3cm}

Consideremos primero un $r \leq -R$ (es decir, un valor de $x$ a la izquierda de $-R$). Por la convergencia en distribución y la continuidad sabemos que $| F_n(r) - F(r) | < \varepsilon$. El objetivo es acotar esta expresión. Primero observemos que $F_n(-R) < F(-R) + \varepsilon$:
$$|F_n(-R) - F(-R)| < \varepsilon \Leftrightarrow - \varepsilon < F_n(-R) - F(-R) < \varepsilon \Leftrightarrow F(-R) - \varepsilon < \color{orange}{F_n(-R) < F(-R) + \varepsilon}$$

Luego entonces, para todo $n \geq N_0$ se cumple que:
$$|F_n(r) - F(r)| \leq F_n(r) + F(r) \leq F_n(-R) + F(-R) \leq F(-R) + \varepsilon + F(-R) \leq 3 \varepsilon$$

Donde la primer desigualdad se cumple por propieda del valor absoluto.\footnote{Por la desigualdad triangular sabemos que $a - b \leq |a - b| \leq |a| + |b|$. Dado que las funciones de distribución toman siemepre valores positivos únicamente se cumple también que $|a| + |b| = a + b$.} La segunda desigualdad se cumple dado que tomamos $r \leq -R$. La tercer desigualdad se cumple por el razonamiento anterior. La última desigualdad se cumple debido a como definimos $-R$.

\vspace{0.3cm}

Consideremos ahora un $r\geq R$. Sabemos lo siguiente:

i. $R$ fue elegido de forma tal que $1 - \varepsilon < F(R)$.

ii. Como hay convergencia puntual (por la convergencia en distribución), existe $N_0$ tal que para todo $n \geq N_0$ se cumple que:
$$|F_n(R) - F(R)| < \varepsilon \Leftrightarrow - \varepsilon < F_n(R) - F(R) < \varepsilon \Leftrightarrow F(R) - \varepsilon < F_n(R) < F(R) + \varepsilon$$

iii. Todas las funciones son de distribución, y por lo tanto, monótonas no decrecientes. Entonces:
$$\begin{array}{ r }
F(R) \leq F(r) \leq 1 \\
F_n(R) \leq F_n(r) \leq 1
\end{array}$$

Utilizando que $1 - \varepsilon < F(R)$, y que $F(R) \leq F(r) \leq 1$ hallamos que $1 - \varepsilon < F(r) \leq 1$. Por otro lado, utilizando que $F(R) - \varepsilon < F_n(R) < F(R) + \varepsilon$, y que $F_n(R) \leq F_n(r) \leq 1$, hallamos que: 
$$\begin{array}{c c}
F(R) - \varepsilon < F_n(R) \leq F_n(r) \leq 1 & \Leftrightarrow \\
-F(R) + \varepsilon > - F_n(R) \geq -F_n(r) \geq -1 & \Leftrightarrow \\
-1 \leq -F_n(r) < -F(R) + \varepsilon
\end{array}$$

\newpage

Sumando verticalmente ambas equaciones obetenemos que:

\begin{center}
\begin{tabular}{c c c c c}
$1 - \varepsilon$ & $<$    & $F(r)$    & $\leq$ & $1$                   \\
$- 1$             & $\leq$ & $-F_n(r)$ & $<$    & $-F(R) + \varepsilon$ \\
\hline
$1 - \varepsilon - 1$ & $<$ & $F(r) - F_n(r)$ & $<$ & $1 - F(R) + \varepsilon$ \\
\end{tabular}
\end{center}

\vspace{0.2cm}

Del lado izquierdo (cota inferior), podemos simplificar para hallar que:
$$- \varepsilon < F(r) - F_n(r) < 1 - F(R) + \varepsilon$$

Por su parte, del lado derecho (cota superior), debemos recordar que $R$ fue elegido de forma tal que $1 - \varepsilon < F(R)$. Pero esto implíca que $1 - F(R) < \varepsilon$. Remplazando obtenemos que:
$$- \varepsilon < F(r) - F_n(r) < 1 - F(R) + \varepsilon < 2 \varepsilon$$

Luego entonces:
$$- \varepsilon < F(r) - F_n(r) < 2 \varepsilon \Leftrightarrow -2 \varepsilon < F(r) - F_n(r) < 2 \varepsilon \Leftrightarrow |F_n(r) - F(r)| < 2 \varepsilon$$

Por lo tanto, hemos probado que, si $n \geq N_0$, $\sup\limits_{x \in [-R, \,R]} \big| F_n(x) - F(x) \big| < 3 \varepsilon$.

\vspace{0.3cm}

Por último, consideremos un $r \in [-R, \, R]$. Dado que $F$ es uniformemente continua en $[-R, \, R]$ sabemos que $\forall \varepsilon > 0$, $\exists \delta > 0$ tal que, si $|x - y| < \delta$, entonces $|F(x) - F(y)| < \varepsilon$. Consideremos la partición $-R < x_1 < x_2 < \ldots < x_k < R$ tal que $x_i - x_{i-1} < \delta$. Como $F_n(x_i) \overset{n}{\rightarrow} F(x_i)$, $\exists n_i$ tal que si $n > n_i$, $|F_n(x_i) - F(x_i)| < \varepsilon$. Es decir, para cada sub-intevalo de la partición, la función converge a partir de algun valor $n_i$, no necesariamente el mismo para todos los sub-intervalos. Sea $N_1 = \max(n_1, \, \ldots, \, n_k)$ (el mayor de todos los $n_i$).

Sea $i$ tal que $x_{i - 1} < r < x_i$ (es decir, el sub-intervalo que contiene al valor $r$), y consideremos $n > N_1$ (esto nos asegura que tengamos convergencia en dicho intervalo). Por lo tanto se cumple que:
$$\begin{array}{ c c }
\begin{array}{c c l}
F_n(r) - F(r) & \leq & F_n(x_i) - F(x_{i-1}) \\
              & \leq & F(x_i) + \varepsilon - F(x_{i-1}) \\
              & \leq & 2 \varepsilon
\end{array}
&
\begin{array}{c c l}
F(r) - F_n(r) & \leq & F(x_i) - F_n(x_{i-1}) \\
              & \leq & F(x_i) - F(x_{i-1}) + \varepsilon \\
              & \leq & 2 \varepsilon
\end{array}
\end{array}$$

Por lo tanto, queda demostrado que, si $n \geq N_1$, $\sup\limits_{x \in [-R, \, R]} \big| F_n (x) - F(x) \big| < 2 \varepsilon$.

\vspace{0.3cm}

Por último, tomemos $n > \max(N_0, \, N_1)$. Entonces se cumple que $\sup \big| F_n(x) - F(x) \big| \overset{n}{\rightarrow} 0$, por lo que queda demostrado que $f_n \rightrightarrows f$.

# Ejercicio 4

*Demostración utilizando composición de funciones.*

Por hipótesis sabemos que $X_n \overset{d}{\rightarrow} X$. Esto implíca que para toda función $f$ continua y acotada se cumple que $E \big[ f(X_n) \big] \overset{n}{\rightarrow} E \big[ f(X) \big]$ (ver teorema 2.39). Sea $h = f \circ g$. Entonces $h$ también es continua y acotada dado que, por hipótesis, $g$ es continua. Por lo tanto:

$$E \big[ f( g(X_n) ) \big] = E \big[ h(X_n) \big] \overset{n}{\rightarrow} E \big[ h(X) \big] = E \big[ f( g(X) ) \big] \Rightarrow g(X_n) \overset{d}{\rightarrow} g(X)$$

*Demostración utilizando el teorema de representación de Skorohod.*

Dado que $X_n \overset{d}{\rightarrow} X$ el teorema de Skorohod nos asegura que existen $X_n^* \overset{d}{=} X_n$ y $X^* \overset{d}{=} X$ tales que $X_n^* \overset{cs}{\rightarrow} X^*$. Dado que $g$ es continua, $g(X_n^*) \overset{cs}{\rightarrow} g(X^*)$. Pero dado que las nuevas variables ($X_n^*$ y $X^*$) se definieron de forma de ser iguales en distribución a las variables originales ($X_n$ y $X$), $g(X_n^*) \overset{cs}{\rightarrow} g(X^*)$ implíca que $g(X_n) \overset{d}{\rightarrow} g(X)$.

***

Sean $X, Y : \Omega \rightarrow F$ dos variables aleatorias y $g$ una función continua. Por definición de igualdad en distribuciones sabemos que si $X \overset{d}{=} Y$ entonces $P(X \in A) = P(Y \in A)$ para todo $A \in \mathcal{B}(F)$. Por lo tanto, 

$$\begin{array}{c c l}
X \overset{d}{=} Y & \Leftrightarrow & P(X \in A) = P(Y \in A) \\
                   & \Leftrightarrow & P(g^{-1}(g(X)) \in A) = P(g^{-1}(g(Y)) \in A) \\
                   & \Leftrightarrow & P(g(X) \in g(A)) = P(g(Y) \in g(A)) \\
                   & \Leftrightarrow & g(X) \overset{d}{=} g(Y)
\end{array}$$

# Ejercicio 5

Comenzemos entendiendo cómo es la sucesión de variables aleatorias con la que debemos trabajar. Se establece que $X_n \sim Unif \left\{ \frac{1}{n},\, \frac{2}{n}, \, \ldots, \, \frac{n-1}{n}, \, 1 \right\}$ con $n \geq 1$, por lo tanto:
$$\begin{array}{ c l c l}
\text{Si } n = 1: & X_1 \sim Unif \big\{ 1 \big\} & \Rightarrow & f_{X_1}(x) = 1 \\ \\
\text{Si } n = 2: & X_2 \sim Unif \left\{\frac{1}{2}, \, 1 \right\} & \Rightarrow & f_{X_2}(x) = \frac{1}{2} \\ \\
\text{Si } n = 3: & X_3 \sim Unif \left\{\frac{1}{3}, \, \frac{2}{3}, \, 1 \right\} & \Rightarrow & f_{X_3}(x) = \frac{1}{3} \\ \\
\text{Si } n = 4: & X_4 \sim Unif \left\{\frac{1}{4}, \, \frac{2}{4}, \, \frac{3}{4}, \, 1 \right\} & \Rightarrow & f_{X_4}(x) = \frac{1}{4} \\
\vdots & \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \vdots & & \,\,\,\,\,\, \vdots \\ 
\text{Si } n = k: & X_k \sim Unif \left\{\frac{1}{k}, \, \frac{2}{k}, \, \ldots, \, \frac{k - 1}{k}, \, 1 \right\} & \Rightarrow & f_{X_k}(x) = \frac{1}{k} \\
\vdots & \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\, \vdots & & \,\,\,\,\,\, \vdots
\end{array}$$

Debemos demostrar que: $X_n \overset{d}{\rightarrow} X \Leftrightarrow \lim\limits_{n \rightarrow +\infty} F_{X_n}(x) = F_x(x)$ para todo punto de continuidad de $F_X$. Dada la sucesión de variables aleatorias descripta arriba sus funciones de distribución serán (por ejemplo):

![](unifs.png)

Comencemos notando que, por cómo está definida la sucesión, $F_{X_n} = 0$ para todo valor de $x < \, ^1\!/_n$, mientras que $F_{X_n}(x) = 1$ para cualquier valor de $x \geq 1$. Para los valores de $x$ entre 0 y 1 debemos notar (ver imagen) que $F_{X_n}$ nunca se separa de $F_X$ en más de $^1\!/_n$. Por lo tanto, tenemos que:
$$\lim\limits_n F_{X_n}(x) = \lim\limits_n \frac{1}{n} \, \lfloor xn \rfloor \leq \lim\limits_n \frac{nx}{n} = x$$

Donde la desigualdad se debe a que $nx - 1 \leq \lfloor xn \rfloor \leq nx$. Por lo tanto, hemos demostrado que $X_n \overset{d}{\rightarrow} X \sim Unif(0,1)$.

# Ejercicio 6

$(\Rightarrow)$
$$\lim\limits_{n \rightarrow +\infty} P(X_n = x_j) = \lim\limits_{n \rightarrow +\infty} \Big[ F_{X_n}(x_j) - F_{X_n}(x_{j - 1}) \Big] = \lim\limits_{n \rightarrow +\infty} F_{X_n}(x_j) - \lim\limits_{n \rightarrow +\infty} F_{X_n}(x_{j - 1}) =$$
$$= F_X(x_j) - F_X(x_{j - 1}) = P(X = X_j)$$

$(\Leftarrow)$
$$\lim\limits_{n \rightarrow +\infty} F_{X_n}(x_j) = \lim\limits_{n \rightarrow +\infty} \sum\limits_{ \substack{ x \in Rec(X_n) \\ x \leq j }} P(X_n = x_j) = \sum\limits_{ \substack{ x \in Rec(X_n) \\ x \leq j }} \lim\limits_{n \rightarrow +\infty} P(X_n = x_j) =$$
$$ = \sum\limits_{ \substack{ x \in Rec(X_n) \\ x \leq j }} P(X = x_j) = F_{X}(x_j) $$

# Ejercicio 7

$$\lim\limits_{ \substack{ p \rightarrow 0 \\ n \rightarrow + \infty \\ np \rightarrow \lambda } } {{n}\choose{x}} p^x (1 - p)^{n - x} = \lim\limits_{ \substack{ p \rightarrow 0 \\ n \rightarrow + \infty \\ np \rightarrow \lambda } } \frac{n!}{(n - x)! \, x!} \, p^x (1 - p)^{n - x} = $$
$$= \lim\limits_{ \substack{ p \rightarrow 0 \\ n \rightarrow + \infty \\ np \rightarrow \lambda } } \frac{n(n - 1)(n - 2) \ldots (n - (x -1))}{x!} \, p^x (1 - p)^{n - x} =$$
$$= \lim\limits_{ \substack{ p \rightarrow 0 \\ n \rightarrow + \infty \\ np \rightarrow \lambda } } \frac{n(n - 1)(n - 2) \ldots (n - (x -1))}{x!} \, \left( \frac{\lambda}{n} \right)^x \left( 1 - \frac{\lambda}{n} \right)^{n - x} =$$
$$= \frac{\lambda^x}{x!} \, \lim\limits_{ \substack{ p \rightarrow 0 \\ n \rightarrow + \infty \\ np \rightarrow \lambda } } \frac{ \overbrace{ n(n - 1)(n - 2) \ldots (n - (x -1)) }^{ \substack{ \text{Hay $x$ términos, por lo tanto es un} \\ \text{  polinomio de grado $x$ } }} }{n^x} \, \left( 1 - \frac{\lambda}{n} \right)^{n - x} \approx$$
$$ \approx \frac{\lambda^x}{x!} \, \lim\limits_{ \substack{ p \rightarrow 0 \\ n \rightarrow + \infty \\ np \rightarrow \lambda } } \frac{ n^x }{ n^x } \, \left( 1 - \frac{\lambda}{n} \right)^{n - x} = \frac{\lambda^x}{x!} \, \lim\limits_{ \substack{ p \rightarrow 0 \\ n \rightarrow + \infty \\ np \rightarrow \lambda } } \underbrace{ \left( 1 - \frac{\lambda}{n} \right)^{n} }_{ \rightarrow e^{-\lambda} } \underbrace{ \left( 1 - \frac{\lambda}{n} \right)^{- x} }_{ \rightarrow 1^{-x} \rightarrow 1 } = \frac{ \lambda^x \, e^{- \lambda}}{ x! }$$

Por lo tanto, $X \sim Poisson(\lambda)$.

# Ejercicio 8


# Ejercicio 9

Primero debemos hallar la distribución de $\tau(m)$:
$$\begin{array}{c c l}
P(\tau(m) = 1) & = & P(X_1 > m) = p_m \\
P(\tau(m) = 2) & = & P(X_1 \leq m) P(X_2 > m) = F(m) \big[ 1 - F(m) \big] = p_m (1 - p_m) \\
P(\tau(m) = 3) & = & P(X_1 \leq m) P(X_2 \leq m) P(X_3 > m) = F(m) \big[ 1 - F(m) \big]^2 = p_m (1 - p_m)^2 \\
\vdots         &   & \vdots \\
P(\tau(m) = k) & = & P(X_1 \leq m) \ldots P(X_{k-1} \leq m) P(X_k > m) = F(m) \big[ 1 - F(m) \big]^{k - 1} = p_m (1 - p_m)^{k - 1} \\
\end{array}$$

Vemos entonces que $\tau(m) \sim Geo(p_m)$. Esto se condice con la definición de $\tau(m)$. Si llamamos "éxito" a sobrepasar el nivel $m$, entonces $\tau(m) =$ "# de experimentos antes de alcanzar el 1er éxito". Si $\tau(m)$ tiene distribución Geométrica con parámetro $p_m$, entonces sobre su función de distribución sabemos que:

$$F_{\tau(m)}(y) = P(\tau(m) \leq y) = \sum\limits_{i = 1}^{ \lfloor y \rfloor } p_m (1 - p_m)^{i - 1} = \sum\limits_{j = 0}^{ \lfloor y \rfloor - 1 } p_m (1 - p_m)^{j} =$$
$$= p_m \, \frac{ 1 - (1 -p_m)^{\lfloor y \rfloor} }{ 1 - (1 - p_m) } = 1 - (1 -p_m)^{\lfloor y \rfloor}$$

Luego, lo que queremos probar es que $p_m \, \tau(m) \overset{d}{\rightarrow} Exp(1) = (1 - e^{-x}) \, \mathbb{I}_{ \{ x \geq 0\} }$. Nótese que no estamos hablando de la distribución de $\tau(m)$, sino de la distribución de $p_m \, \tau(m)$. Pero dado que $p_m$ es constante respecto de $n$, podemos establecer el siguiente vínculo entre ambas distribuciones:

$$F_{p_m \, \tau(m)}(x) = P \big(p_m \, \tau(m) \leq x \big) = P \big(\tau(m) \leq \, ^x \!/ _{p_m} \big) = F_{\tau(m)} \left( ^x \!/ _{p_m} \right)$$

Por lo tanto, para hallar la distribución límite de $p_m \, \tau(m)$ podemos trabajar con la distribución de $\tau(m)$ evaluada en $^x \!/ _{p_m}$. Esto requiere resolver el siguiente límite:
$$\lim\limits_{m \rightarrow + \infty} 1 - (1 - p_m)^{ \left\lfloor \frac{x}{p_m} \right\rfloor }$$

Notemos ahora que $\frac{x}{p_m} - 1 \leq \left\lfloor \frac{x}{p_m} \right\rfloor \leq \frac{x}{p_m}$, por lo tanto, podemos acotar inferiror y superiormente el limite de la siguiente forma:
$$\underbrace{ \lim\limits_{m \rightarrow + \infty} 1 - (1 - p_m)^{ \frac{x}{p_m} - 1 } }_{\overset{m}{\rightarrow} 1 - e^{-x}} \leq \lim\limits_{m \rightarrow + \infty} 1 - (1 - p_m)^{ \left\lfloor \frac{x}{p_m} \right\rfloor } \leq \underbrace{ \lim\limits_{m \rightarrow + \infty} 1 - (1 - p_m)^{ \frac{x}{p_m} }}_{\overset{m}{\rightarrow} 1 - e^{-x}}$$

Dado que ambas cotas convergen a la función de distribución de una variable aleatoria con distribución exponencial de parámetro $1$, concluimos que $p_m \, \tau(m) \overset{d}{\rightarrow} Exp(1)$.

# Ejercicio 10

$$\lim\limits_{n \rightarrow + \infty} F_{ \frac{X_n}{n}}(x) = \lim\limits_{n \rightarrow + \infty} P \left( \frac{X_n}{n} < x \right) = \lim\limits_{n \rightarrow + \infty} P \left( X_n < n \,x \right) =$$
$$= \lim\limits_{n \rightarrow + \infty} \left[ 1 - \left( 1 - \frac{\lambda}{\lambda + n} \right)^{ \lfloor nx \rfloor } \right] = 1 - \lim\limits_{n \rightarrow + \infty} \left( 1 - \frac{\lambda}{\lambda + n} \right)^{ \lfloor nx \rfloor } =$$
$$= 1 - \lim\limits_{n \rightarrow + \infty} \exp \left\{ \log \left( 1 - \frac{\lambda}{\lambda + n} \right)^{ \lfloor nx \rfloor } \right\} = 1 - \lim\limits_{n \rightarrow + \infty} \exp \left\{ \lfloor nx \rfloor \, \log \left( 1 - \frac{\lambda}{\lambda + n} \right) \right\} =$$
\begin{align*}
= 1 - \lim\limits_{n \rightarrow + \infty} \exp \left\{ \lfloor nx \rfloor \, \log \left( 1 + \frac{-\lambda}{\lambda + n} \right) \right\}
&\equalexpl{$\boxed{ \log(1 + u) \approx u \text{ cuando } u \rightarrow 0}$}
1 - \lim\limits_{n \rightarrow + \infty} \exp \left\{ \lfloor nx \rfloor \, \left( \frac{-\lambda}{\lambda + n} \right) \right\} = 
\end{align*}

$$ = 1 - \lim\limits_{n \rightarrow + \infty} \exp \left\{ -\lfloor nx \rfloor \, \left( \frac{\lambda}{\lambda + n} \right) \right\} = 1 - \exp \left\{ - \lim\limits_{n \rightarrow + \infty} \lfloor nx \rfloor \, \left( \frac{\lambda}{\lambda + n} \right) \right\}$$

Luego, para resolver este límite debemos acotar la expresión $\lfloor nx \rfloor$ de la siguiente manera:
$$\lim\limits_{n \rightarrow + \infty} (n - 1) x \left( \frac{\lambda}{\lambda + n} \right) \leq \lim\limits_{n \rightarrow + \infty} \lfloor nx \rfloor \, \left( \frac{\lambda}{\lambda + n} \right) \leq \lim\limits_{n \rightarrow + \infty} nx \, \left( \frac{\lambda}{\lambda + n} \right)$$

Nótese que las expresiones en ambas cotas, son polinómios en $n$. Por lo tanto, la desigualdad anterior es equivalente a:
$$\lim\limits_{n \rightarrow + \infty} \frac{ n \, x \, \lambda}{n} \leq \lim\limits_{n \rightarrow + \infty} \lfloor nx \rfloor \, \left( \frac{\lambda}{\lambda + n} \right) \leq \lim\limits_{n \rightarrow + \infty} \frac{ n \, x \, \lambda}{n}$$

De donde obtenemos entonces que nuestro límite está acotado inferior y superior mente por $\lambda x$. Por lo tanto,
$$\lim\limits_{n \rightarrow + \infty} F_{ \frac{X_n}{n}}(x) = 1 - e^{-\lambda x}$$

Por lo que podemos concluir que $\frac{X_n}{n} \overset{d}{\rightarrow} X \sim Exp(\lambda)$.

# Ejercicio 11


# Ejercicio 12


# Ejercicio 13

## Parte a


## Parte b


# Ejercicio 14

## Parte a

## Parte b

## Parte c


# Ejercicio 15

## Parte a


## Parte b
