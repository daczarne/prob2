---
title: "Práctico 6"
author: "Daniel Czarnievicz"
date: "2019"
output: pdf_document
header-includes:
   - \everymath{\displaystyle}
   - \usepackage[spanish]{babel}
   - \usepackage{xcolor}
   - \usepackage[makeroom]{cancel}
   - \DeclareMathOperator*{\plim}{plim}
   - \usepackage{mathrsfs}
   - \usepackage{amsmath}
   - \usepackage{mathtools}
   - \usepackage{MnSymbol}
   - \newcommand{\equalexpl}[1]{\underset{\substack{\uparrow\\\\\mathrlap{\text{\hspace{-2em}#1}}}}{\approx}}
   - \DeclareMathOperator{\E}{\mathbf{E}}
   - \DeclareMathOperator{\V}{\mathbf{V}}
   - \usepackage{tasks}
geometry: margin=1in
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Ejercicio 1

Sean las variables aleatorias $A_i =$ "sale 6 en la $i$-ésima tirada", y $A = \sum\limits_{i = 1}^{n} A_i =$ "cantidad de veces que sale 6". Entonces $A_i \sim \text{Ber}(1/6)$, mientras que $A \sim \text{Bin}(180, \, 1/6)$. Luego entonces:

$$\Pr ( \text{``sale 6 menos de 25 veces''} ) = \Pr ( A \leq 24 ) = \Pr \left( \frac{ \sum\limits_{i = 1}^{180} A_i - np }{ \sigma \sqrt{n} } \leq \frac{ 24 - np }{ \sigma \sqrt{n} } \right) =$$
$$= \Pr \left( \frac{ \sum\limits_{i = 1}^{180} A_i - 180 \, (1/6) }{ \sqrt{ (1/6) (5/6) } \, \sqrt{180} } \leq \frac{ 24 - 180 \, (1/6) }{ \sqrt{ (1/6) (5/6) } \, \sqrt{180} } \right) = \Pr \left( Z \leq \frac{ 24 - 30 }{ 30 \sqrt{5} } \right)$$

```{r, comment=""}
n <- 180
p <- 1 / 6
mu <- (n * p)
sigma <- sqrt(p * (1 - p))
x <- 24
pnorm((x - mu) / (sigma * sqrt(n)), mean = 0, sd = 1)
pbinom(x, size = n, prob = p)
sum(dbinom(1:x, size = n, prob = p))
```

\newpage

# Ejercicio 2: LDGN de Khinchin

Probar que si $\{X_n\}_{n \geq 1}$ es una sucesión de variables aleatorias iid con $\E(X_1) = \mu$, entonces $\bar{X}_n \overset{p}{\rightarrow} \mu$.

*Dem*: por el teorema de Slutsky sabemos que si una sucesión de variables aleatorias converge en distribución a una constante, entonces también converge en probabilidad a dicha constante. A su vez, por el teorema 3.11 sabemos que la convergencia de funciones características equivale a la convergencia en distribución. Por lo tanto, debemos obtener las funciones características de $\{X_n\}_{n \geq 1}$ y de $\bar{X}_n$, y verificar que se cumpla la convergencia. Primero hallamos la función característica de $\bar{X}_n$:
$$\varphi_{\bar{X}_n}(t) = \E( \exp\{ it \bar{X}_n \}) = \E \left( \exp\left\{ it \frac{1}{n} \sum\limits_{j = 1}^{n} X_j \right\} \right) = \left[ \E \left( \exp\left\{ \frac{it}{n} X \right\} \right) \right]^n = \left[ \varphi_{X} \big( t/n \big) \right]^n$$

Luego, si la sucesión converge a $\mu$ en distribución, entonces converge a una variable aleatoria con distribución degenerada en $\mu$. Por lo tanto, su función característica estará dada por:
$$\varphi_{X}(t) = \E \big( e^{itX} \big) = e^{it\mu} \, \Pr(X = \mu) = e^{it\mu} (1) = e^{it\mu}$$

Para evaluar el límite cunado $n \rightarrow +\infty$ de $\left[ \varphi_{X} \big( t/n \big) \right]^n$ utilizamos un desarrollo de Taylor en torno a $t = 0$. Dado que por hipótesis sabemos que la variable tiene primer momento finito, sabemos que es diferenciable al menos una vez, por lo tanto, nuestro desarrollo será de primer orden:
$$\begin{array}{rcl}
\lim\limits_{n \rightarrow +\infty} \left[ \varphi_{X} \big( t/n \big) \right]^n & = & \lim\limits_{n \rightarrow +\infty} \left[ \varphi_X(0) + \varphi'_X(0) \frac{t}{n} + \frac{t}{n} \varepsilon_1(t_1 / n) \right]^n \\ \\
   & = & \lim\limits_{n \rightarrow +\infty} \left[ 1 + i \mu \frac{t}{n} + \frac{t}{n} \varepsilon_1(t_1 / n) \right]^n \\ \\
   & = & \lim\limits_{n \rightarrow +\infty} \exp\left\{ n \log \left[ 1 + i \mu \frac{t}{n} + \frac{t}{n} \varepsilon_1(t_1 / n) \right] \right\} \\ \\
   & = & \lim\limits_{n \rightarrow +\infty} \exp\left\{ n \left( i \mu \frac{t}{n} + \frac{t}{n} \varepsilon_1(t_1 / n) \right) \right\} \\ \\
   & = & \lim\limits_{n \rightarrow +\infty} \exp\left\{ i \mu t + t \, \varepsilon_1(t_1 / n) \right\} \\ \\
   & = & e^{it\mu}
\end{array}$$

Por lo tanto, hemos probado que $\varphi_{\bar{X}_n}(t) \overset{n}{\rightarrow} \varphi_X(t)$ lo cual implica que $\bar{X}_n \overset{d}{\rightarrow} X$ donde $X$ es tal que $\Pr( X = \mu) = 1$. Por lo tanto, $\bar{X}_n \overset{p}{\rightarrow} \mu$.

# Ejercicio 3

## Parte a: utilizando funciones características

Primero calculamos la función característica de una variable aleatoria con distribución $\Gamma(n, \, \lambda)$.
$$\varphi_{Z_n}(t) = \E \big( e^{itZ_n} \big) = \int\limits_{\mathbb{R}^+} e^{itz_n} f_{Z_n}(z_n) dz_n = \int\limits_{\mathbb{R}^+} e^{itz_n} \frac{\lambda^n}{\Gamma(n)} e^{-\lambda z_n} z_n^{n - 1} dz_n =$$
$$= \frac{\lambda^n}{\Gamma(n)} \int\limits_{\mathbb{R}^+} \exp\big\{ itz_n -\lambda z_n \big\} z_n^{n - 1} dz_n = \frac{\lambda^n}{\Gamma(n)} \int\limits_{\mathbb{R}^+} \exp\big\{ -( \lambda - it) z_n \big\} z_n^{n - 1} dz_n $$

Realizamos el siguiente cambio de variable (los límites de integración no cambian):

$$y_n = (\lambda - it) z_n \Rightarrow z_n = \frac{y_n}{\lambda - it}$$
$$\frac{dz_n}{dy_n} = \frac{1}{\lambda - it} \Rightarrow dz_n = \frac{dy_n}{\lambda - it}$$

Obtenemos entonces:
$$ \frac{\lambda^n}{\Gamma(n)} \int\limits_{\mathbb{R}^+} \exp\big\{ -( \lambda - it) z_n \big\} z_n^{n - 1} dz_n =  \frac{\lambda^n}{\Gamma(n)} \int\limits_{\mathbb{R}^+} e^{ -y_n } \left( \frac{y_n}{\lambda - it} \right)^{n - 1} \frac{dy_n}{\lambda - it} =$$
$$= \frac{\lambda^n}{(\lambda - it)^n \Gamma(n)} \int\limits_{\mathbb{R}^+} e^{ -y_n } y_n^{n - 1} dy_n = \frac{\lambda^n}{(\lambda - it)^n \Gamma(n)} \int\limits_{\mathbb{R}^+} e^{ -y_n } y_n^{n - 1} \frac{\Gamma(n)}{\Gamma(n)} dy_n =$$
$$= \frac{\lambda^n}{(\lambda - it)^n} \underbrace{ \int\limits_{\mathbb{R}^+} e^{ -y_n } y_n^{n - 1} \frac{1}{\Gamma(n)} dy_n }_{ Y_n \sim \Gamma(1, n) } = \frac{\lambda^n}{(\lambda - it)^n} = \left( \frac{\lambda}{\lambda - it}\right)^n =$$
$$= \left( \frac{\lambda - it}{\lambda}\right)^{-n} = \left( 1 - \frac{it}{\lambda}\right)^{-n}$$

Luego, calculamos el límite buscado.
$$\lim\limits_{n \rightarrow +\infty} \varphi_{Y_n}(t) = \lim\limits_{n \rightarrow +\infty} \varphi_{ \frac{\lambda Z_n}{\sqrt{n}} - \sqrt{n} } (t) = \lim\limits_{n \rightarrow +\infty} e^{-it\sqrt{n}} \varphi_{Z_n} \left( ^{t \lambda} \!/ \!_{\sqrt{n}} \right) = \lim\limits_{n \rightarrow +\infty} e^{-it\sqrt{n}} \, \left(  1 - \frac{ ^{it \lambda} \!/ \!_{\sqrt{n}}}{\lambda} \right)^{-n} =$$
$$= \lim\limits_{n \rightarrow +\infty} e^{-it\sqrt{n}} \, \left(  1 - \frac{ it }{ \sqrt{n} } \right)^{-n} = \lim\limits_{n \rightarrow +\infty} \exp\left\{ -n \log \left( 1 - \frac{ it }{ \sqrt{n} } \right) - it \sqrt{n} \right\} =$$
$$= \exp\left\{ - \lim\limits_{n \rightarrow +\infty} \left[ n \log \left( 1 - \frac{ it }{ \sqrt{n} } \right) + it \sqrt{n} \right] \right\} = \exp\left\{ - \lim\limits_{n \rightarrow +\infty} \left[ n \log \left( 1 + \frac{ -it }{ \sqrt{n} } \right) + it \sqrt{n} \right] \right\} =$$
Utilizamos el siguiente equivalente: $\log(1 + u) \leadsto u - \frac{u^2}{2}$ cuando $u \rightarrow 0$, de donde obtenemos:
$$= \exp\left\{ - \lim\limits_{n \rightarrow +\infty} \left[ n \left( \frac{-it}{\sqrt{n}} - \frac{ (^{-it} \!/ \!_{\sqrt{n}})^2 }{2} \right) + it \sqrt{n} \right] \right\} = \exp\left\{ - \lim\limits_{n \rightarrow +\infty} \left[ n \left( \frac{-it}{\sqrt{n}} - \frac{ (it)^2 }{2n} \right) + it \sqrt{n} \right] \right\} =$$
$$= \exp\left\{ - \lim\limits_{n \rightarrow +\infty} \left[ - it \sqrt{n} + \frac{ t^2 }{2} + it \sqrt{n} \right] \right\} = \exp\left\{ - \lim\limits_{n \rightarrow +\infty} \frac{ t^2 }{2} \right\} = e^{-t^2 / 2}$$

Por lo tanto, $Y_n \overset{d}{\rightarrow} Y \sim \text{N}(0, 1)$.

## Parte b: utilizando el Teorema del Límite Central

Sea la sucesión de variables aleatorias $\{X_n\}_{n \geq 1}$ iid donde $X_1 \sim \text{Exp}(\lambda)$. Luego, entonces $\E(X_1) = \lambda^{-1}$, $\V(X_1) = \lambda^{-2}$, y $Z_n = \sum\limits_{j = 1}^{n} X_j \sim \Gamma(n, \, \lambda)$. Luego, si aplicamos el Teorema del Límite Central de Levy, obtenemos que:
$$\frac{ \sum\limits_{j = 1}^{n} X_j - n \E(X_1)}{ \sqrt{n \V(X_1)} } = \frac{Z_n - n \E(X_1)}{ \sqrt{n \V(X_1)} } = \frac{Z_n - n (1 / \lambda)}{ \sqrt{n (1 / \lambda^2) } } = \frac{Z_n - n / \lambda }{ \sqrt{n} / \lambda } =$$
$$= \frac{ \lambda Z_n - n }{ \sqrt{n} } = \frac{ \lambda Z_n }{ \sqrt{n} } - \sqrt{n} = Y_n \overset{d}{\rightarrow} Y \sim \text{N}(0, 1)$$

# Ejercicio 4: aproximación Normal de la Poisson

Primero hallamos la función característica de una variable aleatoria con distribución $\text{Poisson}(\lambda)$:
$$\varphi_{Z_{\lambda}}(t) = \E \big( e^{itZ_{\lambda}} \big) = \sum\limits_{z = 0}^{+\infty} e^{itz} e^{-\lambda} \frac{\lambda^z}{z!} = e^{-\lambda} \sum\limits_{z = 0}^{+\infty} \frac{ (\lambda e^{it})^z }{ z! } = e^{-\lambda} e^{ \lambda e^{it} } = \exp\left\{ \lambda \big( e^{it} - 1 \big) \right\} $$

Luego calculamos el límite de la función característica de la transformación propuesta cuando $\lambda \rightarrow + \infty$.
$$\lim\limits_{\lambda \rightarrow + \infty} \varphi_{ \frac{ Z_{\lambda} }{ \sqrt{\lambda} } - \sqrt{\lambda} }(t) = \lim\limits_{\lambda \rightarrow + \infty} e^{-it \sqrt{\lambda}} \, \varphi_{ Z_{\lambda} } \left( ^t\! / \!_{\sqrt{\lambda}} \right)  = \lim\limits_{\lambda \rightarrow + \infty} e^{-it \sqrt{\lambda}} \, \exp\left\{ \lambda \big( e^{it / \sqrt{\lambda}} - 1 \big) \right\} =$$
$$ = \lim\limits_{\lambda \rightarrow + \infty} \exp\left\{ \lambda \big( e^{it / \sqrt{\lambda}} - 1 \big) -it \sqrt{\lambda} \right\} = \lim\limits_{\lambda \rightarrow + \infty} \exp\left\{ \lambda \left( e^{it / \sqrt{\lambda}} - 1 - \frac{it}{\sqrt{\lambda}} \right) \right\} =$$
$$= \exp\left\{ \lim\limits_{\lambda \rightarrow + \infty} \lambda \left( e^{it / \sqrt{\lambda}} - 1 - \frac{it}{\sqrt{\lambda}} \right) \right\}$$

\newpage

Utilizamos el siguiente desarrollo de Taylor de orden 2 entorno a $x = 0$.
$$e^{x} = e^{0} +  e^{0} \, \frac{x}{1!} + e^{0} \, \frac{x^2}{2!} + \varepsilon_2(x_1) \frac{x^2}{2!}$$
donde $\varepsilon_2(x_1)$ es tal que $\varepsilon_2(x_1) \rightarrow 0$ cuando $x \rightarrow 0$ con $0 < x_1 < t$. Si $x = it / \sqrt{\lambda}$, obtenemos que:
$$\begin{array}{rcl}
e^{it / \sqrt{\lambda}} & = & 1 + \frac{ it / \sqrt{\lambda} }{1!} + \frac{ (it / \sqrt{\lambda})^2 }{2!} + \varepsilon_2(x_1) \frac{ (it / \sqrt{\lambda})^2 }{2!} \\ \\
   & = & 1 + \frac{ it }{ \sqrt{\lambda} } + \frac{ (it)^2 }{ 2 \lambda } + \frac{ (it)^2 }{ 2 \lambda } \varepsilon_2(x_1) \\ \\
   & = & 1 + \frac{ it }{ \sqrt{\lambda} } - \frac{ t^2 }{ 2 \lambda } - \frac{ t^2 }{ 2 \lambda } \varepsilon_2(x_1)
\end{array}$$

Por lo tanto,
$$\begin{array}{rcl}
\lim\limits_{\lambda \rightarrow + \infty} \varphi_{ \frac{ Z_{\lambda} }{ \sqrt{\lambda} } - \sqrt{\lambda} }(t) & = & \exp\left\{ \lim\limits_{\lambda \rightarrow + \infty} \lambda \left( 1 + \frac{ it }{ \sqrt{\lambda} } - \frac{ t^2 }{ 2 \lambda } - 1 - \frac{it}{\sqrt{\lambda}} \right) \right\} \\ \\
   & = & \exp\left\{ \lim\limits_{\lambda \rightarrow + \infty} \lambda \left( - \frac{ t^2 }{ 2 \lambda } \right) \right\} \\ \\
   & = & \exp\left\{ \lim\limits_{\lambda \rightarrow + \infty} \left( - \frac{ t^2 }{ 2 } \right) \right\} \\ \\
   & = & e^{-t^2 / 2}
\end{array}$$

# Ejercicio 5

## Parte i

Comenzamos utilizando las propiedades de la proposición 3.3, de donde obtenemos que:
$$\varphi_{ \sqrt{n}( \bar{Y}_n - 1) }(t) = e^{-it \sqrt{n}} \varphi_{ \bar{Y}_n }(t \sqrt{n}) = e^{-it \sqrt{n}} \varphi_{ \frac{1}{n} \sum\limits_{j = 1}^{n} Y_j }(t \sqrt{n}) =$$
$$= e^{-it \sqrt{n}} \prod\limits_{j = 1}^{n} \varphi_{ \frac{1}{n} Y_j }(t \sqrt{n}) = e^{-it \sqrt{n}} \prod\limits_{j = 1}^{n} \varphi_{ Y_j }(t / \sqrt{n}) = e^{-it \sqrt{n}} \left[ \varphi_{ Y_1 }(t / \sqrt{n}) \right]^n$$

\newpage

Luego debemos hallamos la función característica de $Y_1 = X_1^2 \sim \chi^2_1$. Pero si $Y_1 \sim \chi^2_1$ entonces $Y_1 \sim \Gamma(1/2, \, 1/2)$, por lo que su función característica será (ver detalles en ejercicio 3):
$$\varphi_{Y_1}(t) = \left( 1 - \frac{it}{^1\!/_2} \right)^{-^1\!/_2} = \left( 1 - 2it \right)^{-^1\!/_2}$$

Por lo tanto, necesitamos estudiar el siguiente límite:
$$\lim\limits_{n \rightarrow + \infty} \varphi_{ \sqrt{n}( \bar{Y}_n - 1) }(t) = \lim\limits_{n \rightarrow + \infty} e^{-it \sqrt{n}} \left( 1 - \frac{2it}{\sqrt{n}} \right)^{-^n\!/_2} = \lim\limits_{n \rightarrow + \infty} e^{-it \sqrt{n}} \exp\left\{ -\frac{n}{2} \log  \left( 1 - \frac{2it}{\sqrt{n}} \right) \right\} =$$
$$= \lim\limits_{n \rightarrow + \infty} \exp\left\{ -\frac{n}{2} \log  \left( 1 - \frac{2it}{\sqrt{n}} \right) - it \sqrt{n} \right\} = \exp\left\{ - \lim\limits_{n \rightarrow + \infty} \left[ \frac{n}{2} \log  \left( 1 + \frac{-2it}{\sqrt{n}} \right) + it \sqrt{n} \right] \right\}$$

A continuación utilizamos la equivalencia $\log(1 + u) \leadsto u - \frac{u^2}{2}$ cuando $u \rightarrow 0$, y obtenemos que:
$$= \exp\left\{ - \lim\limits_{n \rightarrow + \infty} \left[ \frac{n}{2} \left( \frac{-2it}{\sqrt{n}} - \frac{(-2it)^2}{2n} \right) + it \sqrt{n} \right] \right\} = \exp\left\{ - \lim\limits_{n \rightarrow + \infty} \left[ - it \sqrt{n} + t^2 + it \sqrt{n} \right] \right\} = e^{-t^2}$$

Por lo tanto, $\sqrt{n} \big( \bar{Y}_n - 1 \big) \overset{d}{\rightarrow} Z \sim \text{N}(0, \, 2)$.

## Parte ii

Dado el resultado de la parte i, y dado que $f:f(x) = x^r$ es continua y derivable, podemos aplicar el Método Delta. De esta forma, obtenemos que para todo $r > 0$:
$$\sqrt{n} \big( \bar{Y}_n^r - 1 \big) = \sqrt{n} \big( \bar{Y}_n^r - 1^r \big) = \sqrt{n} \big( f(\bar{Y}_n) - f(1) \big) \overset{d}{\rightarrow} W \sim \text{N}\big( 0, \, 2 (f'(1))^2 \big) \overset{d}{=} \text{N}\big( 0, \, V^2(r) \big)$$
donde, dado que $f:f(x) = x^r$, $f':f'(x) = r x^{r-1}$. Por lo tanto, $(f'(1))^2 = r^2$ entonces $V^2(r) = 2r^2$.

## Parte iii

Aplicando el resultado de la parte ii obtenemos que:
$$\frac{ \sqrt{n} \left( \bar{Y}_n^{^1/_3} - \left( 1 - \frac{2}{9n} \right) \right) }{ \sqrt{ ^2\!/_9 } } = \frac{ \sqrt{n} \left( \bar{Y}_n^{^1/_3} - 1 \right) }{ \sqrt{ ^2\!/_9 } } + \frac{ \sqrt{n} \left( ^2 / _{9n} \right) }{ \sqrt{ ^2\!/_9 } } =$$
$$= \underbrace{ \left( \sqrt{ ^9\!/_2 } \right) \overbrace{ \sqrt{n} \left( \bar{Y}_n^{^1/_3} - 1 \right)}^{ \overset{d}{\rightarrow} \text{N}(0, \, ^2\!/_9) }}_{ \overset{d}{\rightarrow} \text{N}(0, \, 1) } + \underbrace{ \sqrt{ ^2 / _{9n} } }_{ \overset{n}{\rightarrow} 0 } \overset{d}{\rightarrow} Z \sim \text{N}(0, \, 1)$$

# Ejercicio 6

Por el Teorema del Límite Central de Levy sabemos que $\big( \sigma^2 n \big)^{-^1/_2} \sum\limits_{j = 1}^{n} X_j \overset{d}{\rightarrow} Z \sim \text{N}(0, \, 1)$, por lo que:
$$\frac{ \bar{X}_n }{ \sigma / \sqrt{n} } \overset{d}{\rightarrow} Z \sim \text{N}(0, \, 1)$$

Luego, sobre el denominador sabemos que:
$$S_n^2 = \frac{1}{n} \sum\limits_{j = 1}^{n} \big( X_j - \bar{X}_n \big)^2 = \frac{1}{n} \sum\limits_{j = 1}^{n} X_j^2 - n \underbrace{ \bar{X}_n^2}_{ \overset{p}{\rightarrow} \mu = 0 } \overset{p}{\rightarrow} \sigma^2$$

Por lo tanto,
$$\frac{1}{n \sigma^2} \sum\limits_{j = 1}^{n} X_j^2 \overset{p}{\rightarrow} 1 \Rightarrow \sqrt{ \frac{1}{n \sigma^2} \sum\limits_{j = 1}^{n} X_j^2 } = \frac{ 1 }{ \sigma \sqrt{n} } \sqrt{ \sum\limits_{j = 1}^{n} X_j^2 } \overset{p}{\rightarrow} \sqrt{1}$$
dado que $f(z) = \sqrt{z}$ es continua en todo su dominio. Luego entonces, por el teorema de Slutsky, tenemos que:
$$\frac{ \sum\limits_{j = 1}^{n} X_j }{ \sqrt{ \sum\limits_{j = 1}^{n} X_j^2 } } = \frac{ \left( \frac{1}{\sigma / \sqrt{n}} \right) \sum\limits_{j = 1}^{n} X_j }{ \left( \frac{1}{\sigma / \sqrt{n}} \right) \sqrt{ \sum\limits_{j = 1}^{n} X_j^2 } } \overset{d}{\rightarrow} Z \sim \text{N}(0, \, 1)$$

# Ejercicio 7

**Hipótesis:**

\begin{tasks}(2)
  \task* $X_{n1}, \, \ldots, \, X_{nn}$ sucesión de variable aleatorias independientes.
  \task $\E(X_{nk}) = m_{nk}$
  \task $\V(X_{nk}) = \sigma_{nk}^2 < +\infty$
  \task $S_n = \sum\limits_{k = 1}^{n} X_{nk}$
  \task $V_n^2 = \sum\limits_{k = 1}^{n} \sigma^2_{nk}$
\end{tasks}

## Parte a

Demostrar que:
$$\text{Si } \lim\limits_{n \rightarrow + \infty} \frac{1}{V_n^2} \sum\limits_{k = 1}^{n} \E \left[ \big( X_{nk} - m_{nk} \big)^2 \, \mathbb{I}_{ \{ |X_{nk} - m_{nk}| \geq \varepsilon V_n \} } \right] = 0 \Rightarrow \frac{1}{V_n^2} \max\limits_{i = 1, \ldots, n} \{ \sigma^2_{ni} \} \overset{n}{\rightarrow} 0$$

*Dem:*
$$\lim\limits_{n \rightarrow +\infty} \frac{1}{V_n^2} \max\limits_{i = 1, \ldots, n} \{ \sigma^2_{ni} \} = \lim\limits_{n \rightarrow +\infty} \frac{1}{V_n^2} \max\limits_{i = 1, \ldots, n} \left\{ \E \big( X_{nk} - m_{nk} \big)^2 \right\} =$$
$$= \lim\limits_{n \rightarrow +\infty} \frac{1}{V_n^2} \max\limits_{i = 1, \ldots, n} \Big\{ \underbrace{ \E \big( X_{nk} - m_{nk} \big)^2 \mathbb{I}_{ \{ |X_{nk} - m_{nk}| \geq \varepsilon V_n \} } }_{ \overset{n}{\rightarrow} 0 \text{ por hipótesis } } + \E \big( X_{nk} - m_{nk} \big)^2 \mathbb{I}_{ \{ |X_{nk} - m_{nk}| < \varepsilon V_n \} } \Big\} \leq$$
$$\leq \frac{ \delta }{ 2 } + \lim\limits_{n \rightarrow +\infty} \frac{1}{V_n^2} \max\limits_{i = 1, \ldots, n} \left\{ \E \big( X_{nk} - m_{nk} \big)^2 \mathbb{I}_{ \{ |X_{nk} - m_{nk}| < \varepsilon V_n \} } \right\} =$$
$$= \frac{ \delta }{ 2 } + \lim\limits_{n \rightarrow +\infty} \frac{1}{V_n^2} \max\limits_{i = 1, \ldots, n} \Bigg\{ \underbrace{ \E \big( X_{nk} - m_{nk} \big)^2 \mathbb{I}_{ \left\{ \left| \frac{ X_{nk} - m_{nk} }{ V_n } \right| < \varepsilon \right\} } }_{ \leq \varepsilon^2 \text{ por la indicatriz }} \Bigg\} \leq \frac{\delta}{2} + \varepsilon^2 \leq \delta$$

Luego, dado que $\delta$ es arbitrario, elijo $\varepsilon$ tal que $\varepsilon^2 < \frac{\delta}{2}$, y entonces el límite tiende a cero.

## Parte b

Demostrar que:
$$\text{Si } \lim\limits_{n \rightarrow + \infty} \frac{1}{V_n^2} \sum\limits_{k = 1}^{n} \E \left[ \big( X_{nk} - m_{nk} \big)^2 \, \mathbb{I}_{ \{ |X_{nk} - m_{nk}| \geq \varepsilon V_n \} } \right] = 0 \Rightarrow \max\limits_{i = 1, \ldots, n} \Big\{ \Pr \big( |X_{nk} - m_{nk}| \geq \varepsilon V_n \big) \Big\} \overset{n}{\rightarrow} 0$$

*Dem*:
$$\lim\limits_{n \rightarrow + \infty} \frac{1}{V_n^2} \sum\limits_{k = 1}^{n} \E \left[ \big( X_{nk} - m_{nk} \big)^2 \, \mathbb{I}_{ \{ |X_{nk} - m_{nk}| \geq \varepsilon V_n \} } \right] \geq$$
$$\geq \lim\limits_{n \rightarrow + \infty} \frac{1}{V_n^2} \max\limits_{k = 1, \ldots, n} \left\{ \E \left[ \big( X_{nk} - m_{nk} \big)^2 \, \mathbb{I}_{ \{ |X_{nk} - m_{nk}| \geq \varepsilon V_n \} } \right] \right\} =$$
$$= \lim\limits_{n \rightarrow + \infty} \max\limits_{k = 1, \ldots, n} \left\{ \E \left[ \left( \frac{ X_{nk} - m_{nk} }{V_n} \right)^2 \, \mathbb{I}_{ \left\{ \left| \frac{ X_{nk} - m_{nk} }{ V_n } \right| \geq \varepsilon \right\} } \right] \right\} \geq$$
$$\geq \lim\limits_{n \rightarrow + \infty} \max\limits_{k = 1, \ldots, n} \left\{ \E \left[ \varepsilon^2 \, \mathbb{I}_{ \left\{ \left| \frac{ X_{nk} - m_{nk} }{ V_n } \right| \geq \varepsilon \right\} } \right] \right\} = \lim\limits_{n \rightarrow + \infty} \max\limits_{k = 1, \ldots, n} \left\{ \varepsilon^2 \E \left[ \mathbb{I}_{ \left\{ \left| \frac{ X_{nk} - m_{nk} }{ V_n } \right| \geq \varepsilon \right\} } \right] \right\} =$$
$$= \lim\limits_{n \rightarrow + \infty} \max\limits_{k = 1, \ldots, n} \left\{ \varepsilon^2 \Pr \left( \left| \frac{ X_{nk} - m_{nk} }{ V_n } \right| \geq \varepsilon \right) \right\} = \lim\limits_{n \rightarrow + \infty} \max\limits_{k = 1, \ldots, n} \left\{ \varepsilon^2 \Pr \big( \left| X_{nk} - m_{nk} \right| \geq \varepsilon \, V_n \big) \right\} \overset{n}{\rightarrow} 0$$
donde el límite se cumple dado que está acotado inferiormente por cero (por ser una probabilidad), y superiormente por una expresión que tiende a cero (por hipótesis).

\newpage

# Ejercicio 8

Primero debemos establecer que efectivamente estamos en las condiciones del teorema de Lindeberg. Esto se cumple dado que si las variables son independientes, entonces la suma de las varianzas es la varianza de la suma: $V_n = \V \big(S_n\big) = \sum\limits_{j = 1}^{n} \sigma_{nj}^2$ donde $\sigma_{nj}^2$ es la varianza de la $j$-ésima variable aleatoria en la sucesión. 

Por otro lado, debemos establecer también que las variables deben estar centradas. Esto no nos hace perder generalidad, dado que, si no fuera así, entonces solo debemos centrarlas (restar la media a cada una). Luego entonces:

$$\lim\limits_{n \rightarrow +\infty} \frac{1}{V_n} \sum\limits_{j = 1}^{n} \E \left[ \left| X_j \right|^2 \, \mathbb{I}_{ \left\{ |X_j| > \varepsilon \, \sqrt{V_n} \right\} } \right] \leq \lim\limits_{n \rightarrow +\infty} \frac{1}{V_n} \sum\limits_{j = 1}^{n} \E \left[ K^2 \, \mathbb{I}_{ \left\{ |X_j| > \varepsilon \, \sqrt{V_n} \right\} } \right] =$$
$$= \lim\limits_{n \rightarrow +\infty} \frac{1}{V_n} \sum\limits_{j = 1}^{n} K^2 \E \left[ \mathbb{I}_{ \left\{ |X_j| > \varepsilon \, \sqrt{V_n} \right\} } \right] = \lim\limits_{n \rightarrow +\infty} \frac{1}{V_n} \sum\limits_{j = 1}^{n} K^2 \Pr \left( |X_j| > \varepsilon \, \sqrt{V_n} \right) =$$
$$= \lim\limits_{n \rightarrow +\infty} \frac{1}{V_n} \sum\limits_{j = 1}^{n} K^2 \Pr \left( |X_j|^2 > \varepsilon^2 \, V_n \right) \leq \lim\limits_{n \rightarrow +\infty} \frac{1}{V_n} \sum\limits_{j = 1}^{n} \frac{ K^2 \, \E \big( |X_j|^2 \big) }{ \varepsilon^2 \, V_n } =$$
$$= \lim\limits_{n \rightarrow +\infty} \frac{1}{V_n} \sum\limits_{j = 1}^{n} \frac{ K^2 \, \sigma_{nj}^2 }{ \varepsilon^2 \, V_n } = \lim\limits_{n \rightarrow +\infty} \frac{ K^2 }{ \varepsilon^2 \, V_n } \underbrace{ \sum\limits_{j = 1}^{n} \frac{ \sigma_{nj}^2 }{ V_n } }_{ = 1 } = \lim\limits_{n \rightarrow +\infty} \frac{ K^2 }{ \varepsilon^2 \, \underbrace{ V_n }_{ \rightarrow +\infty} } = 0$$
donde en la primer desigualdad utilizamos el hecho de que las variables están uniformemente acotadas (por hipótesis), mientras que en la segunda desigualdad utilizamos la desigualdad de Markov.

Dado que las variables cumplen la condición **(L)** de Lindeberg, $\frac{ S_n }{\sqrt{V_n}} \overset{d}{\rightarrow} Z \sim \text{N}(0, \, 1)$.

\newpage

# Ejercicio 9: TLC $(2 + \delta)$ de Lyapunov

\begin{tasks}(2)
   \task* $X_{n1}, \, \ldots, \, X_{nn}$ variables aleatorias independientes $\forall n \geq 1$
   \task $\E \left( X_{nj} \right) = 0$
   \task $\V \left( X_{nj} \right) = \sigma_{nj}^2$
   \task $\exists \delta > 0$ tal que $\frac{ 1 }{ B_n^{1 + \frac{\delta}{2}} } \sum\limits_{j = 1}^{n} \E \big( |X_{nj}|^{2 + \delta} \big) \overset{n}{\rightarrow} 0$
   \task $B_n = \sum\limits_{j = 1}^{n} \sigma_{nj}^2$
\end{tasks}

Probar que $\frac{1}{\sqrt{B_n}} \sum\limits_{j = 1}^{n} X_{nj} \overset{d}{\rightarrow} Z \sim \text{N}(0, 1)$

*Dem*: si probamos que $\frac{ 1 }{ B_n^{1 + \frac{\delta}{2}} } \sum\limits_{j = 1}^{n} \E \big( |X_{nj}|^{2 + \delta} \big) \overset{n}{\rightarrow} 0$ implica que $\frac{ 1 }{ B_n } \sum\limits_{j = 1}^{n} \E \big( |X_{nj}|^2 \mathbb{I}_{ \left\{ |X_{nj}| \geq \varepsilon \sqrt{B_n} \right\} } \big) \overset{n}{\rightarrow} 0$, es decir, que la condición **LY** implica la condición **L**, entonces queda demostrado que $\frac{1}{\sqrt{B_n}} \sum\limits_{j = 1}^{n} X_{nj} \overset{d}{\rightarrow} Z \sim \text{N}(0, 1)$, dado que la condición **L** implica la condición $\mathbf{\Lambda}$ de Lindeberg, la cual es condición necesaria y suficiente para la convergencia.

Comencemos estudiando la relación entre los dos sumandos.
$$\E \big( |X_{nj}|^{2 + \delta} \big) = \int\limits_{ \mathbb{R} } |x|^{2 + \delta} dF_{nj}(x) \geq \int\limits_{|x| \geq \varepsilon \sqrt{B_n} } |x|^{2 + \delta} dF_{nj}(x)$$
Esta desigualdad se cumple dado que en la integral del lado izquierdo estamos integrando sobre todo $\mathbb{R}$ (es decir, sobre toda la recata real), mientras que en la integral de la derecha estamos integrando sobre una semirrecta de $\mathbb{R}$ dado que $\left\{ x : |x| \geq \varepsilon \sqrt{B_n} \right\} \subset \mathbb{R}$.
$$\int\limits_{|x| \geq \varepsilon \sqrt{B_n} } |x|^{2 + \delta} dF_{nj}(x) = \int\limits_{|x| \geq \varepsilon \sqrt{B_n} } |x|^2 |x|^{\delta} dF_{nj}(x) = \int\limits_{|x| \geq \varepsilon \sqrt{B_n} } |x|^2 |x|^{\delta} \color{magenta}{\left( \frac{ \varepsilon \sqrt{B_n} }{ \varepsilon \sqrt{B_n} } \right)^{\delta}} \color{black}{dF_{nj}(x)} =$$
$$= \left( \varepsilon \sqrt{B_n} \right)^{\delta} \int\limits_{|x| \geq \varepsilon \sqrt{B_n} } |x|^2 \left| \frac{ x }{ \varepsilon \sqrt{B_n} } \right|^{\delta} dF_{nj}(x)$$
Luego, nuestro de integración establece que $|x| \geq \varepsilon \sqrt{B_n}$, por lo tanto, $\left| \frac{ x }{ \varepsilon \sqrt{B_n} } \right| \geq 1$ de donde obtenemos que $\left| \frac{ x }{ \varepsilon \sqrt{B_n} } \right|^{\delta} \geq 1^{\delta} = 1$, con lo que se cumple entonces la siguiente desigualdad (dado que hacemos más pequeño el integrando):
$$= \left( \varepsilon \sqrt{B_n} \right)^{\delta} \int\limits_{|x| \geq \varepsilon \sqrt{B_n} } |x|^2 \underbrace{ \left| \frac{ x }{ \varepsilon \sqrt{B_n} } \right|^{\delta}}_{ \geq 1 } dF_{nj}(x) \geq \left( \varepsilon \sqrt{B_n} \right)^{\delta} \int\limits_{|x| \geq \varepsilon \sqrt{B_n} } |x|^2 dF_{nj}(x) =$$
$$= \left( \varepsilon \sqrt{B_n} \right)^{\delta} \E \left[ |X_{nj}|^2 \, \mathbb{I}_{ \left\{ |X_{nj}| \geq \varepsilon \sqrt{B_n} \right\} } \right]$$

Por lo tanto, hemos hallado que:
$$\E \big( |X_{nj}|^{2 + \delta} \big) \geq \left( \varepsilon \sqrt{B_n} \right)^{\delta} \E \left[ |X_{nj}|^2 \, \mathbb{I}_{ \left\{ |X_{nj}| \geq \varepsilon \sqrt{B_n} \right\} } \right]$$
Lo cual implica que:
$$\E \left[ |X_{nj}|^2 \, \mathbb{I}_{ \left\{ |X_{nj}| \geq \varepsilon \sqrt{B_n} \right\} } \right] \leq \frac{ \E \big( |X_{nj}|^{2 + \delta} \big) }{ \left( \varepsilon \sqrt{B_n} \right)^{\delta} }$$

Si ahora, sumamos sobre las $j$ variables, multiplicamos por $1 / B_n$ y tomamos límites cuando $n \rightarrow + \infty$, obtenemos que:
$$\lim\limits_{n \rightarrow +\infty} \frac{1}{B_n} \sum\limits_{j = 1}^{n} \E \left[ |X_{nj}|^2 \, \mathbb{I}_{ \left\{ |X_{nj}| \geq \varepsilon \sqrt{B_n} \right\} } \right] \leq \lim\limits_{n \rightarrow +\infty} \frac{1}{B_n} \sum\limits_{j = 1}^{n} \frac{ \E \big( |X_{nj}|^{2 + \delta} \big) }{ \left( \varepsilon \sqrt{B_n} \right)^{\delta} } =$$
$$= \lim\limits_{n \rightarrow +\infty} \sum\limits_{j = 1}^{n} \frac{ \E \big( |X_{nj}|^{2 + \delta} \big) }{ \varepsilon^{\delta} \, B_n^{1 + \frac{\delta}{2}}} = \frac{1}{\varepsilon^{\delta}} \underbrace{ \left[ \frac{1}{B_n^{1 + \frac{\delta}{2}}} \lim\limits_{n \rightarrow +\infty} \sum\limits_{j = 1}^{n} \E \big( |X_{nj}|^{2 + \delta} \big) \right] }_{ \overset{n}{\rightarrow} 0 \text{ por hipótesis}} \overset{n}{\rightarrow} 0$$

# Ejercicio 10

$(\Rightarrow)$ Si $\frac{1}{\sqrt{B_n}} \sum\limits_{j = 1}^{n} \big( X_j - \E(X_j) \big) \overset{d}{\rightarrow} Z \sim \text{N}(0, \, 1) \Rightarrow \sqrt{B_n} \overset{n}{\rightarrow} +\infty$

Para demostrar la condición necesaria, utilizamos el condición $\mathbf{\Lambda}$ de Lindeberg:
$$\forall \varepsilon > 0, \,\,\, \frac{1}{\sqrt{B_n}} \sum\limits_{j = 1}^{n} \int\limits_{|x| > \varepsilon \sqrt{B_n}} |x| \, \big| F_{j}(x) - \Phi_{j}(x) \big| dx \overset{n}{\rightarrow} 0 \Leftrightarrow \frac{1}{\sqrt{B_n}} \sum\limits_{j = 1}^{n} \big( X_j - \E(X_j) \big) \overset{d}{\rightarrow} Z \sim \text{N}(0, \, 1)$$

Supongamos por absurdo que $\sqrt{B_n} \overset{n}{\rightarrow} b < +\infty$. Si esto ocurre, entonces alcanza con demostrar que al menos uno de los $n$ sumandos anteriores es mayor que una constante $c \in \mathbb{R}$ para que la condición de Lindeberg no se cumpla. Para probarlo debemos tener en cuenta que integrar entre $|x| > \varepsilon \sqrt{B_n}$ implica integrar las colas de la distribución. Al observar el integrando, vemos que lo que estamos intentando probar es que la función de distribución de la variables $X_j$ tiende a la de una variable aleatoria con distribución normal estándar. Supongamos, sin pérdida de generalidad de que el intervalo $(a, \, b) \subset \big\{x < \varepsilon \, \sqrt{B_n}  \big\}$. Luego entonces, si trabajamos con la cola derecha (lo mismo podría hacerse con la cola izquierda), obtenemos que:
$$\int\limits_{|x| > \varepsilon \sqrt{B_n}} |x| \, \big| \Phi_{j}(x) \big| dx = \int\limits_{|x| > \varepsilon \sqrt{B_n}} |x| \, \big| F_{j}(x) - \Phi_{j}(x) \big| dx \int\limits_{x > \varepsilon \sqrt{B_n}} x d\Phi_{j}x > c \in \mathbb{R}$$
donde, en la primera igualdad utilizamos que las variables tienen soporte acotado, y en la segunda igualdad utilizamos que, por estar trabajando con la con la derecha $x$ es positiva.

Dado que hemos encontrado (al menos) un sumando que no tiende a cero, y $\sqrt{B_n} \overset{n}{\rightarrow} b < +\infty$, entonces no se cumple la condición $\mathbf{\Lambda}$ de Lindeberg. Por lo tanto, para que sí se cumpla dicha condición $\sqrt{B_n} \overset{n}{\rightarrow} +\infty$.

\newpage

$(\Leftarrow)$ Si $\sqrt{B_n} \overset{n}{\rightarrow} +\infty \Rightarrow \frac{1}{\sqrt{B_n}} \sum\limits_{j = 1}^{n} \big( X_j - \E(X_j) \big) \overset{d}{\rightarrow} Z \sim \text{N}(0, \, 1)$

Para probar la condición suficiente, basta con tomar $\delta = 1$ y probar que se cumple la condición de **LY**. Es decir, alcanza con probar que $\frac{1}{ \sqrt{B_n^3} } \sum\limits_{j = 1}^{n} \E \big[ (X_{j} - \mu_j)^3 \big] \overset{n}{\rightarrow} 0$. Para ello veamos que:

$$\frac{1}{ \sqrt{B_n^3} } \sum\limits_{j = 1}^{n} \E \big[ (X_{j} - \mu_j)^3 \big] = \frac{1}{ B_n \, \sqrt{B_n} } \sum\limits_{j = 1}^{n} \E \big[ (X_{j} - \mu_j)^2 \, (X_{j} - \mu_j) \big] \leq$$
$$\leq \frac{1}{ B_n \, \sqrt{B_n} } \sum\limits_{j = 1}^{n} \E \big[ (X_{j} - \mu_j)^2 \, (b - a) \big] = \frac{(b - a)}{ \sqrt{B_n} } \underbrace{ \left[ \frac{1}{B_n} \sum\limits_{j = 1}^{n} \E \big[ (X_{j} - \mu_j)^2 \big] \right] }_{ = 1} \overset{n}{\rightarrow} 0$$
donde la desigualdad se cumple dado que $X$ es una variable aleatoria de recorrido acotado entre $a$ y $b$.

# Ejercicio 11: Teorema del Límite de Poisson

Primero debemos derivar la función característica de una variable aleatoria con distribución Bernoulli.
$$\varphi_{X_{nj}}(t) = \E \left( e^{itX_{nj}} \right) = e^{it(0)} \lambda_{nj}^{(0)} \big( 1 - \lambda_{nj} \big)^{1 - (0)} + e^{it(1)} \lambda_{nj}^{(1)} \big( 1 - \lambda_{nj} \big)^{1 - (1)} = 1 - \lambda_{nj} + e^{it} \lambda_{nj}$$

Luego, dado que las variables aleatorias son independientes, la función característica de su suma será:
$$\varphi_{T_n}(t) = \prod\limits_{j = 1}^{n} \varphi_{X_{nj}}(t) = \prod\limits_{j = 1}^{n} \left[ 1 - \lambda_{nj} + e^{it} \lambda_{nj} \right]$$

Por último, debemos probar que estamos en las condiciones de la sugerencia provista, tomando, para este caso $z_{nj} = e^{it} \lambda_{nj} - \lambda_{nj}$.

i) $\sum\limits_{j = 1}^{n} z_{nj} = \sum\limits_{j = 1}^{n} \left[ e^{it} \lambda_{nj} - \lambda_{nj} \right] = \sum\limits_{j = 1}^{n} e^{it} \lambda_{nj} - \sum\limits_{j = 1}^{n} \lambda_{nj} \overset{n}{\rightarrow} \lambda e^{it} - \lambda = \lambda(e^{it} - 1)$

ii) dado que $\max\limits_j(\lambda_{nj}) \leq \sum\limits_{j = 1}^{n} \lambda_{nj}$ tenemos que:
$$\delta_n = \max\limits_j |z_{nj}| = \max\limits_j \left| e^{it} \lambda_{nj} - \lambda_{nj} \right| = \max\limits_j \left| \lambda_{nj} \big( e^{it} - 1\big) \right| = \left| e^{it} - 1 \right| \max\limits_j \left| \lambda_{nj} \right| =$$
$$= \left| e^{it} - 1 \right| \max\limits_j \left\{ \sqrt{ \lambda_{nj}^2 } \right\} = \left| e^{it} - 1 \right| \sqrt{ \max\limits_j \left( \lambda_{nj}^2 \right) } \leq \underbrace{ \left| e^{it} - 1 \right| }_{\text{acotado}} \underbrace{ \sqrt{ \sum\limits_{j = 1}^{n} \lambda_{nj}^2 } }_{ \overset{n}{\rightarrow} 0} \overset{n}{\rightarrow} 0$$

iii) $\underbrace{ \delta_n }_{\overset{n}{\rightarrow} 0} \underbrace{ \sum\limits_{j = 1}^{n} z_{nj} }_{\text{acotado}} \overset{n}{\rightarrow} 0$

Por lo tanto:
$$\varphi_{T_n}(t) = \prod\limits_{j = 1}^{n} \varphi_{X_{nj}}(t) = \prod\limits_{j = 1}^{n} \left[ 1 - \lambda_{nj} + e^{it} \lambda_{nj} \right] \overset{n}{\rightarrow} \exp\left\{ \lambda \left( e^{it} - 1 \right) \right\}$$
lo cual es la función característica de una variable aleatoria con distribución $\text{Poisson}(\lambda)$.

# Ejercicio 12

Dado que $X_{n1}, \, \ldots, X_{nn}$ son independientes con distribución Bernoulli, $\text{Rec}(X_{nj}) = \{0, \, 1\}$ para toda variable en la sucesión. Por lo tanto, también lo será para el máximo. Luego entonces:
$$\Pr \left( M_n = 0 \right) = \Pr \left( \text{``todas sean cero''} \right) = \prod\limits_{j = 1}^{n} \left( 1 - \lambda_{nj} \right) = \prod\limits_{j = 1}^{n} \big( 1 + (- \lambda_{nj}) \big) \overset{n}{\rightarrow} e^{-\lambda}$$
donde, al igual que en el Teorema del Límite de Poisson, la convergencia se cumple dado que:

i) $\sum\limits_{j = 1}^{n} (-\lambda_{nj}) = - \sum\limits_{j = 1}^{n} \lambda_{nj} \overset{n}{\rightarrow} -\lambda$

ii) $\delta_n = \max\limits_j|-\lambda_{nj}| = \max\limits_j|\lambda_{nj}| = \sqrt{ \max\limits_j \left( \lambda_{nj}^2 \right) } \leq \sqrt{ \sum\limits_{j = 1}^{n} \lambda_{nj}^2 } \overset{n}{\rightarrow} 0$

iii) $\delta_n \sum\limits_{j = 1}^{n} \left| -\lambda_{nj} \right| = \delta_n \sum\limits_{j = 1}^{n} \left| \lambda_{nj} \right| \overset{n}{\rightarrow} 0$

Por lo tanto, dado que la probabilidad de fracaso es $e^{-\lambda}$, la probabilidad de éxito debe ser:
$$\Pr\left( M_n = 1 \right) = 1 - \Pr\left( M_n = 0 \right) = 1 - e^{-\lambda}$$

Por lo que $M_n \overset{d}{\rightarrow} M \sim \text{Ber}\left( 1 - e^{-\lambda} \right)$.

# Ejercicio 13: Método Delta

Comencemos notando que, en este caso, $\mu$ no es la esperanza de las variables aleatorias $Y_n$. Para demostrar el teorema del Método Delta debemos utilizar un desarrollo de Taylor de primer orden de $f(Y_n)$ entorno a $\mu$, donde $f$ es una función derivable en $\mu$ y con derivada continua en $\mu$. Así, obtenemos que:
$$f(Y_n(\omega)) = f(\mu) + f'(X_n(\omega)) ( Y_n(\omega) - \mu )$$
donde $0 \leq |X_n(\omega) - \mu| \leq |Y_n(\omega) - \mu|$. Luego multiplicamos ambos lados de la igualdad por $\sqrt{n}$ y re-acomodamos términos para obtener:
$$\begin{array}{rcl}
\sqrt{n} \Big[ f(Y_n(\omega)) - f(\mu) \Big] & = & \sqrt{n} \Big[ f'(X_n(\omega)) \big( Y_n(\omega) - \mu \big) \Big] \\ \\
    & = & f'(X_n(\omega)) \underbrace{ \Big[ \sqrt{n} \big( Y_n(\omega) - \mu \big) \Big] }_{\overset{d}{\rightarrow} V \sim \text{N}(0, \, \sigma^2)}
\end{array}$$

Nótese que cuando trabajamos con la variable aleatoria $Y_n$, su derivada debemos evaluarla en $X_n$ y no en $\mu$ (como haríamos con un desarrollo de Taylor de una función no aleatoria). Debemos entonces demostrar la convergencia de $f'(X_n) \overset{p}{\rightarrow} f'(\mu)$.

Para demostrar la convergencia de $f'(X_n)$ debemos utilizar el hecho de que $X_n$ está acotado superiormente por $Y_n$ (dado que no sabemos nada acerca de $X_n$ que podamos utilizar para demostrar su convergencia de forma más directa). Entonces:
$$\begin{array}{rcl}
\Pr \left( |Y_n - \mu| > \varepsilon \right) & = & \Pr \left( \sqrt{n} |Y_n - \mu| > \sqrt{n} \,\varepsilon \right) \\ \\
   & = & \Pr \left( \sqrt{n} |Y_n - \mu| > \sqrt{n} \,\varepsilon \right) \\ \\
   & = & \Pr \left( \sqrt{n} ( Y_n - \mu ) < - \sqrt{n} \,\varepsilon \right) + \Pr \left( \sqrt{n} ( Y_n - \mu ) > \sqrt{n} \,\varepsilon \right) \\ \\
   & = & \Pr \left( \sqrt{n} ( Y_n - \mu ) < - \sqrt{n} \,\varepsilon \right) + 1 - \Pr \left( \sqrt{n} ( Y_n - \mu ) < \sqrt{n} \,\varepsilon \right) \\ \\
   & = & \underbrace{ F_{\sqrt{n} ( Y_n - \mu )} \big( \underbrace{ - \sqrt{n} \,\varepsilon }_{ \overset{n}{\rightarrow} - \infty } \big) }_{\overset{n}{\rightarrow} 0 } + 1 - \underbrace{ F_{\sqrt{n} ( Y_n - \mu )} \big( \underbrace{ \sqrt{n} \,\varepsilon }_{ \overset{n}{\rightarrow} + \infty } \big) }_{ \overset{n}{\rightarrow} 1 } \overset{n}{\rightarrow} 0 \Rightarrow Y_n \overset{p}{\rightarrow} \mu
\end{array}$$
Luego, dado que $X_n$ fue construido de forma tal que $\big\{ | X_n - \mu | > \varepsilon \big\} \subset \big\{ | Y_n - \mu | > \varepsilon \big\}$ obtenemos que:
$$0 \leq \Pr \left( |X_n - \mu| > \varepsilon \right) \leq \Pr \left( |Y_n - \mu| > \varepsilon \right) \overset{n}{\rightarrow} 0 \Rightarrow X_n \overset{p}{\rightarrow} \mu$$

Por último, dado que $f$ es tal que su derivada es continua, al menos en un entorno de $\mu$, tenemos que $f'(X_n) \overset{p}{\rightarrow} f'(\mu)$. Ahora, utilizando el teorema de Slutsky, podemos afirmar que:
$$\sqrt{n} \Big[ f(Y_n(\omega)) - f(\mu) \Big] = \underbrace{ f'(X_n(\omega)) }_{ \overset{p}{\rightarrow} \mu } \underbrace{ \Big[ \sqrt{n} \big( Y_n(\omega) - \mu \big) \Big] }_{\overset{d}{\rightarrow} V \sim \text{N}(0, \, \sigma^2)} \overset{d}{\rightarrow} W \sim \text{N} \Big( 0, \sigma^2(f'(\mu))^2 \Big)$$

\newpage

# Ejercicio 14

## Parte a

Probar que si $\E \left( |X|^n \right) < +\infty$ entonces: $\E \left( |X|^n \right) = n \int\limits_{0}^{+\infty} x^{n - 1} \big( F_X(-x) + 1 - F_X(x) \big) dx$

*Dem*:
$$\begin{array}{rcl}
\E \left( |X|^n \right) & = & \int\limits_{-\infty}^{+\infty} |x|^n dF_X(x) \\
   & = & \int\limits_{-\infty}^{0} |x|^n dF_X(x) + \int\limits_{0}^{+\infty} |x|^n dF_X(x) \\
   & = & \underbrace{ \int\limits_{-\infty}^{0} (-x)^n dF_X(x) }_{I_1} + \underbrace{ \int\limits_{0}^{+\infty} x^n dF_X(x) }_{I_2}
\end{array}$$

Comenzamos trabajando con $I_1$. Para ello primero realizamos el cambio de variable $y = -x$ y luego integramos por partes:
$$\begin{array}{rcl}
I_1 = \int\limits_{-\infty}^{0} (-x)^n dF_X(x) & = & \int\limits_{+\infty}^{0} y^n dF_X(-y) \\ \\
    & = & \underbrace{ y^n F_X(-y) \Big|_{0}^{+\infty} }_{ \overset{y}{\rightarrow} 0 \text{ por lema 3.16}} - \int\limits_{+\infty}^{0}  F_X(-y) d(y^n) \\ \\ 
    & = & \int\limits_{0}^{+\infty} n y^{n - 1} F_X(-y) dy \\ \\
    & = & \int\limits_{0}^{+\infty} n x^{n - 1} F_X(-x) dx \\
\end{array}$$

Trabajando con $I_2$ obtenemos que, si integramos por partes:
$$\begin{array}{rcl}
I_2 & = & \int\limits_{0}^{+\infty} x^n dF_X(x) \\ \\
    & = & \int\limits_{0}^{+\infty} (-x^n) d(1 - F_X(x)) \\ \\
    & = & \underbrace{ (-x^n) \big[ 1 - F_X(x) \big] \Big|_{0}^{+\infty} }_{ \overset{x}{\rightarrow} 0 \text{ por lema 3.16}} - \int\limits_{0}^{+\infty} \big[ 1 - F_X(x) \big] d(-x^n) \\ \\
    & = & \int\limits_{0}^{+\infty} n x^{n - 1} \big[ 1 - F_X(x) \big] dx 
\end{array}$$

Por lo tanto, concluimos que:
$$\E \left( |X|^n \right) = I_1 + I_2 = \int\limits_{0}^{+\infty} n x^{n - 1} F_X(-x) dx + \int\limits_{0}^{+\infty} n x^{n - 1} \big[ 1 - F_X(x) \big] dx =$$
$$= \int\limits_{0}^{+\infty} \Big( n x^{n - 1} F_X(-x) + n x^{n - 1} \big[ 1 - F_X(x) \big] \Big) dx = \int\limits_{0}^{+\infty} n x^{n - 1} \big[ F_X(-x) + 1 - F_X(x) \big] dx$$

## Parte b

$$\begin{array}{rcl}
\int\limits_{-\infty}^{+\infty} |x| \, \big| F_{nk}(x) - \Phi_{nk}(x) \big| dx & = & \int\limits_{-\infty}^{0} |x| \, \big| F_{nk}(x) - \Phi_{nk}(x) \big| dx + \int\limits_{0}^{+\infty} |x| \, \big| F_{nk}(x) - \Phi_{nk}(x) \big| dx \\ \\
   & = & \int\limits_{-\infty}^{0} (-x) \, \big| F_{nk}(x) - \Phi_{nk}(x) \big| dx + \int\limits_{0}^{+ \infty} x \, \big| 1 - 1 + F_{nk}(x) - \Phi_{nk}(x) \big| dx \\ \\
   & = & \int\limits_{-\infty}^{0} (-x) \, \underbrace{ \big| F_{nk}(x) - \Phi_{nk}(x) \big| }_{ \leq \big| F_{nk}(x) \big| + \big| \Phi_{nk}(x) \big| } dx + \int\limits_{0}^{+\infty} x \, \underbrace{ \big| (F_{nk}(x) - 1) + (1 - \Phi_{nk}(x)) \big| }_{ \leq \big| 1 - F_{nk}(x) \big| + \big| 1 - \Phi_{nk}(x) \big| } dx \\ \\
   & \leq & \int\limits_{-\infty}^{0} (-x) \Big[ \big| F_{nk}(x) \big| + \big| \Phi_{nk}(x) \big| \Big] dx + \int\limits_{0}^{+ \infty} x \Big[ \big| 1 - F_{nk}(x) \big| + \big| 1 - \Phi_{nk}(x) \big| \Big] dx \\ \\
\end{array}$$
Dado que tanto $F_{nk}$ como $\Phi_{nk}$ son funciones de distribución, $F_{nk}(x) > 0$ y $\Phi_{nk}(x) > 0$ para todo $x$. Adicionalmente, realizamos el siguiente cambio de variable:
$$\begin{array}{lcl}
u = -x \Rightarrow x = -u & & \frac{dx}{du} = -1 \Rightarrow dx = -du \\ \\
\text{Si } x \rightarrow -\infty \Rightarrow u \rightarrow + \infty & & \text{Si } x = 0 \Rightarrow u = 0
\end{array}$$
de donde obtenemos que:
$$\begin{array}{rcl}
\int\limits_{- \infty}^{+\infty} |x| \, \big| F_{nk}(x) - \Phi_{nk}(x) \big| dx & \leq & \int\limits_{- \infty}^{0} (-x) \Big[ F_{nk}(x) + \Phi_{nk}(x) \Big] dx + \int\limits_{0}^{+ \infty} x \Big[ \big( 1 - F_{nk}(x) \big) + \big( 1 - \Phi_{nk}(x) \big) \Big] dx \\ \\ 
   & = & - \int\limits_{+\infty}^{0} u \Big[ F_{nk}(-u) + \Phi_{nk}(-u) \big| \Big] du + \int\limits_{0}^{+ \infty} x \Big[ \big( 1 - F_{nk}(x) \big) + \big( 1 - \Phi_{nk}(x) \big) \Big] dx \\ \\ 
   & = & \int\limits_{0}^{+\infty} u \Big[ F_{nk}(-u) + \Phi_{nk}(-u) \Big] du + \int\limits_{0}^{+ \infty} x \Big[ \big( 1 - F_{nk}(x) \big) + \big( 1 - \Phi_{nk}(x) \big) \Big] dx \\ \\ 
   & = & \int\limits_{0}^{+\infty} x \Big[ F_{nk}(-x) + \Phi_{nk}(-x) \Big] dx + \int\limits_{0}^{+ \infty} x \Big[ \big( 1 - F_{nk}(x) \big) + \big( 1 - \Phi_{nk}(x) \big) \Big] dx \\ \\ 
   & = & \int\limits_{0}^{+\infty} x \Bigg[ F_{nk}(-x) + \Phi_{nk}(-x) + \big( 1 - F_{nk}(x) \big) + \big( 1 - \Phi_{nk}(x) \big) \Bigg] dx \\ \\ 
   & = & \int\limits_{0}^{+\infty} x \Bigg[ \big( F_{nk}(-x) + 1 - F_{nk}(x) \big) + \big( 1 - \Phi_{nk}(x) + \Phi_{nk}(-x) \big) \Bigg] dx \\ \\ 
   & = & \int\limits_{0}^{+\infty} x \big( F_{nk}(-x) + 1 - F_{nk}(x) \big) dx + \int\limits_{0}^{+\infty} x \big( 1 - \Phi_{nk}(x) + \Phi_{nk}(-x) \big) dx \\ \\ 
\end{array}$$

Por la parte a del ejercicio sabemos que:
$$\E \left( |X|^n \right) = \int\limits_{0}^{+\infty} n x^{n - 1} \big[ F_X(-x) + 1 - F_X(x) \big] dx$$
Luego, si $n = 2$ tenemos que:
$$\E \left( |X|^2 \right) = 2 \int\limits_{0}^{+\infty} x\big[ F_X(-x) + 1 - F_X(x) \big] dx$$

\newpage

Por lo tanto,
$$\begin{array}{rcl}
\int\limits_{-\infty}^{+\infty} |x| \, \big| F_{nk}(x) - \Phi_{nk}(x) \big| dx & \leq & \int\limits_{0}^{+\infty} x \big( F_{nk}(-x) + 1 - F_{nk}(x) \big) dx + \int\limits_{0}^{+\infty} x \big( 1 - \Phi_{nk}(x) + \Phi_{nk}(-x) \big) dx \\ \\ 
   & = & \frac{ \E \big( |X_{nk}|^2 \big) }{2} + \frac{ \E \big( |\Phi_{nk}|^2 \big) }{2} \\ \\
   & = & \frac{ \V \big( X_{nk} \big) }{2} + \frac{ \V \big( \Phi_{nk} \big) }{2} \\ \\
   & = & \frac{ \sigma_{nk}^2 }{2} + \frac{ \sigma_{nk}^2 }{2} = \sigma_{nk}^2 \leq 2 \sigma_{nk}^2 \\ \\   
\end{array}$$
