---
title: "Práctico 2"
author: "Daniel Czarnievicz"
date: "April 19, 2019"
output: pdf_document
header-includes:
   - \everymath{\displaystyle}
geometry: margin=1in
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Ejercicio 1

## Parte a

Si $X_n \overset{cs}{\rightarrow} X$ y $Y_n \overset{cs}{\rightarrow} Y$ entonces $X_n + Y_n \overset{cs}{\rightarrow} X + Y$

*Dem*:

Si $X_n \overset{cs}{\rightarrow} X \Rightarrow \exists A$ con $P(A) = 1$ tal que $\forall \omega \in A$, $X_n(\omega) \overset{n}{\rightarrow} X(\omega)$. Análogamente, si $Y_n \overset{cs}{\rightarrow} Y \Rightarrow \exists B$ con $P(B) = 1$ tal que $\forall \omega \in B$, $Y_n(\omega) \overset{n}{\rightarrow} Y(\omega)$. Sea $D = A \cap B \Rightarrow$ por proposición 1.9/9, $P(D) = P(A \cap B) = 1 \Rightarrow X_n + Y_n \overset{cs}{\rightarrow} X + Y$.

## Parte c

Si $X_n \overset{cs}{\rightarrow} X$ y $g$ es continua, entonces $g(X_n) \overset{cs}{\rightarrow} g(X)$.

*Dem*:

Si $X_n \overset{cs}{\rightarrow} X \Rightarrow \exists A$ con $P(A) = 1$ tal que $\forall \omega \in A$, $X_n(\omega) \overset{n}{\rightarrow} X(\omega)$. Si $g$ es continua, entonces $\forall \omega \in A$, $g(X_n(\omega)) \overset{n}{\rightarrow} g(X(\omega))$. Dado que $P(A) = 1$, $g(X_n) \overset{cs}{\rightarrow} g(X)$.

## Parte e

Si $X_n \overset{cs}{\rightarrow} X$ y $Y_n \overset{cs}{\rightarrow} Y$, y sean $a_n$ y $b_n$ dos sucesiones de números reales tales que $a_n \overset{n}{\rightarrow} a > 0$ y $b_n \overset{n}{\rightarrow} b$, entonces $a_n \, X_n + b_n \, Y_n \overset{cs}{\rightarrow} a \, X + b \, Y$.

*Dem*: alcanza con demostrar que $a_n \, X_n \overset{cs}{\rightarrow} a \, X$ y luego usar el resultado de la parte a del ejercicio.

Si $X_n \overset{cs}{\rightarrow} X \Rightarrow \exists A$ con $P(A) = 1$ tal que $\forall \omega \in A$, $X_n(\omega) \overset{n}{\rightarrow} X(\omega)$. Luego $a_n$ es una sucesión real que sabemos converge a $a$ (por hipótesis), por lo tanto, $\forall \omega \in A$, $a_n \, X_n(\omega) \overset{n}{\rightarrow} a \, X(\omega)$. Dado que $P(A) = 1$, $a_n \, X_n \overset{cs}{\rightarrow} a \, X$.

\newpage

## Parte d

¿Valen los resultado anteriores si $X_n \overset{p}{\rightarrow} X$ y $Y_n \overset{p}{\rightarrow} Y$?

***

**Parte 1**: Si $X_n \overset{p}{\rightarrow} X$ y $Y_n \overset{p}{\rightarrow} Y$ entonces $X_n + Y_n \overset{p}{\rightarrow} X + Y$

\vspace{.3cm}

*Dem*: requiere demostrar que $P(\{ \omega : |X_n(\omega) + Y_n(\omega) - X(\omega) - Y(\omega) | > \varepsilon\} ) \overset{n}{\rightarrow} 0$, $\forall \varepsilon > 0$. Antes de demostrar la convergencia necesitamos explicitar tres resultados:

1. Si $X_n \overset{p}{\rightarrow} X \Rightarrow P(|X_n - X| > \varepsilon / 2) \overset{n}{\rightarrow} 0$, por definición de convergencia en probabilidad.

2. Si $Y_n \overset{p}{\rightarrow} Y \Rightarrow P(|Y_n - Y| > \varepsilon / 2) \overset{n}{\rightarrow} 0$, por definición de convergencia en probabilidad.

3. $\{ \omega \in \Omega : |X_n + Y_n - X - Y| > \varepsilon \} \subset \{ \omega \in \Omega : |X_n - X| + |Y_n - Y| > \varepsilon \}$ por lo tanto, $\varepsilon < |x_n + Y_n - X - Y| \leq |X_n - X| + |Y_n - Y|$

Luego entonces,
$$0 \leq P(|X_n + Y_n - X - Y | > \varepsilon ) \leq P(|X_n - X | > \varepsilon/2 ) + P(|Y_n - Y | > \varepsilon / 2 ) \overset{n}{\rightarrow} 0$$

***

**Parte 2**: Si $X_n \overset{p}{\rightarrow} X$ y $g$ es una función continua entonces $g(X_n) \overset{p}{\rightarrow} g(X)$

\vspace{.3cm}

*Dem*: $g$ es continua $\Leftrightarrow \forall \varepsilon > 0$, $\exists \delta$ tal que si $|X_n(\omega) - X(\omega)| < \delta \Rightarrow |g(X_n(\omega)) - g(X(\omega))| < \varepsilon$. Por contra recíproco esto implíca que si $|g(X_n(\omega)) - g(X(\omega))| \geq \varepsilon \Rightarrow |X_n(\omega) - X(\omega)| \geq \delta$ . Luego entonces:
$$0 \leq P( | g(X_n(\omega)) - g(X(\omega)) | \geq \varepsilon ) \leq P( | X_n(\omega) - X(\omega) | \geq \delta ) \overset{n}{\rightarrow} 0$$
dado que $\{ \omega : |g(X_n(\omega)) - g(X(\omega))| \geq \varepsilon \} \subseteq \{ \omega : |X_n(\omega) - X(\omega)| \geq \delta \}$. Por lo tanto, $g(X_n) \overset{p}{\rightarrow} g(X)$.

***

\newpage

**Parte 3**: si $X_n \overset{p}{\rightarrow} X$ y $Y_n \overset{p}{\rightarrow} Y$, y sean $a_n$ y $b_n$ dos sucesiones de números reales tales que $a_n \overset{n}{\rightarrow} a > 0$ y $b_n \overset{n}{\rightarrow} b$, entonces $a_n \, X_n + b_n \, Y_n \overset{cs}{\rightarrow} a \, X + b \, Y$.

\vspace{.3cm}

*Dem*: al igual que con la convergencia $cs$, alcanza con demostrar que $a_n \, X_n \overset{p}{\rightarrow} a \, X$. Esto implica demostrar que $P(|a_n \, X_n - aX| > \varepsilon) \overset{n}{\rightarrow} 0$.

$$P(|a_n \, X_n - aX| > \varepsilon) = P(|a_n \, X_n - a \, X_n + a \, X_n - a \,X| > \varepsilon) \leq $$
$$ \leq P(|a_n \, X_n - a \, X_n | > \varepsilon / 2) + P(| a \, X_n - a \,X | > \varepsilon / 2) = $$
$$= P(|a_n - a| \, | X_n | > \varepsilon / 2) + P(| a | \, | X_n - X | > \varepsilon / 2) $$
$$= P(|a_n - a| \, | X_n - X + X | > \varepsilon / 2) + \underbrace{P(| a | \, | X_n - X | > \varepsilon / 2) }_{\rightarrow 0 \text{,  porque } X_n \overset{p}{\rightarrow} X} \leq $$
$$ \leq \underbrace{P(|a_n - a| \, | X_n - X | > \varepsilon / 4)}_{{\rightarrow 0 \text{,  porque } X_n \overset{p}{\rightarrow} X}} + P(|a_n - a| \, | X | > \varepsilon / 4) \leq $$
$$ = P \Bigg( \underbrace{|a_n - a|}_{ \substack{ \rightarrow 0 \text{, dado } \\ \text{que } a_n \overset{n}{\rightarrow} a} } > \frac{\varepsilon}{4|X|} \Bigg) \overset{n}{\rightarrow} 0 $$

Procediendo de forma análoga se demuestra que $b_n \, Y_n \overset{p}{\rightarrow} b \, Y$, y aplicando el resultado de la parte 1 queda demostrado que $a_n \, X_n + b_n \, Y_n \overset{p}{\rightarrow} a_n \, X + b \, Y$.

# Ejercicio 2

Verificar que si $X_1; \, X_2; \, \ldots$ iid, entonces $\bar{X}_n$ es un estimador consistente para

a. el parámetro $p$, si $X_1 \sim Bin(1,p)$
b. $\lambda$, si $X_1 \sim Poi(\lambda)$
c. $1/\lambda$, si $X_1 \sim Exp(\lambda)$

## Parte a

Por convergencia en media cuadrática:

$$\left.\begin{array}{r}
E(\bar{X}_n) = E(X_1) = p \\ \\
V(\bar{X}_n) = \frac{V(X_1)}{n} = \frac{p(1 - p)}{n} \overset{n}{\rightarrow} 0
\end{array} \right\}
\Rightarrow \bar{X}_n \overset{mc}{\rightarrow} p \Rightarrow \bar{X}_n \overset{p}{\rightarrow} p$$

Utilizando desigualdad de Chebychev:

$$ P (| \bar{X}_n - p | > \varepsilon ) \leq \frac{V(\bar{X}_n)}{\varepsilon^2} = \frac{ V(X_1) / n}{\varepsilon^2 } = \frac{ V(X_1) }{n \, \varepsilon^2 } = \frac{ p(1 - p) }{n \, \varepsilon^2 } \overset{n}{\rightarrow} 0 \Rightarrow \bar{X}_n \overset{p}{\rightarrow} p$$

## Parte b

Por convergencia en media cuadrática:

$$\left.\begin{array}{r}
E(\bar{X}_n) = E(X_1) = \lambda \\ \\
V(\bar{X}_n) = \frac{V(X_1)}{n} = \frac{\lambda}{n} \overset{n}{\rightarrow} 0
\end{array} \right\}
\Rightarrow \bar{X}_n \overset{mc}{\rightarrow} \lambda \Rightarrow \bar{X}_n \overset{p}{\rightarrow} \lambda$$

Utilizando desigualdad de Chebychev:

$$ P (| \bar{X}_n - \lambda | > \varepsilon ) \leq \frac{V(\bar{X}_n)}{\varepsilon^2} = \frac{ V(X_1) / n}{\varepsilon^2 } = \frac{ V(X_1) }{n \, \varepsilon^2 } = \frac{ \lambda }{n \, \varepsilon^2 } \overset{n}{\rightarrow} 0 \Rightarrow \bar{X}_n \overset{p}{\rightarrow} \lambda$$

## Parte c

Por convergencia en media cuadrática:

$$\left.\begin{array}{r}
E(\bar{X}_n) = E(X_1) = \frac{1}{\lambda} \\ \\
V(\bar{X}_n) = \frac{V(X_1)}{n} = \frac{ 1 / \lambda^2}{n} \overset{n}{\rightarrow} 0
\end{array} \right\}
\Rightarrow \bar{X}_n \overset{mc}{\rightarrow} \frac{1}{\lambda} \Rightarrow \bar{X}_n \overset{p}{\rightarrow} \frac{ 1 }{ \lambda}$$

Utilizando desigualdad de Chebychev:

$$ P (| \bar{X}_n - \lambda | > \varepsilon ) \leq \frac{V(\bar{X}_n)}{\varepsilon^2} = \frac{ V(X_1) / n}{\varepsilon^2 } = \frac{ V(X_1) }{n \, \varepsilon^2 } = \frac{ 1 / \lambda^2 }{n \, \varepsilon^2 } \overset{n}{\rightarrow} 0 \Rightarrow \bar{X}_n \overset{p}{\rightarrow} \frac{1}{\lambda}$$

# Ejercicio 3

## Estimador $T_n = 2 \sum\limits_{i = 1}^{n} X_i / n$

Nótese primero que $T_n = 2 \bar{X}_n = g(\bar{X}_n)$ siengo $g(z) = 2z$ y por tanto continua. Por lo tanto, probar que $T_n \overset{cs}{\rightarrow} \theta$ implíca probar que $\bar{X}_n \overset{cs}{\rightarrow} \theta / 2$ (ver parte c del ejercicio 1).

*Dem*: por el teorema de convergencia monótona sabemos que si existe $E(\bar{X}_n)$, $\bar{X}_n(\omega) \geq 0$, y  $\bar{X_n}(\omega) \uparrow \bar{X}(\omega)$ entonces existe $E(\bar{X}_n)$ para todo $n$ y $\lim\limits_{n \rightarrow + \infty} E(\bar{X}_n) = E(\bar{X})$

1. Por ser una muestra iid de variables $Unif(0. \theta]$, sabemos que $E(\bar{X}_n) = E(X_1) = \theta / 2$.  
2. Sabemos que $0 \leq \bar{X}_n \leq \theta$ para todo $n$, por lo tanto, $\bar{X}_n \geq 0$ para todo $n$.  
3. Ni idea :( así que supongamos que sí

Por lo tanto, $\lim\limits_{n \rightarrow + \infty} E(\bar{X}_n) = E(\bar{X}) = E(X_1) = \theta / 2$


$\bar{X}_n \overset{cs}{\rightarrow} \theta / 2 \Rightarrow T_n \overset{cs}{\rightarrow} \theta$

## Estimador $X_{(n)}$

Sea $X_{(n)} = \max\limits_n \{X_1, X_2, \ldots\} \Rightarrow f_{X_{(n)}}(x) = \frac{n x^{n-1}}{\theta^n} \, \mathbb{I}_{(0 < x < \theta]}$. Luego,

$$P(|X_{(n)} - \theta| > \varepsilon) = 1 - P (|X_{(n)} - \theta | < \varepsilon ) = 1 - P(\theta - \varepsilon < X_{(n)} < \theta + \varepsilon) = $$
$$= 1 - P(\theta - \varepsilon < X_{(n)} < \theta) = 1 - \int\limits_{\theta - \varepsilon}^{\theta} f_{X_{(n)}}(x) dx = 1 - \int\limits_{\theta - \varepsilon}^{\theta} \frac{n x^{n-1}}{\theta^n} dx =$$
$$= 1 - \frac{1}{\theta^n} \left( x^n \big|_{\theta - \varepsilon}^{\theta} \right) = 1 - \left( \frac{\theta^n}{\theta^n} - \frac{(\theta - \varepsilon)^n}{\theta^n}\right) = \frac{(\theta - \varepsilon)^n}{\theta^n} = \left( \frac{\theta - \varepsilon}{\theta}\right)^n $$

Luego por el corolario 2.4 sabemos que si $\forall \varepsilon > 0$, $\sum\limits_{n = 1}^{+\infty} P(|X_{(n)} - \theta| > \varepsilon) < \infty \Rightarrow X_{(n)} \overset{cs}{\rightarrow} \theta$. Lo cual se cumple en este caso:

$$ \sum\limits_{n = 1}^{+ \infty} P ( | X_{(n)} - \theta | > \varepsilon ) = \sum\limits_{n = 1}^{+\infty} \left( \frac{ \theta - \varepsilon }{ \theta } \right)^n = - 1 + 1 + \sum\limits_{n = 1}^{+\infty} \left( \frac{ \theta - \varepsilon }{ \theta } \right)^n =$$
$$= - 1 + \left( \frac{ \theta - \varepsilon }{ \theta } \right)^0 + \sum\limits_{n = 1}^{+\infty} \left( \frac{ \theta - \varepsilon }{ \theta } \right)^n = - 1 + \sum\limits_{n = 0}^{+\infty} \left( \frac{ \theta - \varepsilon }{ \theta } \right)^n < +\infty \text{ dado que } \left| \frac{ \theta - \varepsilon }{ \theta } \right| < 1 $$

Por lo tanto, encontramos que $X_{n} \overset{cs}{\rightarrow} \theta$.

# Ejercicio 4

Mostrar que $d_2(X,Y) = \sqrt{ E(X - Y)^2 }$ es una distance en el conjunto delas variables aleatorias con momento de segundo orden finit.

*Dem*: implica probar todas las características de una distance.

1) $d_2(X,Y) > 0$, $\forall X \neq Y$ se cumple dado que $Z = (X - Y)^2 > 0 \Rightarrow \sqrt{E(Z)} > 0$.

2) $d_2(X,Y) = 0 \Leftrightarrow X = Y$.

3) $d_2(X,Y) = \sqrt{E(X - Y)^2} = \sqrt{E(Y - X)^2} = d_2(Y,X)$

4) Desigualdad triangular
$$d_2(X,Y) = \sqrt{ E(X - Y)^2} = \sqrt{ E(X - Z + Z - Y)^2} = $$
$$= \sqrt{ E (X - Z)^2 + 2 E(X - Z)(Z - Y) + E(Z - Y)^2 \Big]} \leq$$
$$\leq \sqrt{ E (X - Z)^2 + 2 \sqrt{ E(X - Z)^2 \, E(Z - Y)^2 } + E(Z - Y)^2} \leq$$
$$\leq \sqrt{ E (X - Z)^2 + E(Z - Y)^2} \leq \sqrt{ E (X - Z)^2} + \sqrt{ E(Z - Y)^2 } = d_2(X,Z) + d_2(Z,Y)$$

