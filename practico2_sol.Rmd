---
title: "Práctico 2"
author: "Daniel Czarnievicz"
date: "2019"
output: pdf_document
header-includes:
   - \everymath{\displaystyle}
   - \usepackage{xcolor}
   - \usepackage[makeroom]{cancel}
geometry: margin=1in
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Ejercicio 1

## Parte a

Si $X_n \overset{cs}{\rightarrow} X$ y $Y_n \overset{cs}{\rightarrow} Y$ entonces $X_n + Y_n \overset{cs}{\rightarrow} X + Y$

*Dem*:

Si $X_n \overset{cs}{\rightarrow} X \Rightarrow \exists A$ con $P(A) = 1$ tal que $\forall \omega \in A$, $X_n(\omega) \overset{n}{\rightarrow} X(\omega)$. Análogamente, si $Y_n \overset{cs}{\rightarrow} Y \Rightarrow \exists B$ con $P(B) = 1$ tal que $\forall \omega \in B$, $Y_n(\omega) \overset{n}{\rightarrow} Y(\omega)$. Sea $D = A \cap B \Rightarrow$ por proposición 1.9/9, $P(D) = P(A \cap B) = 1 \Rightarrow X_n + Y_n \overset{cs}{\rightarrow} X + Y$.

## Parte c

Si $X_n \overset{cs}{\rightarrow} X$ y $g$ es continua, entonces $g(X_n) \overset{cs}{\rightarrow} g(X)$.

*Dem*:

Si $X_n \overset{cs}{\rightarrow} X \Rightarrow \exists A$ con $P(A) = 1$ tal que $\forall \omega \in A$, $X_n(\omega) \overset{n}{\rightarrow} X(\omega)$. Si $g$ es continua, entonces $\forall \omega \in A$, $g(X_n(\omega)) \overset{n}{\rightarrow} g(X(\omega))$. Dado que $P(A) = 1$, $g(X_n) \overset{cs}{\rightarrow} g(X)$.

## Parte e

Si $X_n \overset{cs}{\rightarrow} X$ y $Y_n \overset{cs}{\rightarrow} Y$, y sean $a_n$ y $b_n$ dos sucesiones de números reales tales que $a_n \overset{n}{\rightarrow} a > 0$ y $b_n \overset{n}{\rightarrow} b$, entonces $a_n \, X_n + b_n \, Y_n \overset{cs}{\rightarrow} a \, X + b \, Y$.

*Dem*: alcanza con demostrar que $a_n \, X_n \overset{cs}{\rightarrow} a \, X$ y luego usar el resultado de la parte a del ejercicio.

Si $X_n \overset{cs}{\rightarrow} X \Rightarrow \exists A$ con $P(A) = 1$ tal que $\forall \omega \in A$, $X_n(\omega) \overset{n}{\rightarrow} X(\omega)$. Luego $a_n$ es una sucesión real que sabemos converge a $a$ (por hipótesis), por lo tanto, $\forall \omega \in A$, $a_n \, X_n(\omega) \overset{n}{\rightarrow} a \, X(\omega)$. Dado que $P(A) = 1$, $a_n \, X_n \overset{cs}{\rightarrow} a \, X$.

\newpage

## Parte d

¿Valen los resultado anteriores si $X_n \overset{p}{\rightarrow} X$ y $Y_n \overset{p}{\rightarrow} Y$?

***

**Parte 1**: Si $X_n \overset{p}{\rightarrow} X$ y $Y_n \overset{p}{\rightarrow} Y$ entonces $X_n + Y_n \overset{p}{\rightarrow} X + Y$

\vspace{.3cm}

*Dem*: requiere demostrar que $P(\{ \omega : |X_n(\omega) + Y_n(\omega) - X(\omega) - Y(\omega) | > \varepsilon\} ) \overset{n}{\rightarrow} 0$, $\forall \varepsilon > 0$. Antes de demostrar la convergencia necesitamos explicitar tres resultados:

1. Si $X_n \overset{p}{\rightarrow} X \Rightarrow P(|X_n - X| > \varepsilon / 2) \overset{n}{\rightarrow} 0$, por definición de convergencia en probabilidad.

2. Si $Y_n \overset{p}{\rightarrow} Y \Rightarrow P(|Y_n - Y| > \varepsilon / 2) \overset{n}{\rightarrow} 0$, por definición de convergencia en probabilidad.

3. $\{ \omega \in \Omega : |X_n + Y_n - X - Y| > \varepsilon \} \subset \{ \omega \in \Omega : |X_n - X| + |Y_n - Y| > \varepsilon \}$ por lo tanto, $\varepsilon < |x_n + Y_n - X - Y| \leq |X_n - X| + |Y_n - Y|$

Luego entonces,
$$0 \leq P(|X_n + Y_n - X - Y | > \varepsilon ) \leq P(|X_n - X | > \varepsilon/2 ) + P(|Y_n - Y | > \varepsilon / 2 ) \overset{n}{\rightarrow} 0$$

***

**Parte 2**: Si $X_n \overset{p}{\rightarrow} X$ y $g$ es una función continua entonces $g(X_n) \overset{p}{\rightarrow} g(X)$

\vspace{.3cm}

*Dem*: $g$ es continua $\Leftrightarrow \forall \varepsilon > 0$, $\exists \delta$ tal que si $|X_n(\omega) - X(\omega)| < \delta \Rightarrow |g(X_n(\omega)) - g(X(\omega))| < \varepsilon$. Por contra recíproco esto implíca que si $|g(X_n(\omega)) - g(X(\omega))| \geq \varepsilon \Rightarrow |X_n(\omega) - X(\omega)| \geq \delta$ . Luego entonces:
$$0 \leq P( | g(X_n(\omega)) - g(X(\omega)) | \geq \varepsilon ) \leq P( | X_n(\omega) - X(\omega) | \geq \delta ) \overset{n}{\rightarrow} 0$$
dado que $\{ \omega : |g(X_n(\omega)) - g(X(\omega))| \geq \varepsilon \} \subseteq \{ \omega : |X_n(\omega) - X(\omega)| \geq \delta \}$. Por lo tanto, $g(X_n) \overset{p}{\rightarrow} g(X)$.

***

\newpage

**Parte 3**: si $X_n \overset{p}{\rightarrow} X$ y $Y_n \overset{p}{\rightarrow} Y$, y sean $a_n$ y $b_n$ dos sucesiones de números reales tales que $a_n \overset{n}{\rightarrow} a > 0$ y $b_n \overset{n}{\rightarrow} b$, entonces $a_n \, X_n + b_n \, Y_n \overset{cs}{\rightarrow} a \, X + b \, Y$.

\vspace{.3cm}

*Dem*: al igual que con la convergencia $cs$, alcanza con demostrar que $a_n \, X_n \overset{p}{\rightarrow} a \, X$. Esto implica demostrar que $P(|a_n \, X_n - aX| > \varepsilon) \overset{n}{\rightarrow} 0$.

$$P(|a_n \, X_n - aX| > \varepsilon) = P(|a_n \, X_n - a \, X_n + a \, X_n - a \,X| > \varepsilon) \leq $$
$$ \leq P(|a_n \, X_n - a \, X_n | > \varepsilon / 2) + P(| a \, X_n - a \,X | > \varepsilon / 2) = $$
$$= P(|a_n - a| \, | X_n | > \varepsilon / 2) + P(| a | \, | X_n - X | > \varepsilon / 2) = $$
$$= P(|a_n - a| \, | X_n - X + X | > \varepsilon / 2) + \underbrace{P(| a | \, | X_n - X | > \varepsilon / 2) }_{\rightarrow 0 \text{,  porque } X_n \overset{p}{\rightarrow} X} \leq $$
$$ \leq \underbrace{P(|a_n - a| \, | X_n - X | > \varepsilon / 4)}_{{\rightarrow 0 \text{,  porque } X_n \overset{p}{\rightarrow} X}} + P(|a_n - a| \, | X | > \varepsilon / 4) \leq $$
$$ = P \Bigg( \underbrace{|a_n - a|}_{ \substack{ \rightarrow 0 \text{, dado } \\ \text{que } a_n \overset{n}{\rightarrow} a} } > \frac{\varepsilon}{4|X|} \Bigg) \overset{n}{\rightarrow} 0 $$

Procediendo de forma análoga se demuestra que $b_n \, Y_n \overset{p}{\rightarrow} b \, Y$, y aplicando el resultado de la parte 1 queda demostrado que $a_n \, X_n + b_n \, Y_n \overset{p}{\rightarrow} a_n \, X + b \, Y$.

# Ejercicio 2

Verificar que si $X_1; \, X_2; \, \ldots$ iid, entonces $\bar{X}_n$ es un estimador consistente para

a. el parámetro $p$, si $X_1 \sim Bin(1,p)$
b. $\lambda$, si $X_1 \sim Poi(\lambda)$
c. $1/\lambda$, si $X_1 \sim Exp(\lambda)$

## Parte a

Por convergencia en media cuadrática:

$$\left.\begin{array}{r}
E(\bar{X}_n) = E(X_1) = p \\ \\
V(\bar{X}_n) = \frac{V(X_1)}{n} = \frac{p(1 - p)}{n} \overset{n}{\rightarrow} 0
\end{array} \right\}
\Rightarrow \bar{X}_n \overset{mc}{\rightarrow} p \Rightarrow \bar{X}_n \overset{p}{\rightarrow} p$$

Utilizando desigualdad de Chebychev:

$$ P (| \bar{X}_n - p | > \varepsilon ) \leq \frac{V(\bar{X}_n)}{\varepsilon^2} = \frac{ V(X_1) / n}{\varepsilon^2 } = \frac{ V(X_1) }{n \, \varepsilon^2 } = \frac{ p(1 - p) }{n \, \varepsilon^2 } \overset{n}{\rightarrow} 0 \Rightarrow \bar{X}_n \overset{p}{\rightarrow} p$$

## Parte b

Por convergencia en media cuadrática:

$$\left.\begin{array}{r}
E(\bar{X}_n) = E(X_1) = \lambda \\ \\
V(\bar{X}_n) = \frac{V(X_1)}{n} = \frac{\lambda}{n} \overset{n}{\rightarrow} 0
\end{array} \right\}
\Rightarrow \bar{X}_n \overset{mc}{\rightarrow} \lambda \Rightarrow \bar{X}_n \overset{p}{\rightarrow} \lambda$$

Utilizando desigualdad de Chebychev:

$$ P (| \bar{X}_n - \lambda | > \varepsilon ) \leq \frac{V(\bar{X}_n)}{\varepsilon^2} = \frac{ V(X_1) / n}{\varepsilon^2 } = \frac{ V(X_1) }{n \, \varepsilon^2 } = \frac{ \lambda }{n \, \varepsilon^2 } \overset{n}{\rightarrow} 0 \Rightarrow \bar{X}_n \overset{p}{\rightarrow} \lambda$$

## Parte c

Por convergencia en media cuadrática:

$$\left.\begin{array}{r}
E(\bar{X}_n) = E(X_1) = \frac{1}{\lambda} \\ \\
V(\bar{X}_n) = \frac{V(X_1)}{n} = \frac{ 1 / \lambda^2}{n} \overset{n}{\rightarrow} 0
\end{array} \right\}
\Rightarrow \bar{X}_n \overset{mc}{\rightarrow} \frac{1}{\lambda} \Rightarrow \bar{X}_n \overset{p}{\rightarrow} \frac{ 1 }{ \lambda}$$

Utilizando desigualdad de Chebychev:

$$ P (| \bar{X}_n - \lambda | > \varepsilon ) \leq \frac{V(\bar{X}_n)}{\varepsilon^2} = \frac{ V(X_1) / n}{\varepsilon^2 } = \frac{ V(X_1) }{n \, \varepsilon^2 } = \frac{ 1 / \lambda^2 }{n \, \varepsilon^2 } \overset{n}{\rightarrow} 0 \Rightarrow \bar{X}_n \overset{p}{\rightarrow} \frac{1}{\lambda}$$

# Ejercicio 3

## Estimador $T_n = 2 \sum\limits_{i = 1}^{n} X_i / n$

Por el ejercicio 1 parte c sabemos que si $g$ es continua y $X_n \overset{cs}{\rightarrow} X$, entonces $g(X_n) \overset{cs}{\rightarrow} g(X)$. Por el LFGN sabemos que $S_n / n \overset{cs}{\rightarrow} E(X_1)$. Por último, dado que $X_n$ es una sucesión iid de variables aleatorias con distribución $Unif(0, \theta]$, sabemos que $E(X_i) = \theta / 2$. Con esto entonces:
$$T_n = 2 \frac{S_n}{n} = g(S_n) \overset{cs}{\rightarrow} g(E(X_1)) = 2 \, \left( \frac{\theta}{2} \right) = \theta$$
dado que $g(z) = 2z$ es continua por ser polinómica.

## Estimador $X_{(n)}$

Sea $X_{(n)} = \max\limits_n \{X_1, X_2, \ldots\} \Rightarrow f_{X_{(n)}}(x) = \frac{n x^{n-1}}{\theta^n} \, \mathbb{I}_{(0 < x < \theta]}$. Luego,

$$P(|X_{(n)} - \theta| > \varepsilon) = 1 - P (|X_{(n)} - \theta | < \varepsilon ) = 1 - P(\theta - \varepsilon < X_{(n)} < \theta + \varepsilon) = $$
$$= 1 - P(\theta - \varepsilon < X_{(n)} < \theta) = 1 - \int\limits_{\theta - \varepsilon}^{\theta} f_{X_{(n)}}(x) dx = 1 - \int\limits_{\theta - \varepsilon}^{\theta} \frac{n x^{n-1}}{\theta^n} dx =$$
$$= 1 - \frac{1}{\theta^n} \left( x^n \big|_{\theta - \varepsilon}^{\theta} \right) = 1 - \left( \frac{\theta^n}{\theta^n} - \frac{(\theta - \varepsilon)^n}{\theta^n}\right) = \frac{(\theta - \varepsilon)^n}{\theta^n} = \left( \frac{\theta - \varepsilon}{\theta}\right)^n $$

Luego por el corolario 2.4 sabemos que si $\forall \varepsilon > 0$, $\sum\limits_{n = 1}^{+\infty} P(|X_{(n)} - \theta| > \varepsilon) < \infty \Rightarrow X_{(n)} \overset{cs}{\rightarrow} \theta$. Lo cual se cumple en este caso:

$$ \sum\limits_{n = 1}^{+ \infty} P ( | X_{(n)} - \theta | > \varepsilon ) = \sum\limits_{n = 1}^{+\infty} \left( \frac{ \theta - \varepsilon }{ \theta } \right)^n = - 1 + 1 + \sum\limits_{n = 1}^{+\infty} \left( \frac{ \theta - \varepsilon }{ \theta } \right)^n =$$
$$= - 1 + \left( \frac{ \theta - \varepsilon }{ \theta } \right)^0 + \sum\limits_{n = 1}^{+\infty} \left( \frac{ \theta - \varepsilon }{ \theta } \right)^n = - 1 + \sum\limits_{n = 0}^{+\infty} \left( \frac{ \theta - \varepsilon }{ \theta } \right)^n < +\infty \text{ dado que } \left| \frac{ \theta - \varepsilon }{ \theta } \right| < 1 $$

Por lo tanto, encontramos que $X_{n} \overset{cs}{\rightarrow} \theta$.

# Ejercicio 4

**Parte a**: mostrar que $d_2(X,Y) = \sqrt{ E(X - Y)^2 }$ es una distancia en el conjunto de las variables aleatorias con momento de segundo orden finito.

*Dem*: implica probar todas las características de una distance.

1) *No negatividad*: $d_2(X,Y) > 0$, $\forall X \neq Y$ se cumple dado que $Z = (X - Y)^2 > 0 \Rightarrow \sqrt{E(Z)} > 0$.

2) *Identidad*: $d_2(X,Y) = 0 \Leftrightarrow X = Y$. El directo requiere definir clases de equivalencias. Es decir, no es cierto así como aparece planteado en el ejercicio. Ahora, si $X \overset{cs}{=} Y$, es decir, $X$ e $Y$ son iguales en un conjunto de probabilidad 1, entonces sí se cumple que si $d_2(X,Y) = 0$, $X$ debe ser igual ($cs$) a $Y$, dado que es la única forma de que su cuadrado sea cero. Por su parte, el recíproco sí es inmediato. Es decir, si $X = Y \Rightarrow d_2(X,Y) = \sqrt{E(X - Y)^2} = \sqrt{E(X - X)^2} = \sqrt{E(0)^2} = \sqrt{E(0)} = \sqrt{0} = 0$. 

3) *Simetría*: $d_2(X,Y) = \sqrt{E(X - Y)^2} = \sqrt{E(Y - X)^2} = d_2(Y,X)$

4) *Desigualdad triangular*:
$$d_2(X,Y) = \sqrt{ E(X - Y)^2} = \sqrt{ E(X - Z + Z - Y)^2} = $$
$$= \sqrt{ E (X - Z)^2 + 2 E(X - Z)(Z - Y) + E(Z - Y)^2 } \leq$$
$$\leq \sqrt{ E (X - Z)^2 + 2 \sqrt{ E(X - Z)^2 \, E(Z - Y)^2 } + E(Z - Y)^2} \leq$$
$$\leq \sqrt{ E (X - Z)^2 + E(Z - Y)^2} \leq \sqrt{ E (X - Z)^2} + \sqrt{ E(Z - Y)^2 } = d_2(X,Z) + d_2(Z,Y)$$

**Parte b**: verificar que $d_2(X_n,Y) \rightarrow 0$ implíca $X_n \overset{p}{\rightarrow} Y$.

$$0 \leq P \left( |X_n - Y| > \varepsilon \right) = P \left( \sqrt{|X_n - Y|^2} > \sqrt{\varepsilon^2} \right) \leq \frac{ E \left( \sqrt{|X_n - Y|^2} \right) }{ \sqrt{\varepsilon^2} }  \leq$$
$$\leq \frac{ \sqrt{ E \left( |X_n - Y|^2 \right)} }{ \varepsilon } = \frac{d_2(X_n,Y)}{\varepsilon} \overset{n}{\rightarrow} 0 \Rightarrow X_n \overset{p}{\rightarrow} Y$$
Donde, para la primer desigualdad usamos que toda probabilidad está acotada inferiormente por cero. Para la segunda utilizamos la desigualdad de Markov dado que $|X_n - Y|^2$ es siempre positiva. Por último, para la tercer desigualdad utilizamos la desigualdad de Jensen donde $g(z) = \sqrt{z}$ es una función cóncava.

\vspace{0.25cm}

**Parte c**: primero debemos percatarnos de que lo que necesitamos hallar es una sucesión de variables aleatorias que converga en probabilidad, pero no en media $L^2$ dado que, que la función de distancia con la que estamos trabajando converga a cero, implíca que $X_n \overset{L^2}{\rightarrow} Y$.

\vspace{0.3cm}

Sean $U \sim Unif(0,1)$ y $X_n = n \, \mathbb{I}_{ [0, \, ^1\!/_n] }(u)$.
$$P_{X_n} ( X_n = n ) = P_{X_n} \big( n \, \mathbb{I}_{ [0, \, ^1\!/_n] }(u) = n \big) = P_{X_n} \big( \mathbb{I}_{ [0, \, ^1\!/_n] }(u) = 1 \big) =$$
$$= P_U \big( u \in [0, \, ^1\!/_n] \big) = F_U \left( ^1\!/_n \right) - F_U(0) = \frac{1}{n}$$
$$P_{X_n} ( X_n = 0 ) = P_U \big( u \notin [0, \, ^1\!/_n] \big) = 1 - P_U \big( u \in [0, \, ^1\!/_n] \big) = 1 -  \frac{1}{n} $$

Luego entonces,
$$ P (|X_n - 0| > \varepsilon) = P(X_n > \varepsilon) = \frac{1}{n} \overset{n}{\rightarrow} 0 \Rightarrow X_n \overset{p}{\rightarrow} 0$$

Mientras que:
$$E ( X_n ) = n \, P(X_n = n) + 0 \, P(X_n = 0) = n \, \frac{1}{n} = 1 \neq 0 = E(X) \Rightarrow E(X_n) \, \cancel{\overset{L^2}{\rightarrow}} \, E(X)$$


# Ejercicio 5

## Parte a: Demostrar que $d_p(X,Y) = E \left( 1 - e^{-|X - Y|} \right)$ es una distancia en el espacio de todas las variables aleatorias.

1) *No negatividad*: $d_p(X,Y) = E \Big( \underbrace{ 1 - \overbrace{ \exp\big\{ \underbrace{- |X - Y| }_{< 0} \big\} }^{< 1} }_{> 0} \Big) > 0$

2) *Identidad*: $d_p(X,Y) = 0 \Leftrightarrow X \overset{cs}{=} Y$. Si la distancia es cero entonces $X \overset{cs}{=} Y$ dado que es la única forma de que la esperanza considerada sea la esperanza de cero. Por su parte, si $X \overset{cs}{=} Y$ entonces la demostración es inmediata: 
$$d_p(X,Y) = E \left( 1 - e^{-|X - X|} \right) = E(1 - e^{-0}) = E(1 - 1) = E(0) = 0$$

3) *Simetría*: $d_p(X,Y) = E \left( 1 - e^{- | X - Y |} \right) = E \left( 1 - e^{- | Y - X |} \right) = d_p(Y,X)$

4) *Desigualdad triangular*: $d_p(X,Y) \leq d_p(X,Z) + d_p(Z,Y)$

$$|X - Y| = |X - Z + Z - Y| \leq |X - Z| + |Z - Y|$$
$$ - |X - Y| \geq - |X - Z| - |Z - Y|$$
$$ \exp\big\{ - |X - Y| \big\} \geq \exp \big\{ - |X - Z| - |Z - Y| \big\}$$
$$ - \exp\big\{ - |X - Y| \big\} \leq - \exp \big\{ - |X - Z| - |Z - Y| \big\}$$

Obtuvimos una expresión de la forma $-e^{-c} \leq -e^{-a-b}$ donde $a,b,c \geq 0$. Queremos ahora probar que si $a,b \geq 0$, entonces $-e^{-a-b} \leq 1 - e^{-a} - e^{-b}$. Comenzamos por escribir la desigualdad como una función:
$$f: \mathbb{R}^+_0 \times \mathbb{R}^+_0 \rightarrow \mathbb{R} / f(a,b) = e^{-a} + e^{-b} - e^{-a-b} - 1 $$
Queremos demostrar entonces que $f(a,b) \leq 0$ para todo $a,b \geq 0$. Primero veamos que $f(0,0) = e^{-0} + e^{-0} - e^{-0-0} - 1 = 0$. Luego, si la función siempre decrece (tanto en trayectorias de $a$ como en trayectorias de $b$), entonces la función es menor a 0 para todo $a,b \geq 0$. Comenzemos entonces calculando sus derivadas parciales.
$$ \frac{\partial}{\partial a} f(a,b) = - e^{-a} + e^{-a-b} \leq 0 \Leftrightarrow e^{-a-b} \leq e^{-a} \Leftrightarrow - a - b \leq - a \Leftrightarrow -b \leq 0 \,\,\,\,\, \textcolor{green}{\checkmark}$$
$$ \frac{\partial}{\partial b} f(a,b) = - e^{-b} + e^{-a-b} \leq 0 \Leftrightarrow e^{-a-b} \leq e^{-b} \Leftrightarrow - a - b \leq - b \Leftrightarrow - a \leq 0 \,\,\,\,\, \textcolor{green}{\checkmark}$$
Ambas se cumplen dado que $a,b \geq 0 \Rightarrow -a,-b \leq 0$. Por tanto, la función $f$ tiene su máximo en $(0,0)$ y decrece a medida que $a$ y $b$ crecen. Hemos entonces demostrado que se cumple la siguiente desigualdad:
$$ - e^{-c} \leq -e^{-a-b} \leq 1 - e^{-a} - e^{-b} \Rightarrow - e^{-c} \leq 1 - e^{-a} - e^{-b} $$

Volviendo a nuestro problema:
$$ - e^{-|X - Y|} \leq - e^{-|X - Z|} \, e^{-|Z - Y|} \leq 1 - e^{-|X - Z|} - e^{-|Z - Y|} \Rightarrow$$
$$\Rightarrow 1 - e^{-|X - Y|} \leq 1 - e^{-|X - Z|} + 1 - e^{-|Z - Y|} \Rightarrow$$
$$\Rightarrow E \left( 1 - e^{-|X - Y|} \right) \leq E \left( 1 - e^{-|X - Z|} + 1 - e^{-|Z - Y|} \right) \Rightarrow$$
$$\Rightarrow E \left( 1 - e^{-|X - Y|} \right) \leq E \left( 1 - e^{-|X - Z|} \right) + E \left( 1 - e^{-|Z - Y|} \right) \Rightarrow$$
$$\Rightarrow d_p(X,Y) \leq d_p(X,Z) + d_p(Z,Y)$$

## Parte b: Demostrar que $d_p(X,Y) \overset{n}{\rightarrow} 0 \Leftrightarrow X_n \overset{p}{\rightarrow} Y$

$\boldsymbol{(\Rightarrow)}$
$$ P(| X_n - Y | > \varepsilon ) = P \left( - | X_n - Y | < -\varepsilon \right) = P \left( e^{- | X_n - Y |} < e^{- \varepsilon} \right) =$$
$$= P \left( - e^{- | X_n - Y |} > - e^{- \varepsilon} \right) = P \left( 1 - e^{- | X_n - Y |} > 1 - e^{- \varepsilon} \right) \leq \frac{ E \left( 1 - e^{- | X_n - Y |} \right) }{ 1 - e^{- \varepsilon} } = $$
$$= \frac{ d_p \left( X_n, Y \right) }{ 1 - e^{- \varepsilon} } \overset{n}{\rightarrow} 0 \Rightarrow X_n \overset{p}{\rightarrow} Y $$

$\boldsymbol{(\Leftarrow)}$
$$0 \leq d_p(X_n,Y) = E \left( 1 - e^{-|X_n - Y|}\right) \leq 1 - e^{- E(|X_n - Y|)} = $$
$$= 1 - \exp\left\{ - E \left[ |X_n - Y| \big( \mathbb{I}_{\{ |X_n - Y| > \varepsilon \}} + \mathbb{I}_{\{ |X_n - Y| \leq \varepsilon \}} \big) \right] \right\} =$$
$$= 1 - \exp\left\{ - E \left[ |X_n - Y| \, \mathbb{I}_{\{ |X_n - Y| > \varepsilon \}} \right] - E \left[ |X_n - Y| \, \mathbb{I}_{\{ |X_n - Y| \leq \varepsilon \}} \right] \right\} \leq $$
$$\leq 1 - \exp\Big\{ - E \big[ \underbrace{|X_n - Y|}_{ \leq \varepsilon} \, \mathbb{I}_{\{ |X_n - Y| \leq \varepsilon \}} \big] \Big\} \leq $$
$$\leq 1 - \exp\Big\{ - E \big[ \varepsilon \, \mathbb{I}_{\{ |X_n - Y| \leq \varepsilon \}} \big] \Big\} = 1 - \exp\Big\{ - \varepsilon \, E \big[ \mathbb{I}_{\{ |X_n - Y| \leq \varepsilon \}} \big] \Big\} = $$
$$ = 1 - \exp\Big\{ - \varepsilon \, \underbrace{P \big( |X_n - Y| \leq \varepsilon \big)}_{ \overset{n}{\rightarrow} 1 } \Big\} = 1 - e^{-\varepsilon} \overset{n}{\rightarrow} 0 \Rightarrow d_p(X_n, Y) \overset{n}{\rightarrow} 0$$

Donde en la primera desigualdad utilizamos la no negatividad de la medida de distancia. En la segunda desigualdad utilizamos la desigualdad de Jensen. La tercera desigualdad se cumple por que $1 - e^{-(a+b)} \leq 1 - e^{-a}$ por ser $1 - e^{-(a+b)}$ monótona creciente en todos los reales. Mientras que la cuarta desigualdad se cumple por que es cuando la indicatriz toma valor 1 (por lo que $|X_n - Y|$ efectivamente es menor que $\varepsilon$).

# Ejercicio 6

## Parte a: probar que si $E \big[ (X_n - X)^2 \big] \overset{n}{\rightarrow} 0 \Rightarrow E(X_n) \overset{n}{\rightarrow} E(X)$.

Comencemos notando que lo que debemos demostrar es que $E(X_n) \overset{n}{\rightarrow} E(X)$, lo cual implíca demostrar que $E(X_n - X) \overset{n}{\rightarrow} 0$. Por la desigualdad de Jensen tenemos que: 
$$\big[ E(X_n - X)\big]^2 \leq E \big[ (X_n - X)^2 \big]$$
dado que $g(z) = z^2$ es convexa. Luego entonces:
$$\sqrt{ \big[ E(X_n - X)\big]^2 } = E(X_n - X) = E(X_n) - E(X) \leq \sqrt{E \big[ (X_n - X)^2 \big]} \overset{n}{\rightarrow} 0$$

Por lo tanto, $E(X_n) \overset{n}{\rightarrow} E(X)$.

## Parte b: probar que si $E \big[ (X_n - X)^2 \big] \overset{n}{\rightarrow} 0 \Rightarrow E(X_n^2) \overset{n}{\rightarrow} E(X^2)$.

Queremos probar que, dadas las hipótesis, $E(X_n^2 - X^2) \overset{n}{\rightarrow} 0$. Utilizando la desigualdad de Cauchy-Schwarz obtenemos que:
$$E(X_n^2 - X^2) = E \big[ (X_n - X)(X_n + X) \big] \leq \underbrace{ \sqrt{ E \big[ (X_n - X)^2 \big] } }_{ \overset{n}{\rightarrow} 0 \text{ por hipótesis}} \sqrt{E \big[ (X_n + X)^2 \big]} $$
Vemos entonces que el límite de $E(X_n^2 - X^2)$ es igual al producto de dos factores, uno de los cuales tiende a cero. Si el otro factor está acotado (no tiende a $+\infty$), entonces el producto tiende a cero y queda demostrada la proposición. Veamos que esto se cumple.
$$(X_n + X)^2 = (X_n - X + 2X)^2 \leq 2(X_n - X)^2 + 2(2X)^2$$
La desigualdad se cumple dado que:
$$0 \leq (a - b)^2 = a^2 + b^2 - 2ab \Rightarrow 0 \leq (a^2 - b^2) \Leftrightarrow 2ab \leq a^2 + b^2$$ 
Sumando $a^2$ y $b^2$ a ambos lados de la desigualdad obtenemos que:
$$2ab + a^2 + b^2 \leq 2a^2 + 2b^2 \Rightarrow (a + b)^2 \leq 2a^2 + 2b^2$$

Volviendo a nuestro problema, si tomamos esperanza obtenemos que:
$$ E \big[ (X_n + X)^2 \big] \leq E \big[ 2 ( X_n - X )^2 + 2 (2X)^2 \big] = 2 \Big[ \underbrace{ E \big[ ( X_n - X )^2 \big] }_{  \overset{n}{\rightarrow} 0 \text{ por hipótesis} } + 4 \, E(X^2) \Big] = 8 \, E(X) $$

Por lo tanto,
$$E(X_n^2 - X^2) = E \big[ (X_n - X)(X_n + X) \big] \leq \underbrace{ \sqrt{ E \big[ (X_n - X)^2 \big] } }_{ \overset{n}{\rightarrow} 0 \text{ por hipótesis}} \underbrace{ \sqrt{E \big[ (X_n + X)^2 \big]} }_{ \text{acotado por } \sqrt{8 \, E(X)} } \overset{n}{\rightarrow} 0$$

Entonces, $E(X_n^2) \overset{n}{\rightarrow} E(X^2)$.

## Parte c: encontrar una sucesión que converga en media pero no en media cuardática

Sea $\{ X_n \}_{n \in \mathbb{N}}$ una sucesión de variables aleatorias iid con reocrrido $Rec(X_n) = \{-n, \, 0, \, n\}$ tales que:
$$p_{X_n}(x) = \left\{ 
\begin{array}{c c c} 
\frac{1}{n} & \text{si} & x = -n \\ \\
1 - \frac{2}{n} & \text{si} & x = 0 \\ \\
\frac{1}{n} & \text{si} & x = n
\end{array}
\right. \Rightarrow F_{X_n}(x) \left\{
\begin{array}{c c c} 
0 & \text{si} & x < -n \\ \\
\frac{1}{n} & \text{si} & -n \leq x < 0 \\ \\
1 - \frac{1}{n} & \text{si} & 0 \leq x < n \\ \\
1 & \text{si} & x \geq n
\end{array} \right.$$

### Convergencia en media

Esperanza de $X_n$: $E(X_n) = -n \left(\frac{1}{n}\right) + 0 \left( 1 - \frac{2}{n}\right) + n \left(\frac{1}{n}\right) = -1 + 1 = 0$

Por lo tanto: $E(X_n - X) = E(X_n) = 0 \Rightarrow X_n \overset{L^1}{\rightarrow} 0$

### Convergencia en media cuadrática

Cuantía de $X_n^2$: $P(X_n^2 = x) = \left\{ \begin{array}{ccc} \frac{2}{n} & \text{si} & x = n^2 \\ \\ 1 - \frac{2}{n} & \text{si} & x = 0 \end{array} \right.$

Segundo momento de $X_n$: $E(X_n^2) = n^2 \left(\frac{2}{n}\right) + 0 \left(1 - \frac{2}{n}\right) = 2n \overset{n}{\rightarrow} + \infty$. Por lo tanto, $E(X_n - 0)^2 = E(X_n^2) = 2n \Rightarrow X_n \,  \cancel{\overset{mc}{\rightarrow}} \, 0$.

\newpage

# Ejercicio 7

## Parte a

$$\begin{array}{ l l }
f_{X_n}(x;\theta) = \mathbb{I}_{ \{ \theta < x < \theta + 1 \} } & 
F_{X_n}(x;\theta) = (x - \theta) \mathbb{I}_{ \{ \theta < x < \theta + 1 \} } \\ \\
f_{X_{1:n}}(x;\theta) = n \, (1 - x + \theta)^{n - 1} \, \mathbb{I}_{ \{ \theta < x < \theta + 1 \} } &
F_{X_{1:n}}(x;\theta) = 1 - (\theta + 1 - x)^n \mathbb{I}_{ \{ \theta < x < \theta + 1 \} }
\end{array}$$

$$P ( | X_{1:n} - \theta | > \varepsilon) = 1 - P ( | X_{1:n} - \theta | < \varepsilon) = 1 - P ( - \varepsilon < X_{1:n} - \theta < \varepsilon) = $$
$$ = 1 - P ( \theta < X_{1:n} < \theta + \varepsilon) = 1 - \big[ F_{X_{1:n}}( \theta + \varepsilon) - F_{X_{1:n}}( \theta) \big] = 1 - F_{X_{1:n}}( \theta + \varepsilon) + F_{X_{1:n}}( \theta) =$$
$$= 1 - \big[ 1 - (\theta + 1 - \theta - \varepsilon)^n \big] + \big[ 1 - (\theta + 1 - \theta)^n \big] =$$
$$= (1 - \varepsilon)^n + 1 - 1^n = (\underbrace{ 1 - \varepsilon }_{\leq 1})^n \overset{n}{\rightarrow} 0 \Rightarrow X_{1:n} \overset{p}{\rightarrow} \theta$$

## Parte b

$$\begin{array}{ l l }
f_{X_n}(x;\theta) = e^{-(x - \theta)} \, \mathbb{I}_{ \{ x > \theta \} } & 
F_{X_n}(x;\theta) = \left( 1 - e^{-(x - \theta)} \right) \mathbb{I}_{ \{ x > \theta \} } \\ \\
f_{X_{1:n}}(x;\theta) = n \, e^{- n ( x - \theta ) } \, \mathbb{I}_{ \{ x > \theta\} } &
F_{X_{1:n}}(x;\theta) = \left( 1 - e^{ -n ( x - \theta )} \right) \mathbb{I}_{ \{ x > \theta \} }
\end{array}$$

$$P ( | X_{1:n} - \theta | > \varepsilon) = 1 - P ( | X_{1:n} - \theta | < \varepsilon) = 1 - P ( - \varepsilon < X_{1:n} - \theta < \varepsilon) = $$
$$ = 1 - P ( \theta < X_{1:n} < \theta + \varepsilon) = 1 - \big[ F_{X_{1:n}}( \theta + \varepsilon) - F_{X_{1:n}}( \theta) \big] = 1 - F_{X_{1:n}}( \theta + \varepsilon) + F_{X_{1:n}}( \theta) =$$
$$ = 1 - \left( 1 - e^{ -n ( \theta + \varepsilon - \theta )} \right) + \left( 1 - e^{ -n ( \theta - \theta )} \right) = 1 - \left( 1 - e^{ - n \varepsilon } \right) = e^{-n \, \varepsilon} \overset{n}{\rightarrow} 0 \Rightarrow X_{1:n} \overset{p}{\rightarrow} \theta$$

## Parte c


## Parte d


# Ejercicio 8

# Ejercicio 9

# Ejercicio 10

# Ejercicio 11

# Ejercicio 12

# Ejercicio 13

# Ejercicio 14

# Ejercicio 15

# Ejercicio 16












